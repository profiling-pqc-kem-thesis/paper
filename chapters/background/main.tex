\chapter{Background}
\label{chapter:background}

\section{Cryptography}

\subsection{Symmetric Cryptography}
\todo[inline]{Revisit}
Cryptographic functions are sorted into two categories - \textit{symmetric} functions and \textit{public-key} functions. Both categories refer to how the keys to the data are used\cite{bernstein2017}.

In the case of symmetric functions, the same key is known to both parties. The key is used for both encryption and decryption. Symmetric functions can also be used to provide authenticity - if only the two parties know of the secret key, one of them may request the other to prove possession of the key\cite{bernstein2017}.

\subsection{Public-Key Cryptography}
\todo[inline]{Revisit}
In the case of public-key cryptography, each party is in possession of two keys - a public key and a private key. The public key is known to everyone, the private key only to the party in ownership of the keys. Anyone may encrypt a message using a party's public key, but only the owner of the private key may decrypt the contents. By using the private key to encrypt data, anyone with the public key may decrypt it. This mechanism provides a way to digitally sign a message and can be used to authenticate a party\cite{bernstein2017}.

\subsection{Key Establishment}

\todo{Make it more clear that this is connected to both symmetric and public-key}

A key establishment protocol, sometimes referred to as a \gls{kex}, is a process in which two or more parties exchange a shared session key known only to them. The protocol should be able to be performed on an untrusted communication channel\cite{boyd2020}.

There are typically two classes of key establishment, key transport and key agreement\todo{Too short sentence}.

A key transport protocol is a protocol where one party generates a shared key which is then securely transferred to one or more parties\cite{boyd2020}.

In a key agreement protocol, the parties jointly influence the outcome of the shared key by deriving the key from information supplied by the involved parties. This should be performed in such a way that no single party can predetermine the resulting shared key on their own\cite{boyd2020}.

\subsection{Key Encapsulation Mechanism}

A \gls{kem} is form of key transport protocol designed to generate a new random shared key. The key is encapsulated in a form which can only be unpacked by the chosen recipient. \glspl{kem} are often more efficient than general encryption schemes and are therefore used in many key establishment designs\cite{boyd2020}.

\todo{Should this be more descriptive? boyd2020 has information on a definition of a protocol. Also make it more descriptive for non-security students}

\subsection{Forward Secrecy}

Forward Secrecy is a term used to describe the security of session keys after one or more long-term keys have been exposed. A key establishment protocol is said to provide forward secrecy if the compromise of long-term keys does not compromise any previously exchanged session key\cite{boyd2020}.

Key agreement protocols may provide forward secrecy if the long-term keys are only used for authentication of the exchange. Another way to provide forward secrecy is to use ephemeral keys - keys that are only used for a single run of a key establishment protocol\cite{boyd2020}.

\section{Classical Cryptography}

\todo[inline]{Define classical cryptography - why isn't this included in the previous section?}

In the pre-quantum era of cryptography, several algorithms based on \gls{rsa} and \gls{ecc} are used for key exchange. Some of these algorithms and parameter sets are recommended for use in a pre-quantum era by organizations such as \gls{nist} and the \gls{ietf}, namely \gls{x25519}\cite{rfc7748}, \gls{ecdhe}\cite{nist2019} and \gls{dhe}\cite{nist2019}.

\todo[inline] {
Is this even relevant? Perhaps under the introduction instead?

In \gls{tls}, key establishment is used to let to parties establish a shared symmetric key. Digital signatures are used to ensure the authenticity of the public keys used in the key establishment. The rest of the communication is secured using symmetric cryptography\cite{bernstein2017}. \gls{tls} may use any of wide variety of different key establishment algorithms. Some of these algorithms and parameter sets are recommended for use in a pre-quantum era by organizations such as \gls{nist} and the \gls{ietf}, namely \gls{x25519}\cite{rfc7748}, \gls{ecdhe}\cite{nist2019} and \gls{dhe}\cite{nist2019}.

% Commented out due too footnote in todo not working
%\gls{x25519} (in some contexts known as \gls{curve25519}\footnote{\href{https://mailarchive.ietf.org/arch/msg/cfrg/-9LEdnzVrE5RORux3Oo\_oDDRksU/}{https://mailarchive.ietf.org/arch/msg/cfrg/-9LEdnzVrE5RORux3Oo\_oDDRksU/}}), is used for key exchange in the \gls{vpn} Wireguard\footnote{\href{https://www.wireguard.com/protocol/}{https://www.wireguard.com/protocol/}}.


% Commented out due too footnote in todo not working
%\gls{ecdh}, \gls{ecdhe}, \gls{dh}, \gls{dhe} are used to exchange session keys in TLS\cite{rfc8446}, SSH\cite{williams2011}, \glspl{vpn} such as OpenVPN\footnote{\href{https://openvpn.net/community-resources/openvpn-cryptographic-layer/}{https://openvpn.net/community-resources/openvpn-cryptographic-layer/}}, IPSec\cite{rfc2409} and Wireguard.

Variants of the mentioned key exchange algorithms are also used in messaging applications such as the Signal protocol\cite{gordon2017}. Some of these algorithms and parameter sets are recommended for use in a pre-quantum era by organizations such as \gls{nist} and the \gls{ietf}, namely \gls{x25519}\cite{rfc7748}, \gls{ecdhe}\cite{nist2019} and \gls{dhe}\cite{nist2019}.
}

\subsection{RSA}

\todo[inline]{Ha exempel kring public-key, introducera RSA och ECC. Förklara RSA, ECC etc. mer - mindre antagande än nu. Utveckla mer kring grund-begrepp.}

\subsection{Elliptic Curve Cryptography}

\todo[inline]{Ha exempel kring public-key, introducera RSA och ECC. Förklara RSA, ECC etc. mer - mindre antagande än nu. Utveckla mer kring grund-begrepp.}

\subsection{Diffie-Hellman}

The \gls{dh} \gls{kex} is a key establishment protocol conceived by Ralph Merkle and published in 1978\cite{merkle1978}. The work was centered around a novel idea, that anything sent across a communication channel will be intercepted by a malicious party. To enable such a secure key exchange, the idea was for the two trusting parties to ensure that the malicious party has to perform a much higher amount of work to derive the shared secret than the exchanging parties. That is, although it is possible for the malicious party to successfully break the key exchange, it must be unfeasible\cite{merkle1978}.

In the \gls{dh} protocol, two parties (Alice and Bob) agree publicly on a secret element $g$ that generates a multiplicative group $\mathbb{G}$. Each party selects a random value $r_A$ and $r_B$, respectively. These random values are in the range between $1$ and the order of $\mathbb{G}$. Alice calculates $t_A=g^{r_A}$ and Bob calculates $t_B=g^{r_B}$. The values $t_A$ and $t_B$ are exchanged on a channel that is not necessarily secure. Alice and Bob may now both calculate a shared secret $\mathbf{Z}=g^{r_A r_B}$ as $\mathbf{Z}=t_A^{r_B}=t_B{r_A}$\cite{merkle1978, boyd2020}.

The security of the key exchange comes from the assumption that it is computationally difficult for a malicious party to recover $g^{r_A r_B}$ from the public channel. The problem is that of a discrete logarithm which was deemed unfeasible in the pre-quantum era, given a sufficient key size\cite{boyd2020}. In the post-quantum era, \gls{dh} is considered to be broken\cite{bernstein2017}.

\subsection{Elliptic-Curve Diffie-Hellman}

The \gls{ecdh} \gls{kex} is an adaption of \gls{dh} over elliptic curves. In this case, \gls{ecc} refers to the use of one of the so called NIST curves (\gls{p-256} etc.)\cite{nist2018}.

Alice computes the point $P=hd_aQ_b$, where $d_A$ is Alice's private key, $Q_B$ Bob's public key and $h$ a \gls{ecc} domain parameter. If the point evaluates to a null point, the calculation has failed. Else, let $z=x_P$ be the x-coordinate of $P$ and convert the element $z$ to a byte string $Z$, the shared secret.

The \gls{x25519} \gls{kex} is a special case of \gls{ecdh} where \gls{curve25519} is used\cite{rfc7748}. 

\subsection{Ephemeral Diffie-Hellman}

In the case of both \gls{dh} and \gls{ecdh}, ephemeral keys may be used to provide forward secrecy. These ephemeral variants are referred to as \gls{dhe} and \gls{ecdhe}. As mentioned in section \ref{section:background:pre-quantum}, the use of ephemeral keys is recommended for all versions of \gls{dh}.

\subsection{Threats to classical cryptography}

\todo[inline]{Skriv en subsection om varför dessa inte kommer att räcka till framöver (kvantdatorer). Då kommer det naturligt, man har beskrivit traditionella krypton, man har beskrivit utmaningarna och varför det inte fungerar efter kvantdatorer. Sedan blir det naturligt att gå över till post-quantum. Använd befintlig text om post-quantum för att motivera problem med dagens krypton? "Threats to traditional crypto"?}

\todo[inline]{Write about why RSA isn't used, but ECDH etc. is?}

\todo[inline]{End with something something classical cryptography is sometimes also referred to as pre-quantum cryptography or the pre-quantum era}.

Cryptography does not usually come with any guarantee of being secure forever. Algorithms and parameters are updated continuously to mitigate attacks as they are found and as already known attacks become more practical\cite{nist2019}. As the performance of conventional computers have increased throughout the years, this has meant that some known attacks such as prime factorization and discrete logarithms have become more computationally feasible. The increase in performance has not yet lead to any major breakage as the algorithms in use are projected to withstand thousands of years of attacks using conventional algorithms\cite{thome2019}. As Moore's law is coming to a halt\cite{theis2017}, the threat of traditional computers becoming powerful enough to break the cryptographic algorithms they use has become less severe.

Since the 1980s, research has been made to utilize quantum mechanics for computation, thus introducing the quantum computer\cite{benioff1980}. By utilizing quantum bits or \glspl{qubit} instead of the bits used in traditional computers, quantum computers are able to represent several states per \gls{qubit}. This mechanic enables a quantum computer to feasibly perform calculations that have been deemed impossible or impractical on a traditional computer\cite{jordan2021}. Recent progress in the field has shown that, as progress have halted in the area of traditional computers, progress made on quantum computers has been increasing exponentially\cite{ibm2020:quantum-computer}.

Parallel to the development of the theoretical quantum computer, algorithms were developed to make use of the mechanics. Two of these algorithms, Shor's algorithm and Grover's algorithm have been shown to threaten pre-quantum cryptographic systems. They have been shown to be impractical or impossible to implement or use on a traditional system, but feasible to use on a quantum computer\cite{shor1997, jordan2021}. Demonstrating that a quantum computer is able to feasibly perform an algorithm that a classical computer cannot is referred to as Quantum Supremacy\cite{farhi2019}.

The performance increase these machines offer and the threats that the algorithms impose has led to a new epoch in computing and cryptography - post-quantum.

%\subsection{Shor's Algorithm}
\todo[inline]{Utveckla Shor's algoritm}
Pre-Quantum public-key cryptography suites are based on one of the following problems being hard to solve; integer factorization and elliptic-curve discrete logarithm. Shor's algorithm can solve these problems, rendering the cryptosystems useless\cite{shor1997}. However, it is known to be difficult or impractical to run on a traditional computer, but easy to run efficiently for a quantum computer. But today's quantum computers are not powerful enough to execute Shor's algorithm on the large numbers used in modern cryptography\cite{bernstein2017}. A study \cite{gidney2019} suggests that it would require 20 million noisy \glspl{qubit} to break 2048 bits \gls{rsa}. Today's most powerful quantum computer is Google's 53-\glspl{qubit} system\cite{google2019:quantum-computer} IBM's 65-\glspl{qubit} system\cite{ibm2020:quantum-computer}. IBM has a 1000-\glspl{qubit} system on their road map scheduled for release in 2023\cite{ibm2020:quantum-computer}.

%\subsection{Grover's Algorithm}
\todo[inline]{Utveckla Grover's algoritm}
Grover's algorithm was first proposed as a way to search in databases in $O(\sqrt N)$ quantum operations where N is the number of items\cite{grover1996}. However, a more practical application for the algorithm is to find the root of a function\cite{bernstein2017}. This enables an attack on some cryptosystem such as \gls{aes}, and the attack will slice the bit security in half. To mitigate this attack, you can double the size of the \gls{aes} key.

%\subsection{Bit Security}

Bit security corresponds to the best security a key of $n$ bits can provide under the best known attack. Values for some widely deployed cryptographic systems are presented in Table \ref{table:background:post-quantum:bit-security}.

\begin{table}[H]
    \centering
    \caption{Security levels for widely deployed cryptographic systems and the bit security\cite{bernstein2017}. Pre-quantum and post-quantum refers to the bit security of the algorithm for the corresponding epoch.}
    \label{table:background:post-quantum:bit-security}
    \begin{tabularx}{\linewidth}{X c c c c}
        \toprule
        \thead{Name} & \thead{Function} & \thead{Pre-Quantum} & \thead{Post-Quantum} & \thead{Attack} \\
        \midrule
        \multicolumn{5}{c}{\thead[l]{Symmetric Cryptography}} \\
        %\midrule
        AES-128 & block cipher & 128 & 64 & Grover\\
        AES-256 & block cipher & 256 & 128 & Grover\\
        Salsa20 & stream cipher & 256 & 128 & Grover\\
        GMAC & MAC & 128 & 128 & -\\
        Poly1305 & MAC & 128 & 128 & -\\
        SHA-256 & hash & 256 & 128 & Grover\\
        SHA-3 & hash & 256 & 128 & Grover\\
        \multicolumn{5}{c}{\thead[l]{Public-key Cryptography}} \\
        %\midrule
        RSA-3072 & encryption & 128 & broken & Shor \\
        RSA-3072 & signature & 128 & broken & Shor \\
        DH-3072 & key exchange & 128 & broken & Shor \\
        DSA-3072 & signature & 128 & broken & Shor \\
        256-bit ECDH & key exchange & 128 & broken & Shor \\
        256-bit ECDSA & signature & 128 & broken & Shor \\
        \bottomrule
    \end{tabularx}
\end{table}

\section{Post-Quantum Cryptography}

\todo[inline]{Post-quantum cryptography relies on an entirely different set of underlying mathematical problems...}

\subsection{The NIST Post-Quantum Standardization Process}
\acrfull{nist} is an American organization under the Department of Commerce. By advancing measurements, standards and technologies, the institute's goal is to promote U.S. innovation and industrial competitiveness\cite{nist:about}.

The organization is split into various divisions. One of these divisions, the Computer Security Division, has assembled the Cryptographic Technology Group. The group focuses on the topics of cryptographic algorithms such as block ciphers, digital signatures, hash functions and post-quantum cryptography\cite{nist:ct}.

The Cryptographic Technology Group has previously held standardization processes for the globally used algorithm suites \gls{aes} and \gls{sha3}. January 3rd, 2017, the Cryptographic Technology Group posted another call for submissions to an open standardization contest. This time for post-quantum cryptography algorithms. The process was estimated to take three to five years with multiple rounds of submissions\cite{nist:call-for-proposals}.

At the time of writing, the process has been ongoing for three years and it has reached a third round of submissions. For the third round, \gls{nist} published finalists and alternate candidates grouped in public-key encryption and key-establishment algorithms as well as digital signature algorithms\cite{nist:round-three-submissions}.

The public-key encryption and key-establishment algorithms finalists were the following. \todo{ref}
\todo[inline]{Write about the types of cryptography to refer to later / knyt ihop säcken för lattice-based-sektionerna o.s.v. sedan}

\begin{itemize}
    \item Classic McEliece
    \item CRYSTALS-KYBER
    \item NTRU
    \item SABER
\end{itemize}

The digital signature finalists were the following. \todo{ref}

\begin{itemize}
    \item CRYSTALS-DILITHIUM
    \item FALCON
    \item Rainbow
\end{itemize}

\subsection{IND-CCA2}
\todo[inline]{Is this really necessary? Have it as text under the nist section?}

\gls{nist} has identified that a relevant attack on post-quantum \glspl{kem} is a chosen-ciphertext attack. Resistence to such an attack is referred to as \gls{ind-cca2} security\cite{nist2017}. 

\todo{Explain entire random oracle model, talk about the actual meaning and the different categories?}

\subsection{Security Categories}
\todo[inline]{Is this really necessary? Have it as text under the nist section?}

\gls{nist} anticipated that submissions to the standardization effort would face uncertainties in terms of estimating the security strength of post-quantum algorithms. They state that the uncertainties are largely caused by two issues. The first issue is the possibility that new attacks will be discovered and the second that there is a limited ability to predict how a quantum computer will behave in terms of cost, speed and memory\cite{nist2017}.

\gls{nist} therefore defined security categories that are not based on the traditional measure of bit security. The following categories were defined. The order denotes the strength of the attack\cite{nist2017}.

\begin{itemize}
    \item 1. Any attack that breaks the relevant security definition must require computational resources comparable to or greater than those required for key search on a block cipher with a 128-bit key (e.g. AES128)
    \item 2. Any attack that breaks the relevant security definition must require computational resources comparable to or greater than those required for collision search on a 256-bit hash function (e.g. SHA256/ SHA3-256)
    \item 3. Any attack that breaks the relevant security definition must require computational resources comparable to or greater than those required for key search on a block cipher with a 192-bit key (e.g. AES192)
    \item 4. Any attack that breaks the relevant security definition must require computational resources comparable to or greater than those required for collision search on a 384-bit hash function (e.g. SHA384/ SHA3-384)
    \item 5. Any attack that breaks the relevant security definition must require computational resources comparable to or greater than those required for key search on a block cipher with a 256-bit key (e.g. AES 256)
\end{itemize}

\subsection{Lattice-Based Cryptography}

A lattice is the set of linear combinations of the basis vectors in a euclidean space. It is defined as follows, where $\mathbb{Z}$ refers to an integral coefficient and $\mathbf{x}_1,\mathbf{x}_2,...,\mathbf{x}_n$ a basis in the euclidean space $\mathbb{R}^n$\cite{bremner2012}.

$$
L=\mathbb{Z}\mathbf{x}_1+\mathbb{Z}\mathbf{x}_2+...+\mathbb{Z}\mathbf{x}_n=\left\{\sum_{i=1}^n a_i\mathbf{x}_i|a_1,a_2,...,a_n\in\mathbb{Z}\right\}
$$

\noindent A lattice provides several applications in number theory and has been applied to cryptography, where the problems imposed by lattices is taken advantage of\cite{bremner2012}. The most significant problem is the shortest vector problem. The problem revolves around approximating the minimal euclidean length of a non-zero lattice vector. The problem has been shown to be hard to solve efficiently and is thought to be secure from future classical and post-quantum algorithms alike\cite{sun2020}.

\subsection{NTRU}

\todo{NTRU is a lattice-based crypto}

The \gls{ntru} \gls{kem} is a unification of several variants. It has its roots in two submissions to the \gls{nist} standardization process; NTRUEncrypt and NTRU-HRSS-KEM.

The original paper on the \gls{ntru} cryptosystem was published in 1998, following the interest of creating a new, efficient and computationally inexpensive public key cryptosystem\cite{ntru1998}. NTRU then used a mixing system which was based on polynomial algebra (modulo some number $p$ and $q$) for encryption. The decryption was centered around an unmixing system. The security of \gls{ntru} relied on the short vector problem being hard to solve\cite{ntru1998}.

\todo[inline]{more information on the KEM?}
\todo{include security category}

\subsection{CRYSTALS-KYBER}

\todo{CRYSTALS-KYBER is a lattice-based crypto}

\todo[inline]{todo}
%\todo{include security category}

\subsection{SABER}

\todo{SABER is a lattice-based crypto}

\todo[inline]{todo}
%\todo{include security category}

\subsection{Code-Based Cryptography}
Code-based cryptosystems take advantage of error-correction code. Typically error-correction code is used to detect or correct a bit flip that occurred. But in cryptography, you can use it to encrypt by adding errors, which can be corrected when you decrypt\cite{bernstein2017}. There are different types of error-correction codes, for example, quasi-cyclic codes and Goppa codes.\cite{sendrier2011}.

\subsection{Classic McEliece}
Classic McEliece is a code-based cryptosystem. It uses random binary Goppa codes.


\todo[inline]{
% From nist 2020
Classic McEliece is a code-based KEM based on the 1979 McEliece cryptosystem built from a hidden Goppa code. Classic McEliece includes some modern improvements for efficiency and to provide CCA security. The security of these improvements is reduced to the one-wayness against chosen-plaintext attack (OW-CPA) security of the original construction as proposed in 1979 [17]. The original construction is not based on a particularly natural computational assumption; however, Goppa code McEliece and related cryptosystems have a long history of study\cite{nist2020}.\\
Combination av olika McEliece implementationer
}

\todo{include security category}

\section{Architectures and Performance}
\todo[inline]{Se över ordning på stycken.}

% "Theoretical" topics on performance optimization

\subsection{Vectorization}

The classification of computer architectures proposed by Flynn, later named to Flynn's taxonomy, describes the four classifications presented below. \cite{flynn1972}.

\begin{itemize}
    \item The \gls{sisd} organization represents the most conventional type of computer. Such an organization is limited by data dependencies. Branching is particularily limiting.
    \item The \gls{simd} organization represents array processes. The performance of such a process increases as $\mathcal{O}(\log_2 N)$ with respect to then number of data stream processors. 
    \item The \gls{misd} organization was typically represented by the plug-board type machines no longer in use.
    \item The \gls{mimd} organization is typically referred to as multi-processors. The organization may be subject to saturation.
\end{itemize}

Due to the properties of \gls{simd}, it has been an attractive target for software optimization with practical use seeing significant throughput increases\cite{dickson2011}.

In order to use the properties of \gls{simd}, the target architecture has to be a vector architecture or one that supports \gls{simd} extensions. Such a target will work on sets of data elements in memory, placing them in sequential registers, operate on the vectors of data using a single instruction and then disperse the results back to main memory. In the case of a vector architecture, these vector payloads are heavily pipelined. The memory overhead is consistent for an entire vector of operands as opposed to linear in regular \gls{sisd} architectures\cite{hennessy2011:vectorization}. Furthermore, a piece of code may have to be vectorized to use the properties offered by \gls{simd}. That is, it will need to be written in such a way that the same operation can be performed on multiple units of data. Not all algorithms can be written in such a way, however\cite{dickson2011}.

Although usage of a CPU's vectorization capabilities may require explicit input from a programmer, advanced compilers may in some cases be able to automatically vectorize code in order to make it run more efficiently\cite{dickson2011}.

\subsection{CPU Architectures}

\todo[inline]{Utöka information kring 32, 64-bitars, förklara mer vad som är karakteristiskt för x86. Kanske lite historik. De är lite "cisc" allihop, men power är lite mer "risc".}

The \acrfull{risc} design methodology is centered around a load/store architecture\cite{flynn1998}. This means that only the load and store instructions may access the memory system\cite{carter2002}. The aim is to reduce the amount of complexity in the instruction set and regularize the instruction format. The regularization simplifies the decoding of the instructions with the aim to improve the overall performance\cite{flynn1998}. The \acrfull{cisc} design methodology, on the other hand, is centered around a register/memory architecture\cite{flynn1998}. The architecture permits arithmetic and other instructions to read their input from, or write to the memory system\cite{carter2002}.

A \gls{cisc} computer generally requires fewer instructions than a \gls{risc} computer to perform a computation. A \gls{cisc} computer may therefore perform better than a \gls{risc} computer that performs instructions at the same rate. A \gls{risc} computer may be implemented at a higher clock rate than a \gls{cisc} computer as the instructions can more efficiently be decoded\cite{carter2002}.

The nature load/store and register/memory architectures of \gls{risc} and \gls{cisc} results in memory-related arithemtic requiring fewer instructions on a \gls{cisc} machine than a \gls{risc} machine. The hardware required for the machine is more complex, however. This trade-off is one of many when comparing the architectures\cite{carter2002}.

An example of an architecture using \gls{cisc} is \gls{x86}. Examples of architectures using \gls{risc} are ARM and RISC-V.

\subsection{x86}

\todo[inline]{Clarify history, talk about intel x86 vs others amd64 instead?}.

\todo[inline]{Utöka information kring 32, 64-bitars, förklara mer vad som är karakteristiskt för x86. Kanske lite historik. De är lite "cisc" allihop, men power är lite mer "risc". AMD64??}

The \gls{x86} family of architectures are some of the most popular architectures for consumer and cloud hardware. The family contains architectures such as i386 and x86-64. It is not uncommon for the architectures to be referred to as just \gls{x86}\cite{carter2002}.

\gls{x86} is in theory a \gls{cisc}, but there has been a fair amount of convergence between \gls{risc} and \gls{cisc}, which makes them a hybrid of sorts\cite{carter2002}.

\subsection{Advanced Vector Extensions (AVX)}

In 1996 the x86 architecture saw the addition of the MMX instruction set. The instruction set used the 64-bit floating-point registers of the CPU to enable performing eight 8-bit operations, or four 16-bit operations simultaneously. The instruction set was superseeded by \gls{sse} in 1999, which added special registers of 128 bits. This enabled instructions to simultaneously perform sixteen 8-bit operations, eight 16-bit operations, or four 32-bit operations. \gls{sse} was in turn superseeded by SSE2 in 2001, SSE3 in 2004 and SSE4 in 2007. Each generation saw improvements in available instructions and floating-point performance\cite{hennessy2011:avx}.

In 2010, the \gls{avx} doubled the width of the registers to 256 bits. This increase effectively doubled the number of simultaneous operations that could be performed. The 256-bit width supported 64-bit operands in parallel\cite{hennessy2011:avx}. \gls{avx} was superseeded by \gls{avx2} and \gls{avx512}, 256-bit\cite{intel:avx2} and 512-bit\cite{intel:avx512} wide respectively.

By using AVX, a developer may optimize a program and make use of the performance properties offered by \gls{simd}\cite{hennessy2011:avx}.

\subsection{Mainframe Hardware}
\todo[inline]{En bra artikel i IEEE Micro publicerades nyligen. Bra översikt av utvecklingen av Z-serien. Jacobi & Webb History of IBM Z mainframe proc. nov/dec 2020}

Mainframe hardware is designed to handle a large amount of data and bulk transactions. To achieve this, they are using custom-made CPUs and special coprocessors. Further are availability, resilience, backward compatibility, and security core features.


\todo[inline]{IBM Z, POWER. RISC. Co-processorer. Mycket ändras när man går från x86 till mainframe. Man kan inte mäta instruktioner etc. som vanligt. En instruktion kan göra mycket.}

\subsection{z15}
\todo[inline]{En bra artikel i IEEE Micro publicerades nyligen. Bra översikt av utvecklingen av Z-serien. Jacobi & Webb History of IBM Z mainframe proc.}
In z15, virtualization is a core feature. It is designed from the bottom up with virtualization in mind with specially engineered hardware to handle it. In the first layer of virtualization, the hardware is divided into \glspl{lpar} that runs its own virtual system. z15 is a 64-bit \gls{cisc} architecture. Their each CPU has 12 custom cores with \gls{smt}2 which means that each core has two logical cores. A mainfram contains multiple of these CPUs. Special coprocessors are built into the CPU, \gls{cpacf} accelerates some cryptographic functions see more in section \ref{subsection:cpacf}, and \gls{nxu} to accelerate compersion are two examples. Further, it has support for \gls{simd}, which enables vectorization and other performance optimizations\cite{redbook:z15}.

\subsection{POWER9}
\todo[inline]{todo}
% \gls{power} finns definierat

\subsection{CPACF}
\label{subsection:cpacf}
\gls{cpacf} is an on-chip coprocessor integrated into all cores on the z15 that assists in cryptographic functions, delivering high speed, low latency encryption, decryption, hashing, and random number generation. It supports multiple variants of \gls{des}, \gls{aes}, \gls{sha}, and \gls{shake}. In this thesis, we will be using an \gls{openssl} engine that has support for \gls{cpacf}\cite{redbook:z15}.

\subsection{CryptoCards}

CryptoCards are IBM's cryptographic \glspl{hsm}. A \gls{hsm} is a physical device placed into a computer to deliver high throughput for cryptographic functions. The devices are designed to be physically secured from tampering and their firmware may be cryptographically verifiable\cite{ibm:hsms}.

CryptoCards are available for \gls{ibmz}, \gls{power} and \gls{x86}, depending on the chosen \gls{hsm}.

At the time of writing, the newest card for \gls{ibmz} and \gls{x86} is the CEX7S / 4769\cite{ibm:4769}. The newest card for the \gls{power} platform is the CEX5S / 4767\cite{ibm:4767}.

Common to both cards is support for both \gls{fips} 140-2 Level 4. The CEX7S / 4769 is also certified according to the \gls{pci-payment} \gls{hsm} standard\cite{ibm:4769,ibm:4767}.

The two cards highly accelerate the use of common cryptographic algorithms such as \gls{aes}, \gls{sha3} and \gls{dh}\cite{ibm:4767,ibm:4769}.