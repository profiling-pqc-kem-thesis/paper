\chapter{Related Work}
\label{chapter:related-work}

In the following sections, related work for this thesis is discussed. First, work related to our experimental study is presented and discussed. Then, we present information on the underlying primitives and algorithms, as well as potential optimizations as outlined in our literature study.

\section{Experimental Performance Studies}

\noindent In~\cite{viet2020} Dang et. al. the second round of submissions to the \gls{nist} standardization process were evaluated in terms of performance. They analyzed six lattice-based \gls{fpga} and \gls{asic} implementations, as well as 12 software-based \gls{kem} implementations on hardware platforms as well as submissions using a software/hardware co-design. They did not focus their performance benchmarks on software implementations. As we solely aim to evaluate the performance of the \gls{post-quantum} \glspl{kem} in software, our aim is different from theirs. Furthermore, we will evaluate the performance of the third round of submissions, which represent almost a year of further progress. Not all algorithms represented in the second round made it through to the third round. Furthermore, several algorithms were changed and some algorithms merged to one. In their work, Dang et. al. did not present data on the software performance of all of the algorithms present in the third round of submissions. For example, the \gls{mceliece} submission is not presented. Furthermore, Dang et. al. only run the implementations on a single \gls{x86} processor which makes the results less generalizable for processors of different generations and architectures. The method used to measure the performance of the software-based algorithms provided a simplified view of the performance as it only takes the elapsed time and cycles into account. We believe a more in-depth method of measurement based in accurate hardware-based counters may provide a more detailed and whole picture.

In~\cite{chikouche2018} the aim of the authors was to evaluate the performance of various \gls{post-quantum} public-key schemes for constrained-resources smart mobile devices in terms of computational time, required memory and power consumption. Though a public-key encryption scheme may be converted into a \gls{kem}, their work does not cover such topics. As the main purpose of the \gls{nist} standardization purpose and our work is to study \gls{post-quantum} \glspl{kem}, we believe one needs to focus on the \glspl{kem} themselves, rather than the underlying schemes.

In~\cite{kumar2020}, Kumar and Pattnaik discussed the underlying mathematics of the third-round submissions NewHope, Frodo, \gls{ntru}, \gls{kyber}, \gls{saber} and \gls{mceliece}. They further described the classical algorithms \gls{dh} and \gls{ecdh}. Their focus on the mathematics and fundamental performance costs provided an up-to-date and low level comparison of the various \glspl{kem}. Given their focus on the algebraic constructs, they did not investigate the practicalities of running the algorithms on real hardware. We believe that it is important to understand the underlying reasons for performance differences found in the submissions, but in order to provide a full understanding on the readiness of today's hardware one should study the practical performance when run in various environments.

In~\cite{vambol2017}, Vambol et. al. evaluated two \gls{post-quantum} public-key schemes - McEliece and Niederreiter. As the work was published before the round one \gls{nist} submissions were released, the work does not use the algorithms that are likely to be standardized. At the time of writing it has been almost four years since the work of Vambol et. al. was published. Since then, a lot of progress has been made in the potential future algorithms - as seen in the various rounds of submissions to \gls{nist}. Furthermore, Vambol et. al. focused on the characteristics of the underlying cryptosystem for the proposed \gls{mceliece} \gls{kem}, as such it does not provide a fair view of the performance characteristics of the \gls{kem} itself, which we are interested in. The authors also focused on computational complexity and parameter sizes of the cryptosystems. Though these topics are relevant and interesting, they do not constitute enough evidence of the performance characteristics of the \gls{post-quantum} \glspl{kem} on various hardware, such as the \gls{z15} mainframe.

\section{Literature on Post-Quantum Characteristics}
\label{section:related-work:post-quantum-characteristics}

\todo[inline]{
https://ieeexplore-ieee-org.miman.bib.bth.se/stamp/stamp.jsp?tp=\&arnumber=9286147

Om vi vill fördjupa bakgrunden - kring algebran etc. för alla submissions
Kanske resultat kring matematiken för krypton?
}

% Constant-time KEMs
In~\cite{bernstein2018}, Bernstein describes the fundamentals of a \gls{kem} in that a correct public-key encryption is applied to a random input to obtain a ciphertext. Bernstein further describes that it is a standard discipline to avoid data flow from secrets to array indices and branch conditions. Although it is simple to construct a valid key generation and encapsulation for a \gls{kem}, it may be difficult to provide constant-time decapsulation.
%%% === THERE IS A LARGE CHANGE IN TOPICS HERE === %%%
Some \gls{x86} processors support the \gls{aes-instruction-set} instruction set~\cite{alkim2016}. This instruction set speeds up \gls{aes}-related operations and may provide cryptosystems with a high-performance choice for noise sampling. As previously explained in section \ref{section:background:simd-avx}, \gls{x86} also supports the \gls{simd} instruction set \gls{avx}. Sinha Roy~\cite{sinha2019} notes that on high-end platforms, \gls{simd} instructions such as those found in \gls{avx2} only provide a limited speedup. On Intel hardware, Sinha Roy measured a 1.5 times higher throughput of an optimized \gls{saber} variant, even though the algorithm was expected to achieve nearly four times higher throughput. One reason behind the identified performance bottleneck is the overhead of vector processing. With improved computer architectures, this overhead can be expected to become lower. Bernstein~\cite{bernstein2014} further highlights four problems with Intel's \gls{x86} instruction set that make it hard to implement cryptographic algorithms securely. The first problem is that they are not committed to provide an instruction set that does not change in a way that can break cryptographic functions. For example, they do not commit to keep instructions constant-time. This is even more important in vector operations. The second problem is that integer vector multiplication is limited to 25 bits. He wants to be able to use the 53-bit multiplier. The third is that the instruction set does not support a vector version of the CARRY instruction. The last problem he highlights with Intel is that they do not supply any documentation on pipelines.

% SHA-3 i ARMv8 - få ihop med arkitekturer på något vis?
%The reliance on fast \gls{sha3} operations has been answered in the form of added hardware support of \gls{sha3} in ARMv8~\cite{kyber2021}.

% code-based?
% code-based?
% code-based?

% Code-based / McEliece operations
In~\cite{wang2018}, \gls{mceliece} is described to make use of matrix multiplication for encryption. As for key generation, \gls{mceliece} relies heavily on the computation of a random permutation of selected field elements. Besides permutations, matrix-related algorithms such as Gaussian elimination is used during key-generation~\cite{mceliece2020}. An operation destinct to the code-based \gls{mceliece} submission is the operation of finding the unique codeword in the Goppa code at a certain distance~\cite{mceliece2020}.

% lattice-based???
% lattice-based???
% lattice-based???

% Gaussian, binomial samplig. AVX2 körs med uint16_t
In~\cite{alkim2016}, it is said that one of the inefficiencies of lattice-based cryptosystems stems from the misconception that high-quality Gaussian noise is crucial for encryption based on learning with errors. Using this Gaussian noise was found to make implementations slower and more complex than they have to be. The Gaussian sampler is also hard to protect against timing attacks. Therefore one may use a centered binomial distribution instead. This distribution may be further optimized by applying vector instructions such as those found in \gls{avx2}. In the case of the New Hope cryptosystem and \gls{avx2}, the operations are carried out on unsigned 16-bit integers.

%% NTT is common in lattice-based crypto. Encoding is important.
To increase the performance of polynomial operations and the performance of lattice-based cryptosystems as whole, one may turn to \glspl{ntt}~\cite{alkim2016}. \gls{ntt} provides an efficient way of multiplying polynomials. The \gls{ntt}, which is a generalization of \gls{fft}, has an asymptotically-fastest time-complexity of $\mathcal{O}(n\log{}n)$~\cite{roy2020}. In~\cite{alkim2016} the bottleneck of the \gls{ntt}-operations was found to be the butterfly operations - each consisting of one addition, one subtraction, and one multiplication by a pre-computed constant. Another performance-sensitive topic is the use of encodings of polynomials in byte arrays~\cite{alkim2016}. There has been research on exchanging this encoding with another encoding that is particularly well-suited for \gls{ntt}-based polynomial multiplication.

% NTTs state-of-the-art performance using AVX2. ARMv8 has hardware support for SHA3
Roberto et. al.~\cite{kyber2021} argue that \glspl{ntt} are extremely efficiently vectorizable on large processors. State-of-the-art performance may be achieved by carefully optimizing \glspl{ntt} using \gls{avx2} integer instructions.

%% AVX2 - 256-bit multiply four doubles. Problem in Intel - only 16 available SIMD registers. Many load / store (more cache more better?)
The importance of \gls{simd} instructions such as \gls{avx} is further established in~\cite{guneysu2013}. By applying a correct representation of the polynomials used in lattice-based cryptosystems, one may utilize the 256-bit wide \gls{avx2} registers to represent four double-precision floats. In the case of \gls{avx}, one may perform one double-precision-vector multiplication and one addition every cycle. It is further established that the main bottleneck might not always be the arithmetic cost. As only 64 polynomial coefficients fit into the 16 available registers in \gls{avx2} - many additional loads and stores are often necessary. The performance of these loads and stores is also more complex to determine when compared to the arithmetic throughput.

% SABER 
% SABER 
% SABER 
% SABER 
%%% === THERE IS A LARGE CHANGE IN TOPICS HERE === %%%
The \gls{post-quantum} \gls{kem} \gls{saber} is designed to be serial by nature~\cite{sinha2019}. The design was chosen to attain simplicity and efficiency on constrained devices. The \gls{saber} algorithm relies heavily on the pseduo-random number generation implemented using \gls{shake}-128, which also occupies a significant portion of \gls{saber}'s execution time~\cite{sinha2019}. Some measurements suggest that around 50-70\% of the overall computation time is spent generating pseudo-random numbers~\cite{saber}.

In~\cite{zhu2021}, Zhu et. al. discuss the performance of cryptosystems based on learning with rounding, such as \gls{saber}. In \gls{saber}, one of the main computational bottlenecks is the polynomial multiplication, which can not be accelerated by using the \gls{ntt} fast multiplication algorithm. This is due to \gls{saber} using a power-of-two modulo and \glspl{ntt} requiring a prime modulo for ciphertexts. When it comes to hardware implementations, it is therefore important to discuss how one may efficiently implement polynomial multiplication without using \gls{ntt}. One may exchange \gls{ntt} with Toom-Cook and Karatsuba algorithms. However, high-speed implementations of Toom-Cook multiplication have been found to add additional overhead. For implementations in hardware, a Karatsuba algorithm may be adopted for accelerating learning with rounding, as found in \gls{saber}. In one implementation, it was found that a 100Mhz hardware implementation required roughly $5.2\mu s$ for encapsulating a key, which was found to be 14 times faster than implementations on a more conventional Intel Core i7 processor. Further efforts have been made to implement \gls{saber} in hardware~\cite{roy2020}. In~\cite{roy2020}, a high-speed instruction-set coprocessor for lattice-based \glspl{kem} is presented. Just like Zhu et. al.~\cite{zhu2021}, Sinha Roy et. al.~\cite{roy2020} also identified that polynomial multiplications plays a performance-critical role in lattice-based public-key cryptography.

% KYBER
% KYBER
% KYBER

% Mjukvara kan NTTs vara bättre än Toom och Karatsuba. Prestandan bygger mer eller mindre på SHA-3. Hashing halva tiden i Kyber
The \gls{post-quantum} \gls{kem} \gls{kyber} uses \glspl{ntt} as an efficient way of performing multiplications of polynomials in $\mathbb{R}_q$~\cite{kyber2021}. In order to increase the performance of the factors when performing calculations using \gls{avx} \gls{simd} instructions, one may apply bit-reversed order. \gls{kyber} further uses SHA3-256, \gls{sha3}-512, \gls{shake}-128 and \gls{shake}-256. The use of \glspl{ntt} have some advantages over Karatsuba and Toom multiplication algorithms. \glspl{ntt} are extremely fast and do not require additional memory. \glspl{ntt} may also be implemented in very little code. The performance of \gls{kyber} is largely bound to the performance of the symmetric primitives of the \gls{sha3} family of algorithms. The hash computations are considerably less performant than the polynomial arithmetic found in \gls{kyber} when run on recent Intel processors. In an \gls{avx2}-optimized \gls{kyber} implementation, the hashes constitute almost half of the encapsulation cycles.

% One may use AES och KECCAK for random number generation, depending on hardware support
In \gls{kyber}, the choice of random number generation during key generation is considered a local decision that any user may make independently of one another~\cite{kyber2021}. On platforms where there is hardware support for \gls{aes}, one may adapt \gls{aes}-based generators. On platforms with support for \gls{keccak}-based algorithms, one may instead rely on them for pseudo-random numbers.

% Hardware
% Hardware
% Hardware

%%% === THERE IS A LARGE CHANGE IN TOPICS HERE === %%%
%% Hårdvara good, software bad. Coprocessor should work. 
In~\cite{roy2020} it is said that there are two general methodologies to implement computationally intensive cryptographic algorithms in hardware. A hardware/software codesign offers a shorter design cycle and higher flexibility, but it may not result in the best performance. The best performance is found in a full-hardware design. By using a instruction-set coprocessor architecture for \gls{saber}, the authors achieved programmability and flexibility, which allowed them to easily extend the instruction set and adapt the architecture for other algorithms and tasks. The architecture followed best practices by making the implementation constant-time. On platforms with support for \gls{simd} instructions such as those found in \gls{avx2}, the cost of pseudo-random number generation is reduced by vectorizing the implementation by a factor of four. As \gls{keccak} is very efficient on hardware platforms, one may run the algorithm in parallel.