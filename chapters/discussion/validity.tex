\section{Threats to Validity}

\subsection{Conclusion validity}

\todo[inline]{Add Conclusion validity}

\subsection{Internal validity}
\label{section:method:internal-validity}
% History - An unrelated event influences the outcomes.
Other software running on the OS may influence the results. To prevent this, we minimized the set of programs running on the system and ran the experiment multiple times at different occasions to decrease the risk of indexing tasks etcetera running on the system.

Linux was used for many reasons. One of them is that it does not have any background tasks like Windows update that can significantly impact the results. It is unknown, however, if there are large indexing tasks or the like which may have interfered the results.

% Maturation - The outcomes of the study vary as a natural result of time.
Another factor, \gls{jit} compilation and garbage collection could have influenced the results. As we used the programming languages C, Assembly and Go, this will not be a problem as they do not use \gls{jit} compilation. Other programs running on the system which use \gls{jit} compilation or garbage collection falls under the previous category and will be handled in the same way.

The post-quantum implementations have not been finalized nor standardized, which may have affected the performance of the implementations and the relevance of the results in the future. \gls{dhe} and \gls{ecdhe} have been standardized and optimized for many years and have mature implementations, meaning they are likely fully optimized by this point. We therefore do not believe that our performance measurements reflect the eventual state of the \gls{post-quantum} algorithms, which should be taken into account when comparing values.

% Instrumentation - Different measures are used in the pre-test and post-test phases.
We have written some of the measurement and visualization tools used in this work. The implementations may have shortcomings and OS differences may lead to different results. To ensure the correctness and consistency between platforms we both have reviewed the code and tested it on different platforms to ensure it was working correctly. When using third-party tools, we validated the results using other software to ensure that there were no discrepancies without a natural cause.

% Testing - The pre-test influences the outcomes of the post-test.
Hardware can throttle because of the temperature increase caused by the benchmarks. This could result in worse performance than intended when running consecutive tests. To mitigate this, benchmarks were not be run directly after each other. We decided to wait between tests. We initially hoped to be able to wait for the temperature of the system to normalize to the same level it was before the test was started. As we have performed several hundreds of benchmarks in each environment, with several thousand algorithm invocations per benchmark, we were however unable to wait more than one minute between benchmarks. Given more time, we believe one may have gotten more reliable data if the wait between tests was longer. However, we did perform some initial tests which showed that one minute wait was mostly enough to lower the temperature of the system after running a benchmark. We therefore do not believe that our results have been impacted noticeably by any throttling, unless noted. 

We could not use the same OS for all of our environments, which could cause varying results. This should however not be an issue. We do not aim to pit \gls{x86} against \gls{z15} in terms of hardware or software, rather represent a type of computer and evaluate their readiness. Furthermore, we could not ensure the same version of libraries and compiler such as GCC, Clang and OpenSSL. We provide full transparency in the software versions used and data accumulated to help interested parties further study our results and correlate differences in versions and performance.

% Selection bias - Groups are not comparable at the beginning of the study.
One may argue that our choice of studying the two post-quantum algorithms \gls{ntru} and \gls{mceliece} was biased, but we argue that that point is invalid or at the very least simplified. The algorithms were selected from four finalists in round 3 of \gls{nist}'s standardization process. \gls{ntru}, \gls{kyber}, and \gls{saber} are all lattice-based algorithms, and at most one of these will be standardized \cite{nist2020}. \gls{ntru} was selected based on the feedback \gls{nist} gave each of the participants. \gls{mceliece} was the only non-lattice-based finalist. It also has a long and good reputation \cite{nist2020}. In addition to these four finalists, there are alternate candidates still in round 3. These were not considered. The choice was made to bring down the scope of this work to a manageable level. Given more time, we believe that our method would scale well with an additional set of algorithms. Our tooling and procedures were created with the potential of analyzing further algorithms in mind.

As has been mentioned previously, we rely on \gls{nist} recommendations as they in many cases provide the authoritative recommendation of algorithms used by protocol implementers \cite{nist:round-three-submissions, nist2019}. We also used work from PQCRYPTO \cite{eu2015} to back up some of \gls{nist}'s recommendations. As we have not identified any other standardization process like that of \gls{nist}, we have concluded that relying on their expertise in this context is correct.

For the implementations of \gls{dhe} and \gls{ecdhe}, we only use the underlying algorithms provided by \gls{openssl}. This was done as \gls{openssl} was identified as the main library for these algorithms on the tested platforms. Using other libraries such as BoringSSL might have resulted in different measurements. The implementations available in \gls{openssl} have been rigorously tested and analyzed by the industry over the decades it has seen use. We are therefore confident that, although the exact measurements may differ between libraries, \gls{openssl} provides a solid foundation for our use case as a representation of today's algorithms.

%Attrition - Dropout from participants
% Not applicable? Perhaps all subjects and platforms will not be able to see all optimizations or compiler flags?

\subsection{Construct validity}

% Extent to which the experiment setting actually reflects the construct under study. Treatment reflects the construct of the cause well. Output reflects the construct of the effect well

% Construct validity evaluates whether a measurement tool really represents the thing we are interested in measuring. It’s central to establishing the overall validity of a method.

As previously mentioned in section \ref{section:method:experiment:phase1:variables}, we were interested in measuring throughput-related values such as CPU cycles, instruction count, wall-clock time as well as memory-related measurements such as heap and stack usage. For our measurements, we relied on the standard Linux kernel-based API named perf (perf\_event\_open). The API was introduced in Linux 2.6.31 which was released in 2009 \cite{linux:perf-released}. The API has grown, and as is tradition with the Linux development, each iteration of the API has been reviewed extensively by multiple people throughout the years. We are confident that the API provides as accurate data as the kernel is able to collect. To make the API usable, we used a lightweight instrumentation tool\footnote{\url{https://github.com/profiling-pqc-kem-thesis/perforator}} which allowed us to use the perf API to measure events for specific regions of code. As with other third-party tools, we validated its function by comparing the results to other tools. By using Linux trace APIs to monitor the target binary, we were able to insert measurements around a function call by interrupting the program of the measurement tool. As the target program was frozen during the handling of these measurements, we strongly believe that no overhead added by the measurement tool was included in the end result. By running the instrumented benchmark separately from the benchmarks measuring wall-clock time or memory allocation, we are certain that we achieved accurate values for all of our measurements.

When studying the data amassed after applying our toolset for micro-benchmarks, we found that the value 9223372036854775808 occurred a considerable amount of times. As it was considerably larger than other values and since we were not expecting similar values for completely different events, we analyzed the fault. Given size of the problem space, we were unable to identify the root cause. We found that 0.7\% of the values recorded were affected by this issue and that it likely originates in an incorrect handling of unsigned 64-bit integers as the value is one higher than the maximum number a signed 64-bit integer may store. In order to clarify the error, we marked the data and ignored them in the data presented in this thesis. Given the low number of affected measurements, we feel confident in our handling of these errors. One measurement that did show a considerable amount of errors, however, is those for the region syndrome\_asm. The measurements for the region consisted of 33\% of these erroneous measurements. Other regions consisted of about 2\% errors. All of our data is published alongside this work for further verification efforts from third parties.

% Number of errors:
%     330 gen_e
%     339 poly_R2_inv
%     164 poly_R2_inv_to_Rq_inv
%      49 poly_Rq_inv
%   2293 poly_Rq_mul
%      56 poly_S3_inv
%     713 poly_Sq_mul
%     281 randombytes
%      24 root
%      50 syndrome
%     522 syndrome_asm

% Total amount:
%   16048 gen_e
%   12036 poly_R2_inv
%     4012 poly_R2_inv_to_Rq_inv
%   12036 poly_Rq_inv
%   204504 poly_Rq_mul
%   12036 poly_S3_inv
%   24036 poly_Sq_mul
%   36108 randombytes
%   30673 root
%     8024 syndrome
%     1578 syndrome_asm

\subsection{Content validity}

% Refers to the extent to which a measure represents all facets of a given construct.
%"refers to the degree to which an assessment instrument is relevant to, and representative of, the targeted construct it is designed to measure."

To answer \researchquestion{2}, we did not need to measure any function beyond the main algorithm invocation. This since the algorithm's usage included all of the system's parts. As such, measuring the time, CPU cycles and total instructions of the entire system should have sufficed.

To help answer \researchquestion{1} and \researchquestion{3} we examined optimizations of different parts of the algorithm. We needed to see how the parts in the system as a whole were performing. For this, we used micro-benchmarks. Micro-benchmarks may add a non-trivial overhead that depends on the number of benchmarks. We evaluated the impact of these. We performed the experiment once with and once without the micro-benchmarks to see the difference and make sure it is not statistically significant. As there were not a significant amount of overhead, each micro-benchmark did not need to be run in isolation.

When measuring memory usage, we did not measure any other performance metric as it could have induced a higher memory load unrelated to the algorithm under test. Instead, we performed the measurement of the memory usage of the key-pair generation, encryption, and decryption separate from the CPU measurements.

To measure the wall-clock duration of the algorithms, we used the highest resolution monotonic clock available in each system. We further disabled all other measurements when measuring the time in order to minimize the risk of interference.

\subsection{Criterion validity}

In the publications for the \gls{nist} submissions, the authors have written their own performance analysis using the \gls{supercop} benchmark tool. The presented measurements are for the total number of cycles used by the algorithms for generating keys, encapsulation and decapsulation. As these figures have not been validated by a third party, we did not use them to validate ours. We did however present comparisons in order to identify potential issue in their measurements or ours.

\subsection{External validity}

% Sampling bias.
As previously discussed under section \ref{section:method:internal-validity}, one may argue that the selection of subjects was biased as we do not take the entire population of post-quantum and classical algorithms into account. Such a comparison would however be unfeasible. That is why the presented sample is based on the accumulative recommendations of several organizations. We therefore argue that our sample is representative of the algorithms that are and likely will be in use.

% History.
Another factor that may hurt the generalizability of the results is the potential of a series of unrelated events influencing the outcome. We have identified several actions to help mitigate this risk, as discussed further in section \ref{section:method:internal-validity}.

% Experimenter effect.
The implementations used for benchmarks were not written by us. They were, however, slightly altered in order to support various forms of optimization. This fact may result in the tested implementations performing differently than if the original implementers would have applied the optimizations. It is therefore plausible that the measurements of the samples will not be general to other implementations using the same techniques.

% Aptitude treatment.
As there were several optimization techniques such as vectorization and compiler flags applied simultaneously, there is a potential that techniques canceled each other out or in other ways impacted the performance negatively. We mitigated this risk by evaluating each form of optimization in isolation, before we combined all techniques into a final optimized implementation. Due to time constraints, we were however unable to apply the same method for Clang. Clang was never used to build the non-optimized variants, only those with all the chosen optimization flags set. We do not believe that using Clang without optimization flags would yield any results which would have the potential to invalidate our results. Given more time, we do however believe it would make for an interesting research topic.

% Situation effect.
Factors such as various settings, time of day and location may have limited the generalizability of the presented findings. We have identified several mitigative actions as defined in section \ref{section:method:internal-validity}.

% Counter threats.
To counter threats across the experiment we aimed to improve replication of the results by enabling others to carry out the experiment on their own. This was done by providing detailed methodology, the used tools and any accumulated data. The data, tools and the tested implementations used are available as open source\footnote{\url{https://github.com/profiling-pqc-kem-thesis}}.

\iffalse
\todo[inline]{
Vi mäter bara Intel x86, ingen AMD. Svårt att generalisera då?
}

% == 1 ==
% Medium?
\todo[inline]{
Vi behövde köra om micro en jäkla massa gånger - främst på gammal / "low-end" hårdvara. Low-end-laptop fick inte komplett data trots många omkörningar.
}

% Låg?
\todo[inline]{
only a single run of callgrind on mceliece and ntru's tests. mceliece is non-deterministic and may behave weird in a single run.
}

% Lågprioriterad - poteniell risk att history-valididet blivit sämre? Pauser mellan borde löst det
\todo[inline]{Validity: vänd på körschemat när man kör så att inte NTRU får köras på natten varje gång - utan att sådana saker slås ut.}

% Low?
\todo[inline]{Svårt att mäta på IBM och i molnet?}

% Låg?
\todo[inline]{diskutera hur benchmarks inte säkerställer att rätt svar ges från algoritmerna. Detta utelämnas p.g.a. prestanda / träffsäkerhet i mätningar. Vi löser det genom att ha tester som är samma kod som benchmarken, fast med validering}

% Låg?
\todo[inline]{
-- Dedicated hardware for mainframes may behave differently
}

% Låg? SSD på alla system. Tillräckligt med RAM, det gick aldrig lågt
\todo[inline]{Swap locks the system while writing/reading to from disk}

% Låg? Conclusion validity? Kan vi anta att våra resultat generaliseras om vi bara testat tre algoritmer (HRSS, HPS, Classic McEliece).
\todo[inline]{We have only selected a subset of potential optimizations?}
\fi