\section{Threats to Validity}

\todo[inline]{ diskutera hur benchmarks inte säkerställer att rätt svar ges från algoritmerna. Detta utelämnas p.g.a. prestanda / träffsäkerhet i mätningar. Vi löser det genom att ha tester som är samma kod som benchmarken, fast med validering}

\subsection{Conclusion validity}

\todo[inline]{Add Conclusion validity}

\subsection{Internal validity}
\label{section:method:internal-validity}
% History - An unrelated event influences the outcomes.
Other software running on the OS may influence the results. To prevent this, we will minimize the set of programs running on the system and run the experiment multiple times at different times.

% Maturation - The outcomes of the study vary as a natural result of time.
Another factor, \gls{jit} compilation and garbage collection could influence the results. As we use the programming languages C and Assembly this will not be a problem as they do not use \gls{jit} compilation and provide full control of memory. Other programs running on the system which use JIT compilation or garbage collection falls under the previous category.

\todo{GPU kernel \gls{jit} compilation}

\todo{Swap locks the system while writing/reading to from disk}
\todo{Cache misses}

\todo{Windows Update?}

The post-quantum implementations are not final nor standardized, which may affect the performance of the implementations and the relevance of the results in the future. \gls{dhe} and \gls{ecdhe} have been standardized and optimized for many years and have mature implementations, they are likely fully optimized by this point.

% Instrumentation - Different measures are used in the pre-test and post-test phases.
We have written the measurement tools, the implementation may have shortcomings and OS differences may lead to different results. To ensure the correctness and consistency between platforms we both have reviewed the code and tested it on different platforms\todo{??????}.

%Testing - The pre-test influences the outcomes of the post-test.
Hardware can throttle because of the temperature increase caused by the benchmarks. This could result in worse performance than intended when running consecutive tests. To mitigate this, benchmarks will not be run directly after each other. We will instead wait for the temperature of the system to normalize to the same level it was before the test was started.

We cannot necessarily use the same OS for the mainframes and x86, which could cause varying results. This is however not an issue. We do not aim to pit x86 against Z, rather represent a type of computer and evaluate their readiness.

%Selection bias - Groups are not comparable at the beginning of the study.
One may argue that the choice of the two post-quantum algorithms is biased, but we argue that that point is invalid. The algorithms were selected from four finalists in round 3 of \gls{nist}'s standardization process. \gls{ntru}, \gls{crystals-kyber}, and \gls{saber} are all lattice-based algorithms, and at most one of these will be standardized. \gls{ntru} was selected based on the comment \gls{nist} gave each of the participants\todo{insert the comment?}. \gls{mceliece} was the only non-lattice-based finalist. It also has a long and good reputation\cite{nist2020}. In addition to these four finalists, there are alternate candidates still in round 3. These were not considered. The choice was made to bring down the scope to a manageable level for this work.

As has been mentioned previously, we lean heavily on \gls{nist} recommendations as they in many cases provide the authoritative recommendation of algorithms used by protocol implementers (\todo{to some extent EU paper as well????}). As we have not identified any other standardization process like that of \gls{nist}, we have concluded that relying on their expertise in this context is correct.

For the implementations of \gls{dhe} and \gls{ecdhe}, we only use the underlying algorithms provided by \gls{openssl} and \gls{openssl} with the IBM engine. This was done as \gls{openssl} was identified as the main library for these algorithms on the tested platforms. Using other libraries such as BoringSSL might have resulted in different measurements. The implementations available in \gls{openssl} have been rigorously tested and analyzed by the industry over the decades it has seen use. We are therefore confident that, although the exact measurements may differ between libraries, \gls{openssl} provides a solid foundation for our use case as a representation of today's algorithms.

\todo{We have only selected a subset of potential optimizations?}

%Attrition - Dropout from participants
\todo{Not applicable? Perhaps all subjects and platforms will not be able to see all optimizations or compiler flags?}

\subsection{Construct validity}

% Extent to which the experiment setting actually reflects the construct under study. Treatment reflects the construct of the cause well. Output reflects the construct of the effect well

% Construct validity evaluates whether a measurement tool really represents the thing we are interested in measuring. It’s central to establishing the overall validity of a method.

As previously mentioned, we are interested in measuring the following values:

\todo[inline]{These do not correspond to the ones we actually measure? Update. Also; clarify that we're measuring two different things - single iteration and multi-core.}
\begin{itemize}
    \item Encapsulation throughput in number of required CPU cycles, instruction count and wall clock time
    \item Decapsulation throughput in number of required CPU cycles, instruction count and wall clock time
    \item Keypair throughput in number of required CPU cycles, instruction count and wall clock time
    \item Hardware utilization in terms of memory (heap and stack allocation)
    \item Hardware utilization in terms of processing (logical core utilization)
\end{itemize}

For our measurements, we rely on the standard Linux kernel-based API named perf (perf\_event\_open). The API was introduced in Linux 2.6.31 which was released in 2009\footnote{\href{https://www.linux.com/news/linux-2631-released/}{https://www.linux.com/news/linux-2631-released/}}. The API has grown, and as is tradition with the Linux development, each iteration of the API has been reviewed extensively by multiple people throughout the years. We are confident that the API provides as accurate data as the kernel is able to collect.

To make the API usable, we provide a lightweight instrumentation tool which is used to measure the values as listed above. By using the C pre-processor, usage of the tool is readable and configurable and written to not introduce any overhead over the raw Linux API when performing measurements. We therefore argue that the tool does not introduce any additional risks over the base API.

By reading the official documentation of the API, we have ensured that the values we measure correspond to those we are interested in. That is, when we use the API to measure CPU cycles, for example, we have asserted that our code indeed refers to the correct measurements as described in the documentation.

\subsection{Content validity}

% Refers to the extent to which a measure represents all facets of a given construct.
%"refers to the degree to which an assessment instrument is relevant to, and representative of, the targeted construct it is designed to measure."

To answer our first research question, \textit{Does performance of post-quantum cryptography algorithms differ between architectures and if so, how?},  we do not need to measure anything other than at the start and end of the algorithm invocation. This, since the algorithm's usage will include all of the system's parts. As such, measuring the time, CPU cycles and total instructions of the entire system should suffice.

For our other research questions we examine optimizations of different parts of the algorithm. We need to see how the parts in the system as a whole are performing. For this, we will use micro-benchmarks. Micro-benchmarks may add a non-trivial overhead that depends on the number of benchmarks. We will evaluate the impact of these. We will perform the experiment once with and once without the micro-benchmarks to see the difference and make sure it is not statistically significant. If there is a significant amount of overhead, each micro-benchmark will be run in complete isolation.

When measuring memory usage, we cannot measure any other performance metric as it could induce a higher memory load unrelated to the algorithm under test. Instead, we perform the measurement of the memory usage of the key-pair generation, encryption, and decryption separate from the CPU measurements.

\todo{Measurement issues with perf - overflow or negative numbers in uint64}

\subsection{Criterion validity}
In the publications for the \gls{nist} submissions, the authors have written their own performance analysis using the \gls{supercop} benchmark tool. The presented measurements are for the total number of cycles used by the algorithms for generating keys, encapsulation and decapsulation. As these figures have not been validated by a third party, we will not use them to validate ours. We will however present comparisons in order to identify potential issue in their measurements or ours.

\subsection{External validity}

% Sampling bias.
As previously discussed under section \ref{section:method:internal-validity}, internal validity, one may argue that the selection of subjects is biased as we do not take the entire population of post-quantum and modern-day algorithms into account. Such a comparison would however be unfeasible. That is why the presented sample is based on the accumulative recommendations of several organizations. We therefore argue that our sample is representative of the algorithms that are and likely will be in use.

% History.
Another factor that may hurt the generalizability of the results is the potential of a series of unrelated events influencing the outcome. We have identified several actions to help mitigate this risk, as discussed further in section \ref{section:method:internal-validity}, internal validity.

% Experimenter effect.
The implementations used for benchmarks are not created by us. They have, however, been slightly altered in order to support various forms of optimization. This fact may result in the tested implementations performing differently than if the original implementers would have applied the optimizations. It is therefore plausible that the measurements of the samples will not be general to other implementations using the same techniques.

% Aptitude treatment.
As there are several optimization techniques such as vectorization and compiler flags applied simultaneously, there is a potential for techniques to cancel each other out or in other ways impact the performance negatively. We will mitigate this risk by evaluating each form of optimization in isolation, before combining all techniques into a truly optimized implementation.

% Situation effect.
Factors such as various settings, time of day and location may limit the generalizability of the presented findings. We have identified several mitigative actions as defined under \ref{section:method:internal-validity}, internal validity.

% Counter threats.
To counter threats across the experiment we aim to improve replication of the results by enabling third parties to carry out the experiment on their own. This is done by providing detailed methodology, the used tools and any accumulated data. The data, tools and the tested implementations used are available as open source\footnote{\href{https://github.com/profiling-pqc-kem-thesis}{https://github.com/profiling-pqc-kem-thesis}}.

\todo[inline]{Perforator, perf lib, go, perf eller något verkar ibland returnera -1 vilket ger talet 9223372036854775808 (int64). Detta är antagligen en bugg i verktygen som vi bortsåg ifrån i resultaten. Vi dubbelkollade så att vi säkert kunde ta bort resultaten utan att det påverkade statistiken. Totalt verkade det utgöra 0.7\% av värdena.

Data från körning på workstation:

Antal "fel":
    330 gen_e
    339 poly_R2_inv
    164 poly_R2_inv_to_Rq_inv
     49 poly_Rq_inv
   2293 poly_Rq_mul
     56 poly_S3_inv
    713 poly_Sq_mul
    281 randombytes
     24 root
     50 syndrome
    522 syndrome_asm

Totalt antal:
  16048 gen_e
   12036 poly_R2_inv
    4012 poly_R2_inv_to_Rq_inv
   12036 poly_Rq_inv
  204504 poly_Rq_mul
   12036 poly_S3_inv
   24036 poly_Sq_mul
   36108 randombytes
   30673 root
    8024 syndrome
    1578 syndrome_asm
}

% ========== READ ME
%\todo[inline]{
%only a single run of callgrind on mceliece and ntru's tests. mceliece is non-deterministic and may behave weird in a single run.
%}


%\todo{Validity: vänd på körschemat när man kör så att inte NTRU får köras på natten varje gång - utan att sådana saker slås ut.}
%\todo{Shell-script som kör alla tester - vänta 10-20 minuter mellan varje test}
%\todo{Taskigt att mäta avx på intel, men inget vektoriserat på IBM}
