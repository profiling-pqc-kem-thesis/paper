\section{Threats to Validity}

\todo[inline]{ diskutera hur benchmarks inte säkerställer att rätt svar ges från algoritmerna. Detta utelämnas p.g.a. prestanda / träffsäkerhet i mätningar. Vi löser det genom att ha tester som är samma kod som benchmarken, fast med validering}

\todo[inline]{
-- Update after result and method changes.

-- Difficult to measure in Cloud environments - shared hosting etc.

-- Dedicated hardware for mainframes may behave differently

-- Did not use Intel C++ Compiler (ICC)

-- Did not use IBM XL
}

\subsection{Conclusion validity}

\todo[inline]{Add Conclusion validity}

\subsection{Internal validity}
\label{section:method:internal-validity}
% History - An unrelated event influences the outcomes.
Other software running on the OS may influence the results. To prevent this, we will minimize the set of programs running on the system and run the experiment multiple times at different times to decrease the risk of indexing tasks etcetera running on the system.

% Maturation - The outcomes of the study vary as a natural result of time.
Another factor, \gls{jit} compilation and garbage collection could influence the results. As we use the programming languages C and Assembly this will not be a problem as they do not use \gls{jit} compilation and provide full control of memory. Other programs running on the system which use \gls{jit} compilation or garbage collection falls under the previous category and will be handled in the same way.

\todo[inline]{Swap locks the system while writing/reading to from disk}
\todo[inline]{Cache misses}

Linux is used for many reasons. One of them is that it does not have any background tasks like Windows update that can significantly impact the results.  


The post-quantum implementations are not final nor standardized, which may affect the performance of the implementations and the relevance of the results in the future. \gls{dhe} and \gls{ecdhe} have been standardized and optimized for many years and have mature implementations, meaning they are likely fully optimized by this point. We therefore do not believe that our performance measurements reflect the eventual state of the \gls{post-quantum} algorithms, which should be taken into account when comparing values.

% Instrumentation - Different measures are used in the pre-test and post-test phases.
We have written some of the measurement tools used, the implementation may have shortcomings and OS differences may lead to different results. To ensure the correctness and consistency between platforms we both have reviewed the code and tested it on different platforms to ensure it was working correctly. When using third-party tools, we validated the results using other software to ensure that there were no discrepancies without a natural cause.

% Testing - The pre-test influences the outcomes of the post-test.
Hardware can throttle because of the temperature increase caused by the benchmarks. This could result in worse performance than intended when running consecutive tests. To mitigate this, benchmarks will not be run directly after each other. We therefore decided to wait between tests. We initially hoped to be able to wait for the temperature of the system to normalize to the same level it was before the test was started. As we have performed several hundreds of benchmarks in each environment, with several thousand algorithm invocations per benchmark, we were however unable to wait more than one minute between benchmarks. Given more time, we believe one may have gotten more reliable data if the wait between tests was longer.

We cannot not use the same OS for the mainframes and x86, which could cause varying results. This should however not be an issue. We do not aim to pit x86 against \gls{z15} in terms of hardware or software, rather represent a type of computer and evaluate their readiness. We also provide full transparency in the software versions used and data accumulated to help interested parties further study our results.

% Selection bias - Groups are not comparable at the beginning of the study.
One may argue that the choice of the two post-quantum algorithms is biased, but we argue that that point is invalid or at the very least simplified. The algorithms were selected from four finalists in round 3 of \gls{nist}'s standardization process. \gls{ntru}, \gls{crystals-kyber}, and \gls{saber} are all lattice-based algorithms, and at most one of these will be standardized \cite{nist2020}. \gls{ntru} was selected based on the feedback \gls{nist} gave each of the participants. \gls{mceliece} was the only non-lattice-based finalist. It also has a long and good reputation \cite{nist2020}. In addition to these four finalists, there are alternate candidates still in round 3. These were not considered. The choice was made to bring down the scope to a manageable level for this work. Given more time, we believe that our method would scale well with an additional set of algorithms. Our tooling and procedures were created with the potential of analyzing further algorithms in mind.

As has been mentioned previously, we rely on \gls{nist} recommendations as they in many cases provide the authoritative recommendation of algorithms used by protocol implementers \cite{nist:round-three-submissions, nist2019}. \cite{eu2015} ware used to back up some of \gls{nist}'s recommendations. As we have not identified any other standardization process like that of \gls{nist}, we have concluded that relying on their expertise in this context is correct.

For the implementations of \gls{dhe} and \gls{ecdhe}, we only use the underlying algorithms provided by \gls{openssl}. This was done as \gls{openssl} was identified as the main library for these algorithms on the tested platforms. Using other libraries such as BoringSSL might have resulted in different measurements. The implementations available in \gls{openssl} have been rigorously tested and analyzed by the industry over the decades it has seen use. We are therefore confident that, although the exact measurements may differ between libraries, \gls{openssl} provides a solid foundation for our use case as a representation of today's algorithms.

\todo[inline]{We have only selected a subset of potential optimizations?}

%Attrition - Dropout from participants
% Not applicable? Perhaps all subjects and platforms will not be able to see all optimizations or compiler flags?

\subsection{Construct validity}

% Extent to which the experiment setting actually reflects the construct under study. Treatment reflects the construct of the cause well. Output reflects the construct of the effect well

% Construct validity evaluates whether a measurement tool really represents the thing we are interested in measuring. It’s central to establishing the overall validity of a method.

As previously mentioned in section \ref{section:method:experiment:phase1:variables}, we are interested in measuring throughput-related values such as CPU cycles, instruction count, wall-clock time as well as memory-related measurements such as heap and stack usage.

For our measurements, we rely on the standard Linux kernel-based API named perf (perf\_event\_open). The API was introduced in Linux 2.6.31 which was released in 2009 \cite{linux:perf-released}. The API has grown, and as is tradition with the Linux development, each iteration of the API has been reviewed extensively by multiple people throughout the years. We are confident that the API provides as accurate data as the kernel is able to collect.

To make the API usable, we used a lightweight instrumentation tool\footnote{\url{https://github.com/profiling-pqc-kem-thesis/perforator}} which allowed us to use the perf API to measure events for specific regions of code. As with other third-party tools, we validated its function by comparing the results to other tools. By using Linux By using the C pre-processor, usage of the tool is readable and configurable and written to not introduce any overhead over the raw Linux API when performing measurements. We therefore argue that the tool does not introduce any additional risks over the base API.

By reading the official documentation of the API, we have ensured that the values we measure correspond to those we are interested in. That is, when we use the API to measure CPU cycles, for example, we have asserted that our code indeed refers to the correct measurements as described in the documentation.

\subsection{Content validity}

% Refers to the extent to which a measure represents all facets of a given construct.
%"refers to the degree to which an assessment instrument is relevant to, and representative of, the targeted construct it is designed to measure."

To answer our first research question, \textit{Does performance of post-quantum cryptography algorithms differ between architectures and if so, how?},  we do not need to measure anything other than at the start and end of the algorithm invocation. This, since the algorithm's usage will include all of the system's parts. As such, measuring the time, CPU cycles and total instructions of the entire system should suffice.

For our other research questions we examine optimizations of different parts of the algorithm. We need to see how the parts in the system as a whole are performing. For this, we will use micro-benchmarks. Micro-benchmarks may add a non-trivial overhead that depends on the number of benchmarks. We will evaluate the impact of these. We will perform the experiment once with and once without the micro-benchmarks to see the difference and make sure it is not statistically significant. If there is a significant amount of overhead, each micro-benchmark will be run in complete isolation.

When measuring memory usage, we cannot measure any other performance metric as it could induce a higher memory load unrelated to the algorithm under test. Instead, we perform the measurement of the memory usage of the key-pair generation, encryption, and decryption separate from the CPU measurements.

\todo{Measurement issues with perf - overflow or negative numbers in uint64}

\subsection{Criterion validity}
In the publications for the \gls{nist} submissions, the authors have written their own performance analysis using the \gls{supercop} benchmark tool. The presented measurements are for the total number of cycles used by the algorithms for generating keys, encapsulation and decapsulation. As these figures have not been validated by a third party, we will not use them to validate ours. We will however present comparisons in order to identify potential issue in their measurements or ours.

\subsection{External validity}

% Sampling bias.
As previously discussed under section \ref{section:method:internal-validity}, internal validity, one may argue that the selection of subjects is biased as we do not take the entire population of post-quantum and modern-day algorithms into account. Such a comparison would however be unfeasible. That is why the presented sample is based on the accumulative recommendations of several organizations. We therefore argue that our sample is representative of the algorithms that are and likely will be in use.

% History.
Another factor that may hurt the generalizability of the results is the potential of a series of unrelated events influencing the outcome. We have identified several actions to help mitigate this risk, as discussed further in section \ref{section:method:internal-validity}, internal validity.

% Experimenter effect.
The implementations used for benchmarks are not created by us. They have, however, been slightly altered in order to support various forms of optimization. This fact may result in the tested implementations performing differently than if the original implementers would have applied the optimizations. It is therefore plausible that the measurements of the samples will not be general to other implementations using the same techniques.

% Aptitude treatment.
As there are several optimization techniques such as vectorization and compiler flags applied simultaneously, there is a potential for techniques to cancel each other out or in other ways impact the performance negatively. We will mitigate this risk by evaluating each form of optimization in isolation, before combining all techniques into a truly optimized implementation.

% Situation effect.
Factors such as various settings, time of day and location may limit the generalizability of the presented findings. We have identified several mitigative actions as defined under \ref{section:method:internal-validity}, internal validity.

% Counter threats.
To counter threats across the experiment we aim to improve replication of the results by enabling third parties to carry out the experiment on their own. This is done by providing detailed methodology, the used tools and any accumulated data. The data, tools and the tested implementations used are available as open source\footnote{\href{https://github.com/profiling-pqc-kem-thesis}{https://github.com/profiling-pqc-kem-thesis}}.

\todo[inline]{Perforator, perf lib, go, perf eller något verkar ibland returnera -1 vilket ger talet 9223372036854775808 (int64). Detta är antagligen en bugg i verktygen som vi bortsåg ifrån i resultaten. Vi dubbelkollade så att vi säkert kunde ta bort resultaten utan att det påverkade statistiken. Totalt verkade det utgöra 0.7\% av värdena.

Data från körning på workstation:

Antal "fel":
    330 gen_e
    339 poly_R2_inv
    164 poly_R2_inv_to_Rq_inv
     49 poly_Rq_inv
   2293 poly_Rq_mul
     56 poly_S3_inv
    713 poly_Sq_mul
    281 randombytes
     24 root
     50 syndrome
    522 syndrome_asm

Totalt antal:
  16048 gen_e
   12036 poly_R2_inv
    4012 poly_R2_inv_to_Rq_inv
   12036 poly_Rq_inv
  204504 poly_Rq_mul
   12036 poly_S3_inv
   24036 poly_Sq_mul
   36108 randombytes
   30673 root
    8024 syndrome
    1578 syndrome_asm
}

% ========== READ ME
%\todo[inline]{
%only a single run of callgrind on mceliece and ntru's tests. mceliece is non-deterministic and may behave weird in a single run.
%}


%\todo{Validity: vänd på körschemat när man kör så att inte NTRU får köras på natten varje gång - utan att sådana saker slås ut.}
%\todo{Shell-script som kör alla tester - vänta 10-20 minuter mellan varje test}
%\todo{Taskigt att mäta avx på intel, men inget vektoriserat på IBM}


% todo[inline]{
% Vi behövde köra om micro en jäkla massa gånger - främst på gammal / "low-end" hårdvara. Low-end-laptop fick inte komplett data trots många omkörningar.
%}

% todo we did not get OpenSSL working on IBM, which could have altered the results for ecdh, dh