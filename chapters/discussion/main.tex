\chapter{Discussion}
\label{chapter:discussion}

\section{Post-Quantum Cryptography on IBM Z}

As mentioned in~\cite{microsoft2020, ibm:z15:2019}, the fact that current hardware provides cryptographic agility is important as the \gls{post-quantum} standard evolves. By providing users with the ability to update the firmware of \glspl{hsm} as well as adding user defined extensions, we believe it is likely that the \gls{nist} submissions may be implemented efficiently on the \glspl{hsm} as the algorithms mature.

The IBM 4769 \gls{hsm} supports \gls{dilithium}, a lattice-based signature algorithm. The implementation is not entirely clear, but it is assumed to be implemented in the \gls{asic}. It is unclear whether or not the functions of the \gls{dilithium} implementation is split, allowing other lattice-based cryptosystems to be implemented using the acceleration offered by the \gls{asic}. If such an implementation was possible via an extension, we assume that the performance increase would be considerate when compared to software-based implementations, given the nature of the \gls{asic} implementations. Even if other lattice-based cryptosystems could not make use of the \gls{asic}, the \gls{fpga} integrated in the \gls{hsm} should in theory offer a significant speedup for \gls{post-quantum} \glspl{kem}~\cite{zhu2021, roy2020}. We believe that, due to the seemingly lack of low-level primitives for lattice-based crypto on IBM's \glspl{hsm}, it is unlikely that we would be able to see an increase in performance of the \gls{post-quantum} \glspl{kem}. Given a study of \gls{fpga} implementations, however, one could expect to see a considerable increase in throughput.

Though we were unable to utilize IBM's \gls{hsm} offerings, we believe that they would significantly increase the speed of the classical algorithms \gls{ecdhe} and \gls{dhe} as the \glspl{hsm} support these algorithms. Even without using the \glspl{hsm}, we did see a significant increase in performance for the \gls{ecdhe} due to its support for \gls{ecc} in \gls{cpacf}. In our parallel benchmarks for measuring the throughput of the classical algorithms, we saw a peak throughput of 100000 keypairs and 20000 \gls{ecdhe} exchanges when using the \gls{x25519} algorithm. We believe that these results give a glimpse of what's possible when successfully targeting the \gls{z15} platform. 

As seen in section \ref{section:results:z15}, the 4769 \gls{hsm} supports \gls{aes}, \gls{sha3} and random number generation. These algorithms were identified to be of importance when it comes to performance tuning of \gls{post-quantum} \glspl{kem}. In the case of some lattice-based \glspl{kem} it was found that a majority of the computation time was spent using \gls{sha3} functions. We therefore believe that by utilizing these primitives as drop-in replacements in the studied implementations, one could expect a significant performance increase. We were not able to put this theory to the test as we were unable to get access to IBM's \gls{hsm} offerings.

IBM \gls{z15} features \gls{simd} operations at 5.2GHz. Based on the literature we have studied, it seems unlikely that the \gls{z15} suffers from the same downclocking when performing \gls{simd} operations as \gls{avx2} on Intel processors. As such, we believe that \gls{simd} implementations on \gls{z15} would result in a higher throughput than a comparable implementation run on consumer or cloud hardware. We found that the \gls{avx2} implementations studied provided inconsistent throughputs. We believe that this, in part, has to do with the aforementioned downlocking of the CPU core when performing \gls{avx} operations on Intel hardware as previously described in section \ref{section:background:simd-avx}. If the stability of \gls{simd} operations on \gls{z15} performs at sustained 5.2GHz, the performance would most likely outperform that of comparable Intel machines.

The use of \gls{simd} instructions on \gls{z15} is a topic we had to omit in this paper due to time constraints. As all of the vectorized \gls{nist} submissions used \gls{avx2} in several thousands of lines of assembly, we found it difficult to efficiently study their implementations and translate them for use on \gls{z15}. Though vector intrinsics would provide us with a more efficient way of translating the implementations for use on \gls{z15}, none of the implementations studied had provided such an implementation in their submission or elsewhere. A more automatic approach would be to use IBM XL which features an automatic conversion of code to \gls{simd} instructions, where applicable~\cite{ibm:xl-autosimd}. We were however unable to access IBM XL due to its licensing cost and as such our results largely rely on non-vectorized code when running on \gls{z15}.

IBM have invested in making Z a quantum safe platform~\cite{ibm:z15:2019} and as such we believe they will continue focus on improving the performance and security of the \gls{post-quantum} \glspl{kem} implementations they offer today and in the future. Based on literature, we have identified that a high-performance \gls{post-quantum} \gls{kem} implementation relies on high-performance \gls{sha3} implementations, random number generation and polynomial arithmetic. By offering hardware-based support for these features, as well as by introducing hardware-accelerated polynomial primitives such as polynomial multiplication, we believe that IBM would further assert the potentials the performance of \gls{post-quantum} \glspl{kem} on Z. In terms of polynomial multiplication, the use of \glspl{ntt} or Karatsuba algorithms was found to be dominant in the algorithms submitted to \gls{nist}. By providing such implementations in hardware, polynomial multiplication could be further optimized.

One complaint regarding the performance of \gls{simd} instructions on hardware with support for \gls{avx2} has been the limited number of vector registers - 16 256-bit registers per processor~\cite{guneysu2013}. The \gls{z15} processor has 32 128-bit registers per processor~\cite{redbook:z15}, meaning that it does not offer a better set of registers than other hardware. As seen in section \ref{section:results:z15}, the low count of available \gls{simd} registers means that an increased number of load and store operations will be required~\cite{guneysu2013}. As we have been unable to find literature comparing the load and store operations performance of \gls{z15} and other hardware, we believe more work is required on the topic to analyze the performance impacts of \gls{post-quantum} \glspl{kem} in that regard.

\todo[inline]{
Minnessäkerhet
We have not focused on memory security in this blabbasd.... IBM memory security...

Cache-storlekar. L3 på Z är gigantisk, våra egna resultat verkar tyda på att cache kan vara viktigt.

Argue that some cryptos use 16-bit, others 64-bit. It may be important. For crypto agility we need both?
}

\section{Readiness for the Post-Quantum Transition}

Jacobi and Webb~\cite{jacobi2020} claim that mainframes process 90\% of the world's credit card transactions. This sensitive information must be kept secure. Therefore one may argue that the transition to post-quantum cryptography needs to happen earlier in the banking sector than in other sectors. A question one may then ask them self is, how would the transition to post-quantum cryptography affect mainframe users? Our data suggests that the performance hit would be severe, especially on mainframe hardware. Mainframes have an outstanding \gls{ecdh} performance because it takes advantage of the \gls{cpacf} co-processor to accelerate the calculations. But \gls{cpacf} does not support any of the post-quantum cryptographic algorithms. After \gls{nist}'s standardization process is finished, IBM should consider adding support for the standardized algorithms on the \gls{cpacf} to alleviate the performance impact of the transition. Furthermore, could IBM add support for it in their cryptocards.

Suppose we have a scenario where we replace the \gls{ecdhe} algorithm in \gls{tls} 1.2 with \gls{kem} and keep everything else unchanged. Then consider a using requesting a website such as a search engine. Note that this is only a theoretical scenario; in reality, further parts will likely be changed to accommodate the \gls{post-quantum} \glspl{kem}. Further, the best case will be used in this scenario - the most optimized versions of the algorithms running on a modern workstation. How would the performance of \gls{tls} be affected? If we replace \gls{ecdhe} with \gls{mceliece} 6960119f, the client will notice an extra delay of about 80 ms, about 800 times slower than today. For the server, this will significantly lower the throughput. If we replace \gls{ecdhe} with \gls{ntru} HPS 4096821, the extra delay is about 0.3 ms, around 4 times slower, probably not noticeable for a client. But for a server handling thousands of requests a second, the transition to post-quantum will be prominent. It is not yet clear if one will be required to generate a new key for every transaction when switching to \glspl{kem} from the traditional \glspl{kex}. If it is not required, we may remove the key generation time from the calculation. The extra delay could be cut down to about 0.1 ms and 0.05 ms for \gls{mceliece} and \gls{ntru}, respectively. We can see that \gls{mceliece} is slower in comparison to \gls{ntru}, but we need to take into account that \gls{mceliece} 6960119f has a higher security level.

To make implementations of cryptos more cross-platform, one may use vector-based intrinsics. The support for these varies between platforms, however. For example, \gls{ibmz} only supports a limited range of the functions. In order to effectively rely on vector intrinsics, we would need a standard that all platforms have support for. A problem with using intrinsics is that it can be hard to ensure constant time operations over multiple platforms. Because the intrinsics would be compiled into different instructions for each architecture, this is also true for the IBM XL AutoSIMD feature that can convert \gls{avx} to \gls{ibmz} \gls{simd} instructions.

\section{The Performance of Post-Quantum Key Encapsulation Mechanisms}

% mceliece - teoretiskt (enligt författarna) sätt är icke-f snabbare än f - men i praktiken är det verkligen inte så. icke-f mycket icke-determinstiskt
According to the \gls{nist} submission of Albrecht et. al.~\cite{mceliece2020}, the systematic (non-f) variant of \gls{mceliece} theoretically performs better during key generation than the semi-systematic (f) variant as it requires less computational work per key-generation attempt. They further state that this is true for as long as the systematic variant succeeds in finding a valid key with as few tries as possible. In reality, we found that the systematic variant consistently performs worse than the semi-systematic variant. Although theoretically faster, it seems as if the key generation of the systematic variant requires more tries than the authors anticipated. We further found that the systematic variant performs considerably more non-deterministically with a larger standard deviation than that of the semi-systematic variant. Although the implementation provided by the authors of the \gls{nist} submission is not yet standardized, we believe that one should pursuit the semi-systematic variant of \gls{mceliece} if \gls{nist} decides on standardizing it - if solely taking performance into account. Albrecht et. al. further discuss that it is unknown if the performance gain of the semi-systematic variant warrants its more complicated implementation or if users will even benefit from the speedup. As we found that the semi-systematic \gls{mceliece} variant 8192128f performed more than three times as fast as the systematic reference implementation and almost two times as fast as the systematic \gls{avx2} implementation, we believe that the more complex semi-systematic form is indeed warranted when only considering the performance of the \gls{kem}.

%%% === TODO LARGE CHANGE IN TOPIC HERE === %%%
% Cloud hardware - dip in performance over time. Counter argument - cloud provider 1 not behaving the same?
When performing our tests on cloud hardware, we anticipated a less consistent result than on dedicated consumer hardware. We believed that, due the virtualized and shared nature of the resources, the cloud environments would yield varied results over time as other users of the system utilized the hardware. We found that the Cloud Provider 2 environment had several performance discrepancies over time when running \glspl{kem} in sequential iterations. We also found, however, that Cloud Provider 1 largely functioned as the dedicated consumer hardware we tested. Although it is difficult to conclude from the small sample of cloud providers in our tests, we argue that there is in fact a non-zero chance that virtualized cloud hardware performs less consistently than dedicated hardware, given that Cloud Provider 2 had performance discrepancies in all of our sequential benchmarks. 

% Modern Laptop - Cache misses, oregelbundna minnesaccess
Another phenomena found in our data is how the Modern Laptop environment consistently yields the largest number of cache misses. Despite having a considerably newer CPU and more available cache than the Old Mid-Range Laptop and the Old Low-Range Laptop, the Modern Laptop environment performed much worse, as seen in Tables \ref{table:results:micro:cache-misses-mceliece-8192128f-enc} and \ref{table:results:micro:cache-misses-ntru-hrss701-enc}. We believe that this is due to the agressive prefetch mechanisms found in newer CPUs. These mechanism could badly predict what memory is necessary for future computation and as such evict memory that is used by the algorithms we benchmarked. The older machines could have less agressive mechanisms, or lack them all together, leading to fewer faults. We believe that this prefetching of cache did not constitute an issue for the Modern Workstation as it had double the amount of cache, resulting in virtually zero cache misses across the board.

%%% === TODO LARGE CHANGE IN TOPIC HERE === %%%
% -- mceliece använder betydligt mycket mer minne än övriga algoritmer - inte lämpligt för IoT etc?
When considering the memory footprint of the proposed \glspl{kem}, one must take into account the stack usage, heap usage as well as the parameter sizes of the algorithms. We found that the heap usage of the algorithms is negligible. The parameter sizes, however, vary considerably between \gls{mceliece} and \gls{ntru}. \gls{mceliece} requires between one and one point three megabytes of memory to store a public key, whilst \gls{ntru} requires a hundredth of that - roughly 1000 bytes. Although \gls{mceliece}'s private key is considerably smaller, it is still ten times as large as \gls{ntru}'s private key at roughly 13KiB. We therefore believe that \gls{mceliece} is impractical for use in low-memory environments such as embedded devices.

%%% === TODO LARGE CHANGE IN TOPIC HERE === %%%
% -- ntru skalar mycket bättre än mceliece sett till trådar
The data we collected for throughput and scaling of the algorithms identified that none of the classical algorithms scaled well when increasing the number of threads that concurrently executed the algorithms. In fact, it seemed as if the worst scaling and throughput was found in \gls{dhe}, followed by \gls{mceliece}. The algorithms saw virtually no increase in throughput once the number of threads surpassed the number of cores of the system. The performance of \gls{ecdhe} varied depending on the environment it was run in, but in general, the scaling was better than that of \gls{dhe}, with improvements made beyond the system's core count. The best scaling by far was found in the AVX2 optimized \gls{ntru} HRSS 701 implementations that saw a near-linear increase in performance with regards to the number of threads - even when passing the core count of the machine itself. Furthermore, the scaling of \gls{ntru} HRSS 701 was found to be the best in Modern Laptop and Modern Workstation, with the latest CPUs included in the test. We therefore strongly believe that \gls{ntru} HRSS 701 is a top candidate for \gls{post-quantum} \glspl{kem}, when only considering performance. Although not strictly comparable, we believe that one may expect similar performance of \gls{ntru} and the classical \gls{ecdhe}, judging from our measurements of throughput and scaling.

\todo[inline]{
-- argue that non-openssl aes is better due to less memory use?
}

\section{The Security of Post-Quantum Key Encapsulation Mechanisms}

% -- ntru mycket snabbare än mceliece, men säkerhetskategorin är lägre. HRSS 701 snabbare än HPS4096821, men säkerhetsnivån är lägre.
Our study has disregarded the security of the \gls{nist} submissions, letting us focus on the performance of the algorithms. It goes without saying that the security of the algorithms is of upmost importance. As we have not performed a study of the security of the algorithms on our own, we rely on the information presented by the \gls{nist} submissions themselves. As presented in \ref{table:background:submissions-security-level}, all of the \gls{mceliece} variants we tested are security level 5. The security level of \gls{ntru} varies between 3 and 5 for the HPS 4096821 variant and between 1 and 3 for the HRSS 701 variant, depending on the locality model used. We found that the HRSS 701 variant of \gls{ntru} overall performed the best out of all \gls{kem} algorithms tested. We further found that \gls{mceliece} variants performed the worst. We therefore believe, given our results, that there may be a correlation between the performance and security level of \gls{post-quantum} algorithms.

Although one may believe our sample set is small, we argue that one has to consider the broader picture. As mentioned in~\cite{ntru2020}, HPS 4096821 and HRSS 701 originated from two different \gls{nist} submissions. We therefore believe that, in part, these algorithms are different from one another - further increasing the size of the sample set. We do believe, however, that a further study of the correlation is required to definitively state whether or not there is a correlation between the security level of an algorithm and its performance.

% -- Återkoppla till säkerhetskategorierna - kategori 1 är AES128.
% -- Koppla ihop med Grover - AES 256 blir teoretiskt AES 128, vilket gör säkerhetsnivån halveras. I praktiken påstår nist2017 att det inte är ett problem - då den är svår att köra. Teoreitskt kan det då vara ett måste med kategori 5.
As described in section \ref{section:background:security-categories}, the categories are largely based on the strength of classic block-ciphers. Security category 1, for example, is defined based on attacks on AES 128. The fifth and strongest security category, category 5, is defined based on attacks on AES 256. These definitions, as well as the implications of Grover's algorithm described in section \ref{section:background:classical-cryptography-threats}, could result in the categories being redefined if Grover's algorithm becomes practical. Such a redefinition could result in category 5 post-quantum to be as strong as category 1 pre-quantum. That is, one may argue that only category 5 algorithms should be considered true post-quantum algorithms.

\section{On Performance Measurements}

% -- diskutera skillnader i våra mätvärden och NIST submissions. SUPERCOP estimerar, beter sig icke-deterministiskt, vi räknar?.
When studying previous work before outlining the method of this thesis, we identified that \gls{supercop} seemed to be the de-facto tool for measuring the performance of cryptographic algorithms. It was referenced and used in both the \gls{mceliece}~\cite{mceliece2020} and the \gls{ntru}~\cite{ntru2020} submissions to \gls{nist}. When studying the source code, we found several areas of concerns which led us to not use the software. One of the reasons was the use of older versions of the \gls{nist} submissions. We were interested in the third round of submissions, whilst \gls{supercop} provided the implementations of the second round. Another reason which contributed to our disregard of \gls{supercop} was the use of the estimated number of CPU cycles performed as the performance measurement. We identified that the code relied heavily on using the elapsed time and the base frequency of the CPU to calculate the number of required CPU cycles. As we were interested in high-quality measurements of not only CPU-cycles, but also instruction counts, cache misses etcetera, we found the Linux kernel API to be more apt for our use case. As it uses the hardware counters found in many CPUs, we argue that our measurements are accurate since they are counted and not calculated, as is the case in \gls{supercop}. Not only did we find the use of perf leading to more deterministic results, but the number of CPU cycles we measured were overall higher than the values found in the \gls{nist} submissions, as presented in Table \ref{table:results:sequential:nist-vs-ours}. We argue that the use of estimated CPU cycles yields inaccurate results which may easily be affected by many variables of the environment, such as the base clock of the CPU. The \gls{supercop} user guide recommends that one disable both hyper-threading and boosting so that it may more accurately calculate the number of CPU cycles~\cite{supercop}. We believe that it is not a common practice in reality to turn off both hyper-threading and boosting, which led us to use both technologies in our measurements. To decrease the risk of environmental factors such as boosting to affect our measurements, we repeated all of the measurements up to a thousand times per benchmark. We also repeated the entire benchmark at two completely different occasions. We argue that a method of measuring the performance, such as the one used by us in this thesis, would provide the public with more accurate and real-world values regarding the performance of the cryptographic algorithms. Despite us using the Linux kernel perf API, we believe that there are valid reasons use \gls{supercop}'s method of measuring CPU cycles. The Linux kernel API requires hardware support in order to accurately measure CPU cycles, instruction counts etcetera. This led us to not be able to accurately benchmark all environments with regards to the CPU cycle count. \gls{supercop}'s solution, however, is less hardware-dependant and as such more suitable for cross-platform measurements on non-Intel hardware and non-Linux operating systems. To help researchers and users achieve more accurate performance measurements, we believe that the industry as a whole could benefit from the addition of hardware-based performance counters in more processor architectures and models.

% i3:a, i7:a på ntru-körningar, xeon på mceliece, svårt att jämföra?
Comparing the performance of the \gls{nist} submissions was further complicated by the wide variety of hardware used. The \gls{ntru} submission~\cite{ntru2020} used a 3.2 GHz Intel Core i3-6100T for running the reference implementation. The same submission then used an Intel Core i7-4770k to run the AVX2 implementation. The \gls{mceliece} submission~\cite{mceliece2020} used an Intel Xeon E3-1275 v3 for its performance measurements. Although we tested a wide range of hardware, we were unable to use the same hardware as the authors. We argue that the fact that a single system was not used for all of the performance benchmarks complicate the comparisons in performance. For example, the Intel Core i3-6100T has 3MB cache~\cite{i36100t} and the Xeon E3-1275 v3 8MB~\cite{xeon31275}. As we found that the number of cache misses were considerably reduced when the available cache was increased, we believe that it is entirely possible for the performance measurements presented in the \gls{ntru} submission to be considerably worse than if they would have used the same environment as the \gls{mceliece} submission. In part, one may argue that this issue is up to \gls{nist} to solve before considering the standardization of an algorithm, but we believe it makes for a more difficult comparison when such a variety of hardware is used to represent the algorithms' performance no matter the stage of the submissions.

% mceliece Ours: ref-optimized modern workstation, theirs Intel  Xeon ref -O3 etc...
% ntru: ours ref-optimized modern workstation, theirs intel i 3-6000? etc...
% ntru ours: avx2-optimized modern ... theris i7-4770K (Haswell)
% supercop 1% av ntruhps4096821, 60% av ntruhrss701 wtf?
% amd64; CoffeeLake (906ea); 2017 Intel Core i7-8700; 6 x 3200MHz; bitvise, supercop-20190910
% https://bench.cr.yp.to/results-kem.html
% NTRU stated that their results were to be presented on the same page, but they were not

% Threats to Validity
\input{chapters/discussion/validity}