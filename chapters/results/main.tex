\chapter{Results and Analysis}
\label{chapter:results}

\section{IBM Z Features for Post-Quantum}
\label{section:results:z15}

In the following section, how the \gls{z15} may take advantage of the characteristics outlined in section \ref{section:related-work:post-quantum-characteristics} is presented.

Wolpert et. al.~\cite{wolpert2020} introduced the IBM \gls{z15} as a chip built on GlobalFoundries' 14nm node. A key innovation that enabled a scaled design without targeting a new die size was the development of the 2MB eDRAM. This memory is used to achieve 256MB of L3 cache and 960MB L4 eDRAM cache. The chip targets a lifetime of 100 000 power-on hours and a 99.99999\% uptime guarantee which is about 3.2 seconds of allowed downtime in a year. In a lab environment based on real-world customer environments, the clock frequency reached sustained 6GHz. IBM has control over the entire hardware stack and the software that runs on it, making it possible for every part in the system to work extremely efficiently with one another~\cite{wolpert2020}. Further, IBM added support for \gls{ecc} to the \gls{cpacf}~\cite{berry2020}.

In~\cite{payer2020}, Payer et. al. outlined a completely redesigned binary and hexadecimal floating-point unit in \gls{z15}. The unit allows for an efficient implementation of \gls{simd} operations at 5.2GHz, without sacrificing reliability. Initially designed for accelerating common machine learning operations, the 32-bit floating-point operations have seen a doubling in throughput. The latency of all operations on the binary floating-point unit have also been reduced. The \gls{z15} has 32 128-bit wide vector registers per processor~\cite{redbook:z15}.

Quantum-safe platforms demand increased cryptographic performances, agility and safety~\cite{busby2020}. The IBM 4769 cryptographic coprocessor was developed to meet these demands. It was further developed with future workloads in mind and has support for custom programs via user defined extensions~\cite{busby2020, ibm:4769}. Due to the \gls{nist} standardization process being ongoing at the release of both the \gls{z15} and the IBM 4769 \gls{hsm}, the cryptographic agility is important~\cite{microsoft2020, ibm:z15:2019}. By combining a new \gls{asic} and a \gls{fpga}, the IBM 4769 is highly programmable and may grow with customer-specific functionality. The IBM 4769 further supports several of the key symmetric primitives used by \gls{post-quantum} algoritms, such as \gls{sha3} and \gls{aes}. It also features a hardware-based Digital Random Number Generator (DRNG)~\cite{ibm:4769:ep11}. Furthermore, it also supports the round two \gls{nist} submission \gls{dilithium}~\cite{ibm:4769:ep11, busby2020}.

% ===== Snowballing ===== %
% ===== Snowballing ===== %
% ===== Snowballing ===== %
% ===== Snowballing ===== %
% ===== Snowballing ===== %

% After initial search:
% -- History of IBM Z Mainframe Processors: 10.1109/MM.2020.3017107
% -- An optimized blockchain solution for the IBM z14: 10.1147/JRD.2018.2795889
% -- Cryptocards: opening the door to an unprecedented level of security: ISSN 03022528
% -- Performance innovations in the IBM z14 platform: 10.1147/JRD.2018.2798340
% -- Improving security against cache memory attacks for dual field multiplier design based on elliptic curve cryptography: 10.1007/s12652-020-02830-1
% -- IBM z14™: 14nm microprocessor for the next-generation mainframe: 10.1109/ISSCC.2018.8310171
% -- SIMD Multi Format Floating-Point Unit on the IBM z15(TM): 10.1109/ARITH48897.2020.00027
% -- IBM z15: Physical design improvements to significantly increase content in the same technology 10.1147/JRD.2020.3008099
% -- Design of the IBM z15 microprocessor: 10.1147/JRD.2020.3008119
% -- The IBM 4769 Cryptographic Coprocessor: 10.1147/JRD.2020.3008145
% -- Addressing cloud computing security issues: 10.1016/j.future.2010.12.006

% Start set:
% -- ntru2020
% -- mceliece2020
% -- kyber2021
% -- saber
% -- History of IBM Z Mainframe Processors: 10.1109/MM.2020.3017107
% -- Cryptocards: opening the door to an unprecedented level of security: ISSN 03022528
% -- SIMD Multi Format Floating-Point Unit on the IBM z15(TM): 10.1109/ARITH48897.2020.00027
% -- IBM z15: Physical design improvements to significantly increase content in the same technology 10.1147/JRD.2020.3008099
% -- Design of the IBM z15 microprocessor: 10.1147/JRD.2020.3008119
% -- The IBM 4769 Cryptographic Coprocessor: 10.1147/JRD.2020.3008145
% -- Cores, Cache, Content, and Characterization: IBM's Second Generation 14-nm Product, z15

% After snowballing
% -- ntru2020
%    -- Post-quantum key exchange - a new hope. In Thorsten Holz and Stefan Savage, editors, Proceedings of the 25th USENIX Security Symposium
%    -- [3] Antoine Joux Anja Becker, Nicolas Gama. Speeding-up lattice sieving without increasing the mem- ory, using sub-quadratic nearest neighbor search
%    -- Daniel J. Bernstein and Edoardo Persichetti. Towards kem unification
%    -- Edoardo Persichetti. Improving the effciency of code-based cryptography
% -- mceliece2020
%    -- Daniel J. Bernstein. Some small suggestions for the Intel instruction set, 2014.
%    --  For safety’s sake: We need a new hardware-software contract!
% -- kyber2021
%    -- https://cryptojedi.org/papers/nttm4-20190513.pdf
% -- saber
%    -- High-Throughput Software Implementation of Saber Key Encapsulation Mechanism
%    -- High-speed instruction-set coprocessor for lattice- based key encapsulation mechanism: Saber in hardware
%    -- A High-performance Hardware Implementation of Saber Based on Karatsuba Algorithm
% -- History of IBM Z Mainframe Processors: 10.1109/MM.2020.3017107
%    -- E. K. Joseph, "Technical overview of secure execution for Linux on IBM Z", 2020.
%    -- P. J. Meaney et al., "IBM zEnterprise redundant array of independent memory subsystem",
%    -- C. Berry et al., "IBM z15: A 12-Core 5.2GHz microprocessor", Proc. IEEE Int. Solid-State Circuits Conf., pp. 54-56, 2020.
% -- SIMD Multi Format Floating-Point Unit on the IBM z15(TM): 10.1109/ARITH48897.2020.00027
%    -- "IBM Unveils z15 With Industry-First Data Privacy Capabilities" in , IBM, 2019, [online]
%    -- IBM z15 (8561) Technical Guide, IBM, 2019
% -- IBM z15: Physical design improvements to significantly increase content in the same technology 10.1147/JRD.2020.3008099 (LOCKED)
%    -- None
% -- Design of the IBM z15 microprocessor: 10.1147/JRD.2020.3008119 (LOCKED)
%    -- None
% -- The IBM 4769 Cryptographic Coprocessor: 10.1147/JRD.2020.3008145 (LOCKED)
%    -- B. B. Chakrabarty, A. Buecker and L. Dymoke-Bradshaw, "Security on the IBM mainframe", vol.
%    -- T. W. Arnold, C. Buscaglia and F. Chan, "IBM 4765 cryptographic coprocessor", IBM J. Res. Dev., vol. 56, no. 1/2, pp. 10:1–10:13, Jan.–Mar. 2012.
%    -- T. W. Arnold, M. Check and E. A. Dames, "The next generation of highly reliable and secure encryption for the IBM z13", IBM J. Res. Dev., vol. 59, no. 4/5, pp. 6:1–6:2, Jul.–Sep. 2015. (LOCKED)
% -- Cores, Cache, Content, and Characterization: IBM's Second Generation 14-nm Product, z15
%    -- None

\section{Identifying Hot Paths}
\label{section:results:hot-paths}

The following sections analyze the \gls{ntru} and \gls{mceliece} submissions to the \gls{nist} standardization process, in terms of the reference implementations and what branches the code takes, how many times they are taken, as well as how many instructions are retired in each function. Unless otherwise stated, the measurements are the percentage of instructions performed in a function or in a line of code, relative to the parent. All measurements were done on an unoptimized reference implementation.

All of the data as well as further visualizations not included in this work is published on GitHub\footnote{\href{https://github.com/profiling-pqc-kem-thesis/data}{https://github.com/profiling-pqc-kem-thesis/data}}.

\subsection{NTRU}

The \gls{ntru} reference implementations consists of three API methods - keypair generation via \textit{crypto\_kem\_keypair}, encryption (encapsulation) via \textit{crypto\_kem\_enc} and decryption (decapsulation) via \textit{crypto\_kem\_dec}  decryption (decapsulation). These API functions may in turn call several internal functions, such as the polynomial math library which functions are prefixed \textit{poly\_}.

The following graphs contain relative measurements denoting the percentage of instructions spent in each method. Both the HPS and HRSS variants of \gls{ntru} perform alike, hence only one set of graphs is shown for \gls{ntru} as a whole.

Figure \ref{figure:result:hot-paths:ntru:crypto_kem_keypair} shows us that $40\%$ of the instructions of generating a keypair is spent in the library function \textit{poly\_Rq\_mul} - a function which multiplies two polynomials in $\mathbb{R}^q$. Another $20\%$ of the instructions are spent on inverting a polynomial in $\mathbb{R}^2$ - in \textit{poly\_R2\_inv}. In total, the polynomial math accounts for at least $85\%$ of the time spent on generating a keypair.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{chapters/results/hot-paths/ntru/crypto_kem_keypair.pdf}
    \caption{Relative instruction count of \gls{ntru}'s \textit{crypto\_kem\_keypair} in an unoptimized reference implementation}
    \label{figure:result:hot-paths:ntru:crypto_kem_keypair}
\end{figure}

\noindent Figure \ref{figure:result:hot-paths:ntru:crypto_kem_enc} describes the encapsulation API function. The key is generated using random bytes which are fed through 256-bit \gls{aes} in its Electronic Code Book (ECB) configuration to produce uniformly random bytes. Again, we see a significant percentage of the instructions spent in the polynomial library - in \textit{poly\_Rq\_mul}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{chapters/results/hot-paths/ntru/crypto_kem_enc.pdf}
    \caption{Relative instruction count of \gls{ntru}'s \textit{crypto\_kem\_enc} in an unoptimized reference implementation}
    \label{figure:result:hot-paths:ntru:crypto_kem_enc}
\end{figure}

\noindent The decryption function of \gls{ntru} is presented in figure \ref{figure:result:hot-paths:ntru:crypto_kem_dec}. Virtually all of the instructions spent decrypting (decapsulating) a key is spent in the \textit{poly\_Rq\_mul} function.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{chapters/results/hot-paths/ntru/crypto_kem_dec.pdf}
    \caption{Relative instruction count of \gls{ntru}'s \textit{crypto\_kem\_dec} in an unoptimized reference implementation}
    \label{figure:result:hot-paths:ntru:crypto_kem_dec}
\end{figure}

\noindent Looking at the code of \textit{poly\_Rq\_mul} as shown in figure \ref{figure:result:hot-paths:ntru:poly_Rq_mul}, it is evident that the vast majority of instructions are spent on multiplying and adding numbers in loops. The value of \textit{NTRU\_N} corresponds to the parameter set. For \gls{ntru} HRSS 701, the value is 701 and for \gls{ntru} HPS 4096821 the value is 821.

\begin{figure}[H]
    \centering
    \begin{lstlisting}[language=C]
void poly_Rq_mul(poly *r, const poly *a, const poly *b) {
  int k, i;

  for (k = 0; k < NTRU_N; k++) {
    r->coeffs[k] = 0;
    for (i = 1; i < NTRU_N - k; i++) // 10.21%
      r->coeffs[k] += a->coeffs[k + i] * b->coeffs[NTRU_N - i]; // 42.75%
    for (i = 0; i < k + 1; i++) // 8.20%
      r->coeffs[k] += a->coeffs[k - i] * b->coeffs[i]; // 38.79%
  }
}
    \end{lstlisting}
    \caption{Annotated reference source code of \gls{ntru}'s \textit{poly\_Rq\_mul}}
    \label{figure:result:hot-paths:ntru:poly_Rq_mul}
\end{figure}

\clearpage To summarize, it is shown that factors such as the speed of \gls{aes} has little to do with the overall performance of the algorithm. Furthermore, it seems as if the polynomial library functions account for the vast majority of instructions spent on key-pair generation, encryption and decryption. The function \textit{poly\_Rq\_mul}, which multiplies two polynomials in $\mathbb{R}^q$, accounts for most of the calculations performed.

\subsection{Classic McEliece}
As with \gls{ntru}, the reference implementation of \gls{mceliece} consists of three API methods - \textit{crypto\_kem\_keypair}, \textit{crypto\_kem\_enc} and \textit{crypto\_kem\_dec} for key-pair generation, encryption (encapsulation) and decryption (decapsulation), respectively. These API functions may in turn call several internal functions. We will only present the hot-paths for the \gls{mceliece} 8192128 non-f variant because it can quite accurately represent the hot-paths for all \gls{mceliece} variants.

In figure \ref{figure:result:hot-paths:classic-mceliece:crypto_kem_keypair}, we can see the \textit{crypto\_kem\_keypair} function and its only significant internal function, the \textit{pk\_gen} function that takes up 98.57\% of the execution time. In figure \ref{figure:result:hot-paths:classic-mceliece:pk_gen}, a more in-depth representation of the \textit{pk\_gen} function is presented. In that figure, all of its internal functions are shown. As can be seen, they do not make up much of the accumulated instruction counts. Most of the instructions are inside of the \textit{pk\_gen} function itself.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{chapters/results/hot-paths/classic-mceliece/8192128/crypto_kem_keypair.pdf}
    \caption{Relative instruction count of \gls{mceliece}'s \textit{crypto\_kem\_keypair} in an unoptimized reference implementation}
    \label{figure:result:hot-paths:classic-mceliece:crypto_kem_keypair}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{chapters/results/hot-paths/classic-mceliece/8192128/pk_gen.pdf}
    \caption{Relative instruction count of \gls{mceliece}'s \textit{pk\_gen} in an unoptimized reference implementation}
    \label{figure:result:hot-paths:classic-mceliece:pk_gen}
\end{figure}

\noindent The encryption function of \gls{mceliece} is presented in figure \ref{figure:result:hot-paths:classic-mceliece:crypto_kem_enc}. Most time is spent in the syndrome function. In figure \ref{figure:result:hot-paths:classic-mceliece:crypto_kem_dec} the decryption function is presented. The majority of the time is spent in the \textit{synd} function.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{chapters/results/hot-paths/classic-mceliece/8192128/crypto_kem_enc.pdf}
    \caption{Relative instruction count of \gls{mceliece}'s \textit{crypto\_kem\_enc} in an unoptimized reference implementation}
    \label{figure:result:hot-paths:classic-mceliece:crypto_kem_enc}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{chapters/results/hot-paths/classic-mceliece/8192128/crypto_kem_dec.pdf}
    \caption{Relative instruction count of \gls{mceliece}'s \textit{crypto\_kem\_dec} in an unoptimized reference implementation}
    \label{figure:result:hot-paths:classic-mceliece:crypto_kem_dec}
\end{figure}

\section{The Performance of Post-Quantum Algorithms}

This section presents the results of the experiment as outlined in section \ref{section:method:experiment}. It further discusses the performance of \gls{post-quantum} \glspl{kem} and how they may differ on various architectures and hardware.

\subsection{Heap Usage}

As outlined in the method, section \ref{section:method:experiment:phase1:measurements}, we measured the heap usage of the algorithms by monitoring heap allocation and deallocation methods such as malloc using heaptrack. We found that the heap allocation did not change across environments nor between the analyzed optimizations except for some few. As for the implementations, this is rather unsurprising as changing how the allocations worked would alter the behavior of the code. As we found that the heap behavior of the algorithms were consistent throughout our test, we will for the rest of this section not refer to any specific environment or optimization used for the algorithms unless otherwise noted. When talking about allocation, we will refer to the peak allocation - that is, the maximum allocated bytes recorded during a function's lifetime. Our measurements did not capture the peak heap allocation during a benchmark as it would include memory allocations introduced by our tooling. The heap usage metrics do not reflect the total memory usage as memory in the form of parameters passed to the algorithms or memory that is part of the stack is not included.

At one point, the \gls{ntru} implementations allocated $264$ bytes generating a keypair. These bytes were allocated by \gls{openssl} to initialize the library's envelope suite of functions. In context, this was done as the \gls{ntru} implementation used \gls{openssl}'s \gls{aes} ECB implementation to generate uniform pseduo-random bytes. The invocation of the \gls{aes} function itself resulted in $208$ bytes being allocated by the \gls{openssl} \gls{aes} ECB implementation. Beyond these bytes, the \gls{ntru} implementation did not require any additional heap allocation during its runtime.

Just like the \gls{ntru} implementations, the \gls{mceliece} implementations made use of \gls{openssl} to create uniform pseudo-random bytes. As such, the same $264$ bytes were allocated to initialize \gls{openssl}'s envelope suite of functions, as well as the $208$ bytes to actually encrypt random data to produce a uniformly distributed series of bytes. No further allocations were found to be made.

The \gls{ecdhe} and \gls{dhe} implementations were based entirely on functions made available via the \gls{openssl} library. All of the implementations had an increase in the number of allocations made when compared to \gls{ntru} and \gls{mceliece}. The \gls{dhe} implementation when run in the IBM Community Cloud environment saw an allocation of $16960$ bytes during keypair generation made from a function to calculate the constant-time modulo exponentiation on an arbitrary large number. All other environments allocated $9024$ bytes for the same invocation. Furthermore, the keypair generation saw the allocation of a further $1024$ bytes in the IBM Community Cloud environment seemingly related to the same modulo operation.

\begin{table}
    \centering
    \small
    \caption{Heap allocation in bytes for \acrshort{dhe}}
    \label{table:results:memory:dhe-heap}
    \begin{tabularx}{\linewidth}{X c c c}
        \toprule
        \thead{Environment} & \thead{\gls{openssl} Version} & \multicolumn{2}{c}{\thead{Sum of Peaks}}\\
        & & \thead{Keypair} & \thead{Exchange} \\
        \midrule
        IBM Community Cloud & 1.1.1g FIPS & 42800 & 39908 \\
        Cloud Provider 1 & 1.1.1 & 14156 & 11934 \\
        Cloud Provider 2 & 1.1.1f & 14040 & 11762\\
        Modern Workstation & 1.1.1f & 14040 & 11762 \\
        Modern Laptop & 1.1.1f & 14040 & 11762 \\
        Old Mid-Range Laptop & 1.1.1f & 14040 & 11762\\
        Old Low-Range Laptop & 1.1.1f & 14040 & 11762\\
        \bottomrule
    \end{tabularx}
\end{table}

The operation of dividing an integer was found to correlate to an additional $536$ bytes in the Cloud Provider 1 environment and $528$ bytes in all of the other environments. The environments saw an additional $1432$ bytes corresponding to the handling of arbitrary large numbers whilst generating a keypair. Fetching random data raised the allocation with $632$ bytes in all environments. There were further allocations made, all of which are summarized in Table \ref{table:results:memory:dhe-heap}. In the table, the peak heap usage of each function of the implementation has been summed up to produce a sum of peaks. This value does not correspond to the peak heap usage of the algorithm as a whole. Though it is unclear why the exchange stage, which includes a keypair generation, uses less memory, it could be due to OpenSSL's internal handling of memory.

\begin{table}
    \centering
    \small
    \caption{Heap allocation in bytes for \acrshort{ecdhe}}
    \label{table:results:memory:ecdhe-heap}
    \begin{tabularx}{\linewidth}{X c c c c}
        \toprule
        \thead{Environment} & \thead{Curve} & \thead{OpenSSL Version} & \multicolumn{2}{c}{\thead{Sum of Peaks}}\\
        & & & \thead{Keypair} & \thead{Exchange} \\
        \midrule
        IBM Community Cloud & \gls{p-256} & 1.1.1g \acrshort{fips} & 12184 & 0 \\
        IBM Community Cloud & \gls{curve25519} & 1.1.1g \acrshort{fips} & 4524 & 1024 \\

        Cloud Provider 1 & \gls{p-256} & 1.1.1 & 9220 & 0 \\
        Cloud Provider 1 & \gls{curve25519} & 1.1.1 & 8380 & 0 \\

        Cloud Provider 2 & \gls{p-256} & 1.1.1f & 4884 & 1872 \\
        Cloud Provider 2 & \gls{curve25519} & 1.1.1f & 2556 & 512\\

        Modern Workstation & \gls{p-256} & 1.1.1f & 4884 & 1872 \\
        Modern Workstation & \gls{curve25519} & 1.1.1f & 2556 & 512 \\
        
        Modern Laptop & \gls{p-256} & 1.1.1f & 4884 & 1872 \\
        Modern Laptop & \gls{curve25519} & 1.1.1f & 2556 & 512 \\
        
        Old Mid-Range Laptop & \gls{p-256} & 1.1.1f & 4884 & 1872\\
        Old Mid-Range Laptop & \gls{curve25519} & 1.1.1f & 2556 & 512\\
        
        Old Low-Range Laptop & \gls{p-256} & 1.1.1f & 4884 & 1872\\
        Old Low-Range Laptop & \gls{curve25519} & 1.1.1f & 2556 & 512\\
        \bottomrule
    \end{tabularx}
\end{table}

The \gls{ecdhe} implementation behaved much like the \gls{dhe}, with the peaks differing between some environments. The implementation too worked with \gls{openssl}'s arbitrary numbers and as such had several of the same allocations as the \gls{dhe} implementations. The peaks are presented in Table \ref{table:results:memory:ecdhe-heap}. Though it is unclear why the exchange stage is sometimes zero, it could be due to \gls{openssl}'s internal handling of memory.

\subsection{Stack Usage}

In terms of stack usage, the results varied between the implementations, compilers and features. For instance, the polynomial math function poly\_Rq\_mul in \gls{ntru}, which was identified as constituting most of the time spent in the algorithm, varied between taking up $70499$ and $188$ bytes. In all environments, the HPS 4096821 \gls{avx2} variant took up $70499$ bytes. This did not change between compilers or optimization flags, which could be due to the fact that the implementation was written in assembly - leaving little room for the compiler to alter the behavior. The same goes for the HRSS 701 variant of \gls{ntru} - the size was consistently 55317 bytes across environments and optimization flags. The lowest sizes were found when the reference implementation was compiled with optimization. The sizes are presented in Table \ref{table:results:memory:ntru-stack}. Although the optimization flags used were not chosen for improving the size of the binary, the size of the functions were lowered in all cases. Although the same version of GCC was used for several environments, the results differed between $188$ and $192$ bytes. Clang continually produced larger regions than GCC. Furthermore, more recent versions of the compilers seemed to produce smaller sizes.

\begin{table}
    \small
    \centering
    \caption{Stack size of poly\_Rq\_mul in bytes for the optimized reference implementation}
    \label{table:results:memory:ntru-stack}
    \begin{tabularx}{\linewidth}{X c c c c}
        \toprule
        \thead{Environment} & \thead{Compiler} & \thead{Compiler Version} & \thead{Optimized Size} & \thead{Reference Size}\\
        \midrule
        Cloud Provider 1 & clang & 6.0.0 & 702 & - \\
        IBM Community Cloud & clang & 10.0.1 & 522 & - \\
        Cloud Provider 2 & clang & 10.0.0 & 380 & - \\
        Modern Laptop & clang & 10.0.0 & 380 & - \\
        Modern Workstation & clang & 10.0.0 & 380 & - \\
        Modern Laptop & clang & 10.0.0 & 380 & - \\
        Old Low-Range Laptop & clang & 10.0.0 & 300 & - \\
        Old Mid-Range Laptop & clang & 10.0.0 & 300 & - \\
    
        IBM Community Cloud & gcc & 8.3.1 & 242 & 382 \\
        Old Low-Range Laptop & gcc & 9.3.0 & 196 & 255 \\
        Old Mid-Range Laptop & gcc & 9.3.0 & 196 & 255 \\
        Cloud Provider 1 & gcc & 7.5.0 & 192 & 253\\
        Cloud Provider 2 & gcc & 9.3.0 & 188 & 255\\
        Modern Laptop & gcc & 9.3.0 & 188 & 255\\
        Modern Workstation & gcc & 9.3.0 & 188 & 255\\
        \bottomrule
    \end{tabularx}
\end{table}

The same correlation between optimization flags and compiler versions could not be found in the case of \gls{mceliece} and pk\_gen. In that case, the largest recorded size was found in IBM Community Cloud's optimized reference implementation with $22858$ bytes. The optimized \gls{avx2} implementation of the Modern Workstation and Modern Laptop came second with $22123$ bytes. Lastly, the smallest size recorded was found in the optimized reference implementation of Old Mid-Range Laptop and Old Low-Range Laptop at $2026$ bytes.

As for other functions of the \glspl{kem}, the data was too massive to comprehensively refer to here. There are, however, tables in the appendix for average sizes as compared to the reference implementation compiled with GCC. In Table \ref{table:result:ntru-average-stack-increase-cloud}, for example, one may see that Clang seems to produce smaller binaries. Furthermore, the optimized \gls{avx2} builds result in the largest binaries - with symbols taking 8-10 times the size of the reference implementation.

As the \gls{ecdhe} and \gls{dhe} implementations studied relied on the dynamically linked \gls{openssl} library, the stack usage of these were found to be minimal.

\subsection{Parameter Sizes}

The measurements presented up until this point have been related to either the runtime behavior of the algorithms, or the static requirements they place on the environment in the form of stack usage. These measurements have not included the parameters fed to the algorithms - which could affect real-world applications. The remaining part of this section presents data gathered from the implementations.

The \glspl{kem} tested conformed to the same API. Their signatures are presented in figure \ref{figure:result:memory:kem-api}. The function crypto\_kem\_keypair, which generates a keypair takes in a public key and a secret key. The encapsulation function, crypto\_kem\_enc, takes in a ciphertext buffer, a key buffer and the public key of a peer. Lastly, the decapsulation function, crypto\_kem\_dec takes in a key buffer, the ciphertext as received from a peer and the secret key. The sizes of these parameters differed between algorithms and implementations - but were not changed depending on the environment or optimizations. Table \ref{table:results:memory:kem-parameter-sizes} shows the sizes of the parameters in bytes. The sizes did not differ between the semi-systematic (f) variants of \gls{mceliece}. Therefore only two variants of \gls{mceliece} are presented.

The APIs were similar, but not the same for the \gls{kex} implementations. That is due to \gls{dh} requiring the so called Diffie-Hellman Parameters $p$ and $g$. The APIs are shown in figure \ref{figure:results:memory:kex-api}. As with the \gls{kem} implementations, the \gls{kex} implementations used two buffers for a public and a private key when generating a keypair. The \gls{dhe} implementation also required the previously mentioned domain parameters $p$ and $g$ - both buffers of bytes containing the respective parameter. The crypto\_dh\_enc functions further used a key buffer. There is no analogous ciphertext as the \glspl{kex} are fundamentally different from the \glspl{kem} as covered in \ref{section:background:diffie-hellman}. The sizes of the parameters are presented in Table \ref{table:results:memory:kex-parameter-sizes}.

\begin{table}
    \centering
    \small
    \caption{Parameter sizes in bytes of the \acrshort{kem}s under test}
    \label{table:results:memory:kem-parameter-sizes}
    \begin{tabularx}{\linewidth}{X c c c c c}
        \toprule
        \thead{Algorithm} & \thead{Parameters} & \thead{public\_key} & \thead{private\_key} & \thead{ciphertext} & \thead{key}\\
        \midrule
        \gls{mceliece} & 8192128(f) & 1357824 & 14120 & 240 & 32 \\
        \gls{mceliece} & 6960119(f) & 1047319 & 13948 & 226 & 32 \\
        \gls{ntru} & HRSS 701 & 1138 & 1450 & 1138 & 32 \\
        \gls{ntru} & HPS 4096821 & 1230 & 1590 & 1230 & 32 \\
        \bottomrule
    \end{tabularx}
\end{table}

\begin{table}
    \centering
    \small
    \caption{Parameter sizes in bytes of the \acrshort{kex}s under test}
    \label{table:results:memory:kex-parameter-sizes}
    \begin{tabularx}{\linewidth}{X c c c c c c}
        \toprule
        \thead{Algorithm} & \thead{Parameters} & \thead{public\_key} & \thead{private\_key} & \thead{$p$} & \thead{$g$} & \thead{key}\\
        \midrule
        \acrshort{dhe} & 2048 & 255 & 255 & 256  & 1 & 32 \\
        \acrshort{ecdhe} & P-256 & 65 & 32 & - & - & 32 \\
        \gls{x25519} & - & 32 & 32 & - & - & 32 \\
        \bottomrule
    \end{tabularx}
\end{table}

\subsection{Sequential Performance}

The performance of the algorithms in a single run was analyzed by performing a thousand sequential runs in series, for each algorithm under test at two different occasions. The complete method is outlined in section \ref{section:method:experiment:phase1}.

As covered in section \ref{section:background:mceliece}, \gls{mceliece} supports two forms of keypair-generation algorithms - a systematic form and a semi-systematic form. The systematic forms, such as \gls{mceliece} 8192128 and the semi-systematic forms, such as \gls{mceliece} 8192128f, use different algorithms for generating the keypairs. The authors of the \gls{nist} submission state that these two forms may have different performance characteristics~\cite{mceliece2020}. In Table \ref{table:results:sequential-mceliece-6960119-keypair-modern-workstation}, these two forms can be seen with every set of performance improvements tested in the experiment. As can be seen in the table, the reference implementation in its systematic form was remarkably inconsistent, with a standard deviation of $20580.20$ as compared to the $6.91$ of the semi-systematic form. The change in performance characteristics is also visible in the mean, with the semi-systematic form taking about $30\%$ of the time on average as compared to the systematic form. In both cases, the \gls{avx2} implementation had the second largest standard deviation with $302.58$ and $6.91$ for the systematic form and the semi-systematic form, respectively. Despite being a \gls{simd} implementation, \gls{avx2} performed worse than the optimized reference implementation in both forms of the algorithm. The optimized \gls{avx2} implementation, however, outperformed the optimized reference implementation with four to six times the performance. The same results were found to be general for several environments, as shown in Table \ref{table:results:sequential:mceliece-6960119-keypair} and \ref{table:results:sequential:mceliece-6960119f-keypair}.

\input{chapters/results/sequential/Modern Workstation_deviation_mceliece}

In all environments but one, we found consistent performance throughout all sequential runs, non-deterministic behavior and outliers aside. The Cloud Provider 2 environment provided inconsistent results throughout the sequential performance benchmarks - across various algorithms and runs. As seen in figure \ref{figure:results:sequential:mceliece-decrpyt-cloud-provider-2}, there are several occasions where the performance shifted over time. In the figure, the speedup relative to the reference GCC implementation is used as opposed to the absolute duration of each iteration. Depending on how deterministic the behavior of the algorithms were, as well as how stable the environment was, one could have expected more or less the same performance throughout the test. As the Cloud Provider 2 environment used two virtual CPU cores, it was not entirely unexpected to find such inconsistent results. The same behavior was not found in any of the environments tested with dedicated hardware, nor the Cloud Provider 1 environment.

\begin{figure}
    \centering
    \includegraphics[scale=0.75]{chapters/results/sequential/mceliece_6960119_decrypt_Cloud Provider 2.pdf}
    \caption{The speedup of \gls{mceliece} 6960119 decryption over time. Note that the green and blue lines are coincident}
    \label{figure:results:sequential:mceliece-decrpyt-cloud-provider-2}
\end{figure}

In Table \ref{figure:results:sequential:ntru-hrss701} all of the average durations and speedups for each optimization and environment are shown. In the table, one may see conclusive evidence that the optimized \gls{avx2} implementations performed the best in all environments with support for \gls{avx2}. It is not clear, however, what compiler produces the best performance. In the environments without support for \gls{avx2}, the optimized reference implementation performed the best. Just as with the \gls{avx2} implementations, it is unclear whether or not GCC or Clang produced the best results. It seems as if GCC produced more consistent results, with less variation in the averages. In the Old Low-Range Laptop environment, for example, one may see that Clang required about two times the duration of the comparable subject compiled with GCC. In other cases, Clang seemed to be on par with, or better than, GCC. As the speedup is percentual based on the environment, one may expect them to be similar across environments. That is, the speedup of using \gls{avx2} instead of the reference implementation, should in theory be similar on both the Modern Workstation and Modern Laptop, for example. In practice we found this to not be true. Although similar, the \gls{avx2} implementation compiled and optimized by Clang on the Modern Workstation had an additional 78 times speedup when compared to the Modern Laptop. On the other hand, the Old Mid-Range Laptop and the Old Low-Range Laptop performed near identically.

\input{chapters/results/sequential/ntru_hrss701}

By studying the average duration taken by the various subjects in each environment, one may clearly see that \gls{avx2} performed the best when it was available. When \gls{avx2} was not available, the optimized reference implementation performed the best. The results were however indecisive in terms of what compiler produced the most optimized binaries.

\begin{table}
    \centering
    \small
    \caption{Comparison of CPU cycles of our and the submission authors' measurements}
    \label{table:results:sequential:nist-vs-ours}
    \begin{tabularx}{\linewidth}{X c r r r r}
        \toprule
        \thead{Algorithm} & \thead{Operation} & \multicolumn{2}{c}{Reference Cycles} & \multicolumn{2}{c}{AVX2 Cycles}\\
        & & \thead{Ours} & \thead{Theirs} & \thead{Ours} & \thead{Theirs}\\
        \midrule
        \multicolumn{4}{l}{\gls{mceliece}}\\
        mceliece6960119 & keypair & 2063786004 & \multicolumn{1}{c}{-} & 380284953 & 438217685\\
        mceliece6960119f & keypair & 801046672 & \multicolumn{1}{c}{-} & 207228427 & 246508730\\
        mceliece8192128 & keypair & 2227969526 & \multicolumn{1}{c}{-} & 453667430 & 514489441\\
        mceliece8192128f & keypair & 835007190 & \multicolumn{1}{c}{-} & 275660079 & 316202817\\
        
        mceliece6960119(f) & encrypt & 41429828 & \multicolumn{1}{c}{-} & 205954 & 161224\\
        mceliece8192128(f) & encrypt & 40992651 & \multicolumn{1}{c}{-} & 238652 & 178093\\
        
        mceliece6960119(f) & decrypt & 41429828 & \multicolumn{1}{c}{-} & 205954 & 301480\\
        mceliece8192128(f) & decrypt & 40992651 & \multicolumn{1}{c}{-} & 238652 & 326531\\
        \midrule
        \multicolumn{4}{l}{\gls{ntru}}\\
        ntruhps4096821 & keypair & 22392736 & 22511180 & 688900 & 431667\\
        ntruhrss701 & keypair & 16893273 & 16487419 & 320425 & 340823\\
        
        ntruhps4096821 & encrypt & 6782969 & 1566922 & 297125 & 98809\\
        ntruhrss701 & encrypt & 4552302 & 1069326 & 88768 & 50441\\
        
        ntruhps4096821 & decrypt & 18128147 & 4237744 & 104332 & 75384\\
        ntruhrss701 & decrypt & 13511286 & 3113303 & 190756 & 62267\\
        \bottomrule
    \end{tabularx}
\end{table}

In Table \ref{table:results:sequential:nist-vs-ours} the measurements performed by the respective submission authors are compared to the measurements gathered in this thesis. Both the \gls{mceliece} and \gls{ntru} measurements were gathered using \gls{supercop}~\cite{mceliece2020,ntru2020} whilst ours were counted using hardware registers. The authors of the \gls{mceliece} submission did not present any measurements for the reference implementations. Our measurements were performed in the Modern Workstation environment on an Intel i9-9990k. The consumed cycles of the \gls{mceliece} \gls{avx2} implementations were measured by the authors on a Intel Xeon E3-1275 v3. The \gls{ntru} authors used an Intel i3-6100T for the reference implementations and an Intel i7-4770k for the \gls{avx2} implementations.

\subsection{Throughput Performance}

As outlined in section \ref{section:method:experiment:phase2}, the throughput (iterations per second) of the algorithms in each environment under test was evaluated for a series of parallelism configurations. The initial plan was to use the best-performing implementations of each algorithm for each environment, as identified by the sequential benchmarks. The results from the sequential benchmark previously presented show that both the GCC and Clang implementations with the highest available optimization such as \gls{avx2} performed the best. We therefore ran the parallel benchmarks on both GCC and Clang builds of the best-performing variant.

The throughput of the \gls{dhe} implementation was found to consistently slow down after the number of threads surpass the number of physical cores multiplied with the degree of \gls{smt}. In figure \ref{figure:results:throughput:dh-old-mid-range-laptop}, the throughput of the Old Mid-Range Laptop can be seen. Old Mid-Range Laptop, a dual core machine with four threads, leveled off at the four threads mark. As mentioned previously in section \ref{table:method:experiment:phase1:implementation-configurations}, two variants of \gls{dhe} were tested - one compiled and optimized by GCC, the other by Clang - both of which are presented in the aforementioned figure. The performance difference between the two compilers seemed to be negligible, which one may expect as the OpenSSL library used to provide the implementation with virtually all of its functionality is dynamically linked. That means that it is not recompiled nor optimized by the compiler used to compile the program. The same leveling found in Old Mid-Range Laptop was also identified in all of the other environments tested.

\begin{figure}
    \centering
    \includegraphics[scale=0.75]{chapters/results/throughput/Old Mid-Range Laptop_dh.pdf}
    \caption{Throughput of \acrshort{dhe} on Old Mid-Range Laptop for various thread counts}
    \label{figure:results:throughput:dh-old-mid-range-laptop}
\end{figure}

The \gls{ecdhe} implementation was found to be drastically more performant than the \gls{dhe} implementation, with a speedup ranging between 5 to 350 times the throughput. The highest increase in performance was found in the IBM Community Cloud environment, seen in figure \ref{figure:results:throughput:ecdh-ibm-community-cloud}. The throughput, drastically dropped when tested on four or more threads. The peak at two threads coincides with the number of available threads\footnote{It is unknown whether or not the cores are physical cores, logical cores or virtual cores} of the environment. What's more is a 30\% drop in throughput in the exchange phase of the \gls{x25519} implementation compiled and optimized using GCC. The drop was consistent through both of the runs of the complete benchmark at two completely different occasions. It was found that during the \gls{x25519} run on four threads, two achieved half the throughput of the others.

\begin{figure}
    \centering
    \includegraphics[scale=0.75]{chapters/results/throughput/IBM Community Cloud_ecdh.pdf}
    \caption{Throughput of \acrshort{ecdhe} on IBM Community Cloud for various thread counts}
    \label{figure:results:throughput:ecdh-ibm-community-cloud}
\end{figure}

In Table \ref{table:results:throughput:ecdh-25519}, the throughput for each environment is shown for the \gls{x25519} keypair generation. The top value for each row is the average throughput as keypair generation per second and the bottom value is the speedup as compared to the GCC implementation using one thread. As can be seen, the IBM Community Cloud environment outperforms all of the other environments when using one or two threads. The mainframe and cloud hardware tested saw a decrease in throughput at one point or another, whilst the consumer hardware continually performed better with more threads for all the thread counts tested.

\input{chapters/results/throughput/ecdh_25519_keypair}

When looking at the throughput of \gls{mceliece} in environments which do not support \gls{avx2}, such as the IBM Community Cloud presented in figure \ref{figure:results:throughput:mceliece-ibm-community-cloud}, we found that the keypair and decrypt throughput was significantly lower than that of the encrypt stage. The performance of the decrypt stage was roughly $0.1\%$ of the encrypt stage. Furthermore, the parameter set with the largest parameters - \gls{mceliece} 8192128f achieved a higher encryption throughput than the \gls{mceliece} 6960119f variant.

\noindent Furthermore, there was a large difference between the encrypt stage of the subjects compiled and optimized using GCC and those using Clang. Lastly, it seemed as if the throughput leveled off after the number of used threads surpassed the number of available threads. As mentioned, the same overall behavior was found in all of the environments using the optimized reference implementation for benchmarking the parallel throughput, such as Old Mid-Range Laptop in figure \ref{figure:results:throughput:mceliece-old-mid-range-laptop}.

\begin{figure}
    \centering
    \includegraphics[scale=0.75]{chapters/results/throughput/IBM Community Cloud_mceliece.pdf}
    \caption{Throughput of \gls{mceliece} on IBM Community Cloud for various thread counts}
    \label{figure:results:throughput:mceliece-ibm-community-cloud}
\end{figure} 

When \gls{mceliece} ran in environments with support for \gls{avx2}, such as the Modern Workstation shown in figure \ref{figure:results:throughput:mceliece:modern-workstation}, the results were more stable. Not only were the compilers much more consistent with one another, but the difference in throughput between the two variants seemed to become smaller. It is still clear to see, however, that the smallest parameter size had an overall lower throughput for encryption than the largest parameter size tested. The keypair and decrypt results seem to show that the smallest parameter size performed the best.

\begin{figure}
    \centering
    \includegraphics[scale=0.75]{chapters/results/throughput/Modern Workstation_mceliece.pdf}
    \caption{Throughput of \gls{mceliece} on Modern Workstation for various thread counts}
    \label{figure:results:throughput:mceliece-modern-workstation}
\end{figure}

When running \gls{ntru} in parallel to measure throughput, we found that the performance scaled well with regards to the number of threads used. In figure \ref{figure:results:throughput:ntru-modern-workstation}, \gls{ntru} HRSS 701 running in the Modern Workstation environment seemed to have near-linear scaling for the thread counts tested. The variant also performed considerably better than HPS 4096821 in both the keypair generation stage and the encryption stage. The decrypt stage, however, seems to have provided near identical measurements. We found that the same scaling also occured in the Modern Laptop environment.

In Table \ref{table:results:throughput:ntru-hrss701-decrypt}, the measurements for various compilers and thread counts are shown for all of the tested environments. The top value for each row is the throughput as decryptions per second and the bottom value the relative throughput compared to the GCC implementation using one thread. The best scaling was found in the Modern Workstation environment, followed by the Modern Laptop. Although the tested cloud environments supported \gls{avx2}, they did not see the same performance increase when more threads were used. %For all environments except IBM Community Cloud and Cloud Provider 2, GCC seems to have produced the highest increase in throughput over the reference implementation compiled with GCC, without performance optimizations.

\begin{figure}
    \centering
    \includegraphics[scale=0.75]{chapters/results/throughput/Modern Workstation_ntru.pdf}
    \caption{Throughput of \gls{ntru} on Modern Workstation for various thread counts}
    \label{figure:results:throughput:ntru-modern-workstation}
\end{figure}

\input{chapters/results/throughput/ntru_hrss701_decrypt}

\subsection{Micro-benchmarks}

\begin{table}
    \centering
    \footnotesize
    \caption{Monitored functions}
    \label{table:results:performance:micro-functions}
    \begin{tabularx}{\linewidth}{l X}
        \toprule
        \thead{Name} & \thead{Description} \\
        \midrule
        \multicolumn{2}{c}{\thead[l]{\gls{mceliece} and \gls{ntru}}} \\
        %\midrule
        crypto\_kem\_keypair & Generate a keypair \\
        crypto\_kem\_enc & Generate and encapsulate a key \\
        crypto\_kem\_dec & Decapsulate an encapsulated key \\
        \midrule
        \multicolumn{2}{c}{\thead[l]{\gls{mceliece}}} \\
        pk\_gen & Generate the public-key\\
        gen\_e & Generate an error vector of a specific weight (encryption)\\
        syndrome & Create the syndrome using the public key and an error vector (encryption)\\
        % syndrome\_asm & The same as syndrome, but implemented in assembly targeting AVX2\\
        root & A polynomial at one or more field elements are evaluated (decryption)\\
        synd & Syndrome computation (decryption)\\
        \midrule
        \multicolumn{2}{c}{\thead[l]{\gls{ntru}}} \\
        poly\_Rq\_mul & Multiply a polynomial with another in $\mathbb{R}_q$\\
        poly\_S3\_inv & Invert a polynomial in $\mathbb{S}_3$\\
        randombytes & Retrieve uniformly random bytes \\
        poly\_Rq\_inv & Invert a polynomial in $\mathbb{R}_2$\\
        poly\_R2\_inv & Invert a polynomial in $\mathbb{R}_2$\\
        poly\_R2\_inv\_to\_Rq\_inv & Lift an inverted polynomial from $\mathbb{R}_2$ to $\mathbb{R}_q$ \\
        poly\_Sq\_mul & Multiply a polynomial in $\mathbb{S}_q$ with another\\
        \bottomrule
    \end{tabularx}
\end{table}

In section \ref{section:method:experiment:phase1:implementation-configurations} it was written that the monitored functions for the micro benchmark would be based off of data found in the hot paths analysis. In the end, the functions presented in Table \ref{table:results:performance:micro-functions} were monitored during the micro benchmark. Note that although the randombytes function is available in both \gls{ntru} and \gls{mceliece}, it was found in section \ref{section:results:hot-paths} to not be significant enough for \gls{mceliece} to warrant further analysis. Note also that the gf\_add and gf\_mul are missing from the \gls{mceliece} monitored functions. Due to their extremely high invocation count, an equally extreme overhead was induced. This made the process of measuring these regions infeasible.

To evaluate what changes in the implementations contribute to a certain speedup, micro-benchmarks were run as further described in section \ref{section:method:experiment:phase1}. One of the micro-benchmarks performed was the counting of the number of cache misses occurring in an environment. The cache misses for various environments and performance optimizations for \gls{mceliece} 8192128f keypair generation is presented in Table \ref{table:results:micro:cache-misses-mceliece-8192128f-enc}. The same stage for the \gls{ntru} HRSS 701 implementations tested are presented in Table \ref{table:results:micro:cache-misses-ntru-hrss701-enc}. In both of the tables, the Old Low-Range Laptop is missing for some configurations, due to it failing to complete the micro benchmarks a multitude of times. Not all environments supported measuring the cache misses. Therefore the Cloud Provider 1 and IBM Community Cloud environments are not presented.

The number of cache misses in \gls{mceliece}, when compared to \gls{ntru}, was substantially higher with comparable means being up to 190 times as many. The standard deviation was also considerably higher than those found in \gls{ntru} benchmarks. Despite running several generations older hardware with less then half the cache of the Modern Laptop, the Old Low-Range Laptop seem to consistently have fewer cache misses. The only cloud hardware tested, Cloud Provider 2, had an unknown amount of cache available for use during the benchmark, it being a shared environment with virtual cores. The Cloud Provider 2 saw an increased number of cache misses when compared to dedicated consumer hardware such as the Old Mid-Range Laptop, the Old Low-Range Laptop and the Modern Workstation.

\input{chapters/results/micro/mceliece_8192128f_crypto_kem_enc}

With an average of 69116705 cache misses when the GCC reference implementation was run in the Cloud Provider 2 environment, keypair generation in \gls{mceliece} 8192128 had the largest number of cache misses recorded during our testing. For the same algorithm and compiler configuration, the Modern Workstation had an average of 20403 cache misses. As the 8192128 parameter set of \gls{mceliece} is the systematic variant, it may run further iterations than the semi-systematic variant of \gls{mceliece}. As this is the only difference between the 8192128 and the 8192128f parameter set, it is likely to be the cause of the higher number of cache misses. In the lower end of the spectrum, the Modern Workstation saw an average of a single cache miss for all configurations of \gls{ntru} HPS 4096821 keypair generation. The environment had the largest amount of cache out of all the dedicated hardware tested. Using the same configurations, the Cloud Provider 2 environment saw an average of between 1440 and 3468 cache misses. As dedicated hardware had similar amounts of cache misses as the tested cloud environments, it may be difficult to directly determine if the type of environment affects the number of cache misses.

That the \gls{mceliece} implementations had a drastically larger amount of cache misses than the \gls{ntru} implementations was found in all of the tests of the experiment.

\noindent Another value we measured was the number of page-faults that occurred during each stage of the algorithms. As with other micro-benchmarks, this data was collected for the \gls{post-quantum} \glspl{kem} in environments with support for measuring them.

\gls{ntru} had zero page-faults in all of the runs, across all implementations and environments. Like \gls{ntru}, \gls{mceliece} had zero page-faults, except during key-pair generation. In Table \ref{table:results:micro:page-faults-mceliece} the mean of each parameter set is presented for the implementations which had page faults. All page faults occured during the keypair stage and for the GCC compiler.

\input{chapters/results/micro/page-faults}

In figure \ref{figure:results:micro:mceliece-8192128f-cloud-provider-2} the CPU cycles consumed for various regions of code in the \gls{mceliece} 8192128f implementation run on Cloud Provider 2 may be seen. The number of CPU cycles used for the unoptimized reference implementation compiled with GCC during keypair generation was about 30\% of the decrypt stage. In comparable implementations such as those presented in figure \ref{figure:results:micro:mceliece-8192128f-modern-workstation}, the value was found to be much closer to one another. The Cloud Provider 2 environment was the only environment where such a drastic change in consumed CPU cycles may be seen between the two stages. As the Cloud Provider 2 used the same compiler versions as the environments it outperformed, it is unlikely that the performance gain came from a difference in versions. As the increase was consistent throughout all of the measurements, the performance may be connected to an optimization made for the specific platform not found in other environments.

\begin{sidewaysfigure}
    \centering
    \includegraphics[width=\textwidth]{chapters/results/micro/mceliece_8192128f_Cloud Provider 2_cpu-cycles.pdf}
    \caption{CPU cycles consumed for \gls{mceliece} 8192128f on Cloud Provider 2}
    \label{figure:results:micro:mceliece-8192128f-cloud-provider-2}
\end{sidewaysfigure}

In just about all cases, implementations optimized by Clang performed better than the comparable GCC implementation. In most other cases Clang's optimizations performed similar to GCC's. In some few cases, however, Clang was found to yield a binary which performed significantly worse than the GCC optimization. In figure \ref{figure:results:micro:ntru-hps4096821-modern-laptop}, for example, the reference implementation optimized by Clang required 13 times the cycles of the GCC implementation for the poly\_R2\_inv region.

When comparing the HPS 4096821 and the HRSS 701 parameter sets of \gls{ntru}, it was found that the performance was close to identical for each of the studied optimizations. The relationship to the previously mentioned figure may be seen in figure \ref{figure:results:micro:ntru-hrss701-modern-laptop} where the \gls{ntru} HRSS 701 algorithm was run in the Modern Laptop environment. 

\begin{sidewaysfigure}
    \centering
    \includegraphics[width=\textwidth]{chapters/results/micro/ntru_hps4096821_Modern Laptop_cpu-cycles.pdf}
    \caption{CPU cycles consumed for \gls{ntru} HPS 4096821 on Modern Laptop}
    \label{figure:results:micro:ntru-hps4096821-modern-laptop}
\end{sidewaysfigure}
