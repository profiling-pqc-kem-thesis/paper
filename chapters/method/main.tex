\chapter{Method}
\label{chapter:method}

\section{Selected Methods}
\label{section:method:selected-method}

%Argue for one or more methods that are applicable for our questions...
We are using two methods to answer our three research questions - a literature study and an experiment.

A literature study will be conducted to answer \researchquestion{2} and \researchquestion{3}. We have chosen a study based in literature as the scope of the question, as well as our limited access to mainframe hardware, makes it difficult for us to perform a practical study of the performance potentials of using various hardware features.

To provide information to help us answer \researchquestion{1}, we will conduct an experiment in form of a performance test. An experiment was chosen as we may isolate variables and control the environment in which the study is conducted. Furthermore, we are only interested in the raw performance metrics from different computer platforms. Therefore, a survey would not be relevant because we are not interested in user experience, rather the algorithms' performance on various platforms. Neither is a case study relevant because it does not test any hypotheses, and it is mostly used in social sciences\cite{wohlin2012}\todo{revisit motivation}.

\section{Literature Study}
\label{section:method:literature-study}

As previously mentioned, we aim to answer \researchquestion{2} and \researchquestion{3} by conducting a literature study. We will limit the scope to the four finalists in \gls{nist}'s standardization process, round 3.

By studying the algorithms' underlying mathematics, the authors' own optimizations as well as relevant literature on cryptography optimization, we aim to identify what parts of the algorithms are possible and suitable to optimize.

When potential areas of improvements have been identified, literature will be studied to find relevant methods available on \gls{z15}, such as specialized instruction sets and other hardware features. IBM's official documentation as well as research conducted by third parties will be studied. That way, we hope to get a balanced view of the capabilities of the platform.

To identify relevant research papers, we will search for peer-reviewed papers by using databases such as Scopus and Web of Science\todo{update databases used}. By searching for relevant keywords such as \textit{post-quantum cryptography}, \textit{cpacf performance}, \textit{opencryptoki hsm}\todo{update keywords}, we will select papers that seem relevant based on their title and abstracts. We will then read the selected papers in their entirety to determine the quality and relevance.

The selected papers, as well as the the papers submitted to \gls{nist}, will be included in a start set. The start set will then be used for forward and backward snowballing - a useful technique to find more papers that are relevant to provide information to help answer our research question\cite{wohlin2014}.
\todo[inline]{Kommer ni att ha specificerade inclusion/exclusion criteria?}

\section{Experiment}
\label{section:method:experiment}

\subsection{Goal}
\label{section:method:experiment:goal}

Our goal with the experiment is to analyze \gls{post-quantum} \glspl{kem} for the purpose of evaluating the readiness of today's consumer, mainframe and cloud hardware with the respect to their performance from the point of view of businesses and professionals, in the context of replacing the \glspl{kex} used today in TLS, SSH, VPNs and other applications.

The experiment has two phases. In phase one, we aim to study the effects of changing variables possibly related to the performance of the subjects. By studying the performance characteristics of a single run of the algorithms, we also aim to be able to identify what implementation performs the best.

In the second phase, we aim to test the performance characteristics of the algorithms in a parallel fashion. By evaluating the performance of the algorithms when performed in parallel over multiple threads, we aim to identify how the underlying platform takes advantage of the available hardware to run the best optimized implementation as identified in the first phase.

\subsection{Subjects}
\label{section:method:experiment:subjects}

The test subjects of the experiment are two of the post-quantum algorithms as submitted to \gls{nist}'s third round of submissions, namely \gls{mceliece} and \gls{ntru}. Three of the four finalists - \gls{ntru}, \gls{crystals-kyber}, and \gls{saber}, are lattice-based algorithms, and at most one of these will be standardized. We selected \gls{ntru} based on the positive comments it received and the negative comments the other contestants received from \gls{nist}. \gls{mceliece} was chosen as it was the only non-lattice-based finalist and it has a long and excellent reputation\cite{nist2020}.

The \gls{nist} submissions under test are presented in table \ref{table:method:experiment:phase1:test-subjects}. All of the selected parameter sets for the subjects have been recommended by their respective authors in the third round of submissions. In the case of \gls{mceliece}, we also rely on previous work from the EU-funded project \gls{pqcrypto}, which identified that not all parameter sets recommended by the authors are appropriate. That is, \gls{pqcrypto} identified that the key size should be a minimum of one megabyte, meaning some candidates were excluded\cite{eu2015}.

\begin{table}[H]
    \centering
    \caption{Test Subjects}
    \label{table:method:experiment:phase1:test-subjects}
    \begin{tabularx}{\linewidth}{l c X}
        \toprule
        \thead{Name} & \thead{Parameter Set} & \thead{Comment} \\
        \midrule
        \gls{mceliece} & 6960119 & \\
        \gls{mceliece} & 8192128 & \\
        \gls{mceliece} & 6960119f & Differs in the key-generation only\\
        \gls{mceliece} & 8192128f & Differs in the key-generation only\\
        \gls{ntru} & hrss701 & \\
        \gls{ntru} & hps2048677 & \\
        \bottomrule
    \end{tabularx}

\end{table}

As previously mentioned in section \ref{section:background:classical-cryptography}, there are several recommended classical \glspl{kex}. These recommended algorithms and parameter sets will constitute the control subjects for this study and are presented in table \ref{table:method:experiment:phase1:control-subjects}. Although the control subjects may not be directly comparable to the \glspl{kem}, we hope to provide context for how these classical algorithms perform.

\begin{table}[H]
    \centering
    \caption{Control Subjects}
    \label{table:method:experiment:phase1:control-subjects}
    \begin{tabularx}{\linewidth}{l X}
        \toprule
        \thead{Name} & \thead{Parameter Set} \\
        \midrule
        \gls{ecdhe} & \gls{p-256}, \gls{x25519} \\
        \gls{dhe} & \gls{rsa} with 2048 bit keys \\
        \bottomrule
    \end{tabularx}
\end{table}

In the case of the \gls{nist} submissions, in this context, the submitted implementations are not thought to be production-ready due to them not yet being standardized nor battle-tested. This, in addition to our interest in exploring optimization techniques that are applicable for post-quantum cryptography, results in us exploring various ways the implementations may be optimized.

As the control subjects are intended to represent the algorithms already in use, they are thought to be a solved problem. That is, in this context, implementation details are of no concern - an implementation on the native platform is assumed to be optimized and production-ready.

\subsection{Target Environments}
\label{section:method:experiment:environments}

To fulfil the goal of the experiment, several hardware configurations will be tested. These configurations have been divided into three categories, namely consumer, cloud and mainframe hardware.

The consumer hardware was chosen to represent different tiers of personal computers in use for different purposes. The cloud hardware was chosen to represent various cloud providers' Virtual Private Server offerings - a common target for many applications \cite{eurostat2021}. The offerings are all virtual CPUs which run through hypervisors in a shared hosting environment. The selected mainframe hardware is intended to represent the latest mainframes available from IBM at the time of writing.

Where possible and applicable, the environments run on the same version of Ubuntu (20.04) and use the same versions of GCC (9.3.0) and Clang (10.0.0). The exact version of each tool in each environment is available in the data published alongside this work in a GitHub repository\footnote{\href{https://github.com/profiling-pqc-kem-thesis/data}{https://github.com/profiling-pqc-kem-thesis/data}}.

\begin{table}[H]
    \centering
    \caption{Consumer Hardware - x86}
    \label{table:method:experiment:phase1:consumer-hardware}
    \begin{tabularx}{\linewidth}{X c c c}
        \toprule
        \thead{Label} & \thead{CPU} & \thead{RAM} & \thead{OS}\\
        \midrule
        Modern Workstation & Intel i9-9900k & 32GB 3600MHz DDR4 & Ubuntu 20.04\\
        Modern Laptop & Intel i7-8565u & 16 GB 2600MHz DDR4 & Ubuntu 20.04\\
        Old Mid-Range Laptop & Intel i5-3230m & 8GB 1600MHz DDR3 & Ubuntu 20.04\\
        Old Low-Range Laptop & Intel i3-3120m & 8GB 1600MHz DDR3 & Ubuntu 20.04\\
        \bottomrule
    \end{tabularx}
\end{table}

\begin{table}[H]
    \centering
    \caption{Cloud Hardware - x86}
    \label{table:method:experiment:phase1:server-hardware}
    \begin{tabularx}{\linewidth}{X c c c}
        \toprule
        \thead{Label} & \thead{CPU} & \thead{RAM} & \thead{OS}\\
        \midrule
        Cloud Provider 1\footnotemark & Intel Xeon 8168, 4 \gls{vcpu} & 4GB & Ubuntu 18.04\footnotemark\\
        Cloud Provider 2\footnotemark & Intel Xeon 5118\footnotemark, 2 \gls{vcpu} & 4GB & Ubuntu 20.04\\
        \bottomrule
    \end{tabularx}
\end{table}
\addtocounter{footnote}{-4}
\addtocounter{footnote}{1}
\footnotetext{An Azure "Fsv2" Virtual Private Server in USA East}
\addtocounter{footnote}{1}
\footnotetext{The provider offered Ubuntu 20.04, but at an additional cost}
\addtocounter{footnote}{1}
\footnotetext{A DigitalOcean "Basic \$20 Droplet" in Amsterdam}
\addtocounter{footnote}{1}
\footnotetext{DigitalOcean does not disclose the exact CPU model used, but by using the available metrics we found one likely model}

\begin{table}[H]
    \centering
    \caption{Mainframe Hardware}
    \label{table:method:experiment:phase1:mainframe-hardware}
    \begin{tabularx}{\linewidth}{X c c c c}
        \toprule
        \thead{Label} & \thead{Model} & \thead{CPU} & \thead{RAM} & \thead{OS}\\
        \midrule
        IBM Community Cloud\footnotemark & IBM T01 & IBM \gls{z15} 2 cores\footnotemark & 4GB & RHEL 8.3\footnotemark\\
        \bottomrule
    \end{tabularx}
\end{table}
\addtocounter{footnote}{-3}
\addtocounter{footnote}{1}
\footnotetext{A "General Purpose VM" accessed via LinuxOne Community Cloud}
\addtocounter{footnote}{1}
\footnotetext{It is unknown if they are dedicated or virtual}
\addtocounter{footnote}{1}
\footnotetext{The provider did not offer an Ubuntu distribution}

\subsection{Phase One - Profiling}
\label{section:method:experiment:phase1}

In the first phase, we aim to identify how possibly performance-related variables alter the performance characteristics of the subjects. The performance of the algorithms in a single sequential run will be evaluated.

\subsubsection{Variables}
\label{section:method:experiment:phase1:variables}

The independent variables that were identified to potentially be related to performance are presented in table \ref{table:method:experiment:phase1:independent-variables}.

As the thesis is interested in the performance characteristics of consumer, cloud and mainframe hardware, FPGA implementations have been left out from the table. Furthermore, older vectorization extensions sets such as SSE is not considered as they have been superseeded by AVX.
\begin{table}[H]
    \centering
    \caption{Independent Variables}
    \label{table:method:experiment:phase1:independent-variables}
    \begin{tabularx}{\linewidth}{l>{\RaggedRight}l>{\RaggedRight}X>{\RaggedRight\arraybackslash}X}
        \toprule
        \thead{Group} & \thead{Label} & \thead{Potential Values} & \thead{Comment} \\
        \midrule
        Compiler & Compilers & GCC, Clang, IBM XL, Intel C++ Compiler (ICC) & \\
        Compiler & Flags & -O0, -O1, ..., -O5, -march, -mtune, -fomit-frame-pointer, -fwrapv, -fPIC, -fPIE, -mcpu, -Qunused-arguments & There are virtually infinite possible flags. These are the minimal ones used for optimization in this context and as used by previous work (\gls{supercop}, \gls{nist} publications) \\
        Implementation & Libraries & \gls{sha3} / \gls{shake} / \gls{keccak}, \gls{aes-instruction-set}, \gls{cpacf} & \\
        Implementation & Optimizations & \gls{avx}, \gls{avx2}, \gls{avx512}, \gls{simd} (GPU), \gls{simd} on \gls{ibmz}, \gls{hsm} & These are the optimizations available in the selected implementations \\
        Environment & OS & OS type and version, scheduling etc. & \\
        Environment & Hardware & CPU model, CPU architecture, available cores, available RAM, RAM model and specification & \\
        \bottomrule
    \end{tabularx}
\end{table}


\begin{table}[H]
    \centering
    \caption{Dependent Variables}
    \label{table:method:experiment:phase1:dependent-variables}
    \begin{tabularx}{\linewidth}{l c X}
        \toprule
        \thead{Group} & \thead{Stages} & \thead{Comments}\\
        \midrule
        Throughput & \makecell{Key-pair generation,\\Encapsulation/Decapsulation,\\Exchange} & CPU cycles, instruction count, wall-clock time, cache misses, page faults\\
        Hardware Utilization & \makecell{Key-pair generation,\\Encapsulation/Decapsulation,\\Exchange} & Memory usage (heap and stack allocation), logical core utilization\\
        \bottomrule
    \end{tabularx}
\end{table}
\todo{update - not final}
\todo{Update the above table with correct values}

\todo[inline]{Förtydliga att hot-paths enbart utförs på referens-implementationen på en plattform. Förtydliga att minneskontroll enbart utförs på de enkla beräkningar och att ingen minneskontroll när vi kör i bulk. Minneskontroll görs inte vid multicore-benchmark p.g.a. att vi allokerar minne o.s.v.}

\subsubsection{Selected Toolset - Processor}
\label{section:method:experiment:phase1:selected-toolset-processor}
The instrumentation will rely on the Linux perf (perf\_event\_open) API. This API is part of the kernel and produces highly accurate measurements, regardless of the underlying hardware. The recorded values for CPU cycles, instructions etc. may be counted or sampled. The API functions externally, by monitoring the entire lifetime of a process, or internally by instrumenting a program with specifically measured blocks of code.\todo{Cite this properly?}

As such the Linux perf API was identified as the only applicable tool for profiling the CPU usage of an algorithm. To help aid the usage of the API, we used a small tool\footnote{\href{https://github.com/profiling-pqc-kem-thesis/perforator}{https://github.com/profiling-pqc-kem-thesis/perforator}} which enabled us to monitor the algorithms during their lifetime.

Valgrind (Callgrind) was identified as usable for identifying hot paths in the code. The tool was run on a single iteration of the reference implementations' tests to produce metrics of the call stack. The data was then analyzed manually in KCacheGrind and QCacheGrind to produce data for graphs of the hot paths.

\subsubsection{Rejected Toolset - Processor}
\label{section:method:experiment:phase1:rejected-toolset-processor}
Other sampling-based measurement alternatives that require the tested binary to be instrumented, such as prof, gprof and gperftools are unable to accurately measure IO-bound tasks, the retired instructions or time spent inside of the kernel\footnote{\href{https://ftp.gnu.org/old-gnu/Manuals/gprof-2.9.1/html\_mono/gprof.html}{https://ftp.gnu.org/old-gnu/Manuals/gprof-2.9.1/html\_mono/gprof.html}}

Another, non-sampling-based tool that monitor the entire lifetime of a process, Valgrind and its sub-tool Callgrind was found to not support running all of our samples on all targeted environments and was therefore deemed inappropriate for use as the main instrumentation tool. Callgrind can and will however be used for identifying the hot paths of the reference implementations as the implementation can be run in a working environment with its results general to wherever the implementation may be run.\todo{Revisit Valgrind comments}

Other libraries or APIs such as those available in \gls{supercop} and PAPI were identified via testing and a source code analysis to provide inaccurate and non-deterministic results and were therefore not applicable for this use case. In the case of \gls{supercop}, some measurements were identified to rely on extrapolation and estimation of data which led us to reject the tool.

\subsubsection{Selected Toolset - Memory}
\label{section:method:experiment:phase1:selected-toolset-memory}

To identify the heap usage we resorted to heaptrack, a tool which allowed us to intercept each heap allocation and deallocation. The tool was found to be simple to build and run on all of our target environments, without yielding a significant overhead. Furthermore, the accompanying tools let us easily automate the process of identifying heap usage on a per-function basis.

For stack analysis we aimed to identify what amount of memory was required to use a specific algorithm. To identify the stack usage we resorted to statically analyzing the assembly code and symbol table produced by the compilers used. The symbol table includes the size of each (non-inlined) function and the assembly code could be used to identify the stack allocation made by each function. The script we wrote to solve this automatically is produced is published alongside this work.

\subsubsection{Rejected Toolset - Memory}
\label{section:method:experiment:phase1:rejected-toolset-memory}

Intel's vTune is a profiling suite with support for x86. It was found to work well for gathering memory-related metrics of the CPU during execution of a program. It was unavailable on \gls{s390x}, however. It was therefore rejected as we could rely on other tools to yield cross-platform metrics. 

As previously mentioned, Valgrind is a non-sampling-based tool which enables it to provide accurate measurements. Although we found it was not applicable for us to use for processor profiling, it was identified as a common and popular tool for analyzing memory. In the end we found that, although Valgrind comes with a stack analyzer, we were more interested in the pre-allocated stack usage than the runtime stack usage. Furthermore, heaptrack ended up providing simpler tools to work with and helped us identify allocations on a per-function basis automatically.

\subsubsection{Measurements}
\label{section:method:experiment:phase1:measurements}

Using the aforementioned toolset, we will monitor the subjects and measure the following values\footnote{Not all platforms support all events}.

\begin{itemize}
    \item Retired instruction count
    \item Total cycle count
    \item Page fault count
    \item Cache miss count
    \item Task clock
    \item Heap allocation
    \item Stack usage
    \item Wall clock time
\end{itemize}

The measurements will be taken at different points in the algorithms' lifecycles - such as when the algorithm is first invoked. The same measurements are to be repeated for all of the stages of the algorithms as presented in table \ref{table:method:experiment:phase1:dependent-variables}. By first analyzing the hot paths of the \gls{nist} submissions, we will also measure the parts of the algorithm that are invoked the most. As some variables in the environment have been identified, such as the various implementations for \gls{shake} and \gls{aes}, the use of these will be measured as well. In some cases, the authors have provided optimized implementations. In such cases, these optimized functions will be measured as well.

As we've identified that some tools may introduce overhead when monitoring a program, we have decided to split the tests into four parts. These parts are presented in Table \ref{table:method:experiment:phase1:benchmark-divisions}. By splitting the benchmarks we hope to capture accurate values for each measurements without affecting others.

\begin{table}[H]
    \centering
    \caption{Benchmark Divisions}
    \label{table:method:experiment:phase1:benchmark-divisions}
    \begin{tabularx}{\linewidth}{l>{\RaggedRight}X>{\RaggedRight\arraybackslash}X}
        \toprule
        \thead{Name} & \thead{Focus} & \thead{Measurements}\\
        \midrule
        Micro benchmark & Measurements on a per-function basis & Retired instruction count, total cycle count, page fault count, cache miss count, task clock \\
        Sequential benchmark & Accurate times & Wall clock time \\
        Heap benchmark & Heap allocation on a per-function basis & Heap allocation \\
        Stack benchmark & Stack allocation on a per-function basis & Stack usage \\
        \bottomrule
    \end{tabularx}
\end{table}

The measurement will be repeated 1000 times for each stage of the algorithms and each part of the benchmark\footnotemark{The heap benchmark will only run 10 iterations as the heap usage should be deterministic. Furthermore, the cost of running further iterations was deemed too large}. If the total duration of the benchmark surpassed 15 minutes, the test was set to timeout and the results recorded until then used. The timeout was calculated based on the time we had allocated for running the benchmarks. To not let one iteration affect another, we let the system under test idle between each benchmark during one minute. From some early testing we identified that one minute was sufficient for the systems to cool down to a level similar to that before. One minute was also the largest amount of time we could afford to wait, as waiting any longer would make the benchmarks surpass our allocated budget.

For the exact instrumentation used in each subject, please refer to the source code published alongside this work\footnote{\href{https://github.com/profiling-pqc-kem-thesis/subjects}{https://github.com/profiling-pqc-kem-thesis/subjects}}.

\subsubsection{Compiler Configurations}
\label{section:method:experiment:phase1:compiler-configurations}

The compilers were configured using a series of performance-related flags. The flags, presented in Table \ref{table:method:experiment:phase1:compilers}, are non-exhaustive, but have been identified to imply most of the performance-related options. With IBM XL on Power, for example, -O5 implies automatic \gls{simd} via -qsimd=auto and improved data locality via -qhot \cite{ibmxl2018}. Using GCC, -O3 implies loop unrolling and jam transformations via -floop-unroll-and-jam, strict alignment of functions via -fstrict-aliasing etcetera \cite{gcc2021}. We opted against using the flag -ffastmath as it was documented as an unsafe option for programs relying on the compliance of IEEE or ISO specifications for math functions \cite{gcc2021}.

\begin{table}[H]
    \centering
    \caption{Compiler Configurations}
    \label{table:method:experiment:phase1:compilers}
    \begin{tabularx}{\linewidth}{l c c X}
        \toprule
        \thead{Label} & \thead{Name} & \thead{Flags}\\
        \midrule
        GCC Minimal & GCC & Minimal required\\
        Clang Minimal & Clang & Minimal required\\
        IBM XL Minimal & IBM XL & Minimal required\\
        GCC Optimized & GCC & -march=native -mtune=native -O3\\
        Clang Optimized & Clang & -march=native -mtune=native -O3\\
        IBM XL Optimized & IBM XL & -march=native -mtune=native -O5\\
        \bottomrule
    \end{tabularx}
\end{table}

In order to see the exact flags used to compile each sample, please refer to the source code for the experiment\footnote{\href{https://github.com/profiling-pqc-kem-thesis}{https://github.com/profiling-pqc-kem-thesis}}.

\subsubsection{Implementation Configurations}
\label{section:method:experiment:phase1:implementation-configurations}

The implementation details presented in Table \ref{table:method:experiment:phase1:implementation-configurations} will be changed in order to evaluate potential performance differences.

\begin{table}[H]
    \centering
    \caption{Implementation configurations}
    \label{table:method:experiment:phase1:implementation-configurations}
    \begin{tabularx}{\linewidth}{l>{\RaggedRight}X>{\RaggedRight\arraybackslash}X}
        \toprule
        \thead{Label} & \thead{Description}\\
        \midrule
        Reference Implementation & The reference implementation as published in the \gls{nist} submissions, or the available \gls{openssl} implementation. \\
        Optimized Implementation(s) & Use the authors' provided optimizations.\\
        Optimized \gls{shake} implementation & Use a \gls{keccak} implementation optimized for the native platform (plain, \gls{avx2}, \gls{avx512}).\\
        Optimized \gls{aes} implementation & Exchange the included \gls{aes} implementation with \gls{openssl} which is optimized for the native platform (plain, \gls{aes}, \gls{aes-ni}).\\
        Fully optimized implementation & Optimized implementation, optimized \gls{shake} implementation, optimized \gls{aes} implementation.\\
        \bottomrule
    \end{tabularx}
\end{table}
\todo{The above table requires more up-to-date information.}

\subsubsection{Configuration Summary}
\label{section:method:experiment:phase1:configuration-summary}

\begin{table}[H]
    \centering
    \caption{Configuration Summary}
    \label{table:method:experiment:phase1:configuration-summary}
    \begin{tabularx}{\linewidth}{l X X}
        \toprule
        \thead{Hardware} & \thead{Compiler Configurations} & \thead{Implementation Configurations}\\
        \midrule
        Modern Workstation & GCC Minimal, GCC Optimized, Clang Minimal, Clang Optimized & All (\gls{avx2}, \gls{aes-instruction-set})\\
        Modern Laptop & GCC Minimal, GCC Optimized, Clang Minimal, Clang Optimized & All (\gls{avx2}, \gls{aes-instruction-set}) \\
        Old Mid-Range Laptop & GCC Minimal, GCC Optimized, Clang Minimal, Clang Optimized & All (\gls{avx}, \gls{aes-instruction-set})\\
        Old Low-Range Laptop & GCC Minimal, GCC Optimized, Clang Minimal, Clang Optimized & All (\gls{avx})\\
        Cloud Provider 1 & GCC Minimal, GCC Optimized, Clang Minimal, Clang Optimized & All (\gls{avx512}, \gls{aes-ni})\\
        Cloud Provider 2 & GCC Minimal, GCC Optimized, Clang Minimal, Clang Optimized & All (\gls{avx512}, \gls{aes-ni})\\
        IBM Community Cloud & GCC Minimal, GCC Optimized, IBM XL, IBM XL Optimized & All (\gls{cpacf})\\
        \bottomrule
    \end{tabularx}
\end{table}
\todo{Update with accurate information}

\subsection{Phase Two - Parallelism}
\label{section:method:experiment:phase2}

In phase two, the performance characteristics of the best-performing algorithms from the first phase will be tested in a parallel environment. We aim to identify how the underlying platform takes advantage of the available hardware to run the subjects in a parallel fashion. The same toolset as previously presented was used in this phase.

\subsubsection{Variables}
\label{section:method:experiment:phase2:variables}

The independent variables that were identified to potentially be related to performance are presented in table \ref{table:method:experiment:phase2:independent-variables}. As previously mentioned, the algorithms under test will be run in parallel. As the implementations are selected based on the result from the first phase, several independent variables identified in the first phase is omitted and not of interest in the second.

\begin{table}[H]
    \centering
    \caption{Independent Variables}
    \label{table:method:experiment:phase2:independent-variables}
    \begin{tabularx}{\linewidth}{l>{\RaggedRight}l>{\RaggedRight\arraybackslash}X}
        \toprule
        \thead{Group} & \thead{Label} & \thead{Comment} \\
        \midrule
        Parallelism & Thread Count & Number of threads running sequential algorithms at the same time \\
        Environment & Hardware & CPU model, CPU architecture, available cores, available RAM, RAM model and specification \\
        Environment & OS & OS type and version, scheduling etc. \\
        \bottomrule
    \end{tabularx}
\end{table}

In table \ref{table:method:experiment:phase2:dependent-variables}, the dependant variables of interest are presented. These are the variables that relate to the goal as specified in section \ref{section:method:experiment:goal}. Note that, as \gls{ecdh} and \gls{dh} are not \glspl{kem}, but \gls{kex} algorithms - they have an exchange phase, not an encapsulation nor decapsulation phase. For implementation details, please refer to section \ref{section:method:experiment:phase2:measurements}.

\begin{table}[H]
    \centering
    \caption{Dependent Variables}
    \label{table:method:experiment:phase2:dependent-variables}
    \begin{tabularx}{\linewidth}{l X}
        \toprule
        \thead{Measurement} & \thead{Stages}\\
        \midrule
        Throughput & \makecell{Key-pair generation,\\Encapsulation/Decapsulation,\\Exchange}\\
        Hardware Utilization & \makecell{Key-pair generation,\\Encapsulation/Decapsulation,\\Exchange}\\
        \bottomrule
    \end{tabularx}
\end{table}

The hardware utilization of the system will not be considered for this test, instead we focus on how the throughput differs between architectures and environments.

\subsubsection{Measurements}
\label{section:method:experiment:phase2:measurements}

Using the aforementioned toolset, we will instrument the subjects and measure the wall clock time whilst running the benchmark. To evaluate the parallel performance behavior of the algorithm and environment, a simple tool was written. The tool creates a number of threads and lets each thread perform the algorithm under test for 100 iterations. The total wall-clock time for all threads to complete their iterations will be used to calculate the throughput. If the total duration of the benchmark surpassed 15 minutes, the test was set to timeout and the results recorded until then used. The timeout was calculated based on the time we had allocated for running the benchmarks. To not let one iteration affect another, we let the system under test idle between each benchmark during one minute. From some early testing we identified that one minute was sufficient for the systems to cool down to a level similar to that before. One minute was also the largest amount of time we could afford to wait, as waiting any longer would make the benchmarks surpass our allocated budget.

For the exact instrumentation used in each subject, please refer to the source code published alongside this work\footnote{\href{https://github.com/profiling-pqc-kem-thesis/subjects}{https://github.com/profiling-pqc-kem-thesis/subjects}}.

\subsubsection{Configurations}
\label{section:method:experiment:phase2:configurations}

The best-performing implementation for each subject as identified in phase one will be tested on a series of parallelism configurations, on each environment specified in section \ref{section:method:experiment:environments}.

The parallelism configurations is synonymous with the number of threads used and will depend on the degree of \gls{smt} and available physical cores on the target environment. The number of threads will start of at one and double, up to the product of the number of physical cores and the \gls{smt} degree, multiplied by four.

For a machine with two physical cores and a \gls{smt} degree of two, this calculation yields the thread counts $1$, $2$, $4$, $8$ and $16$.