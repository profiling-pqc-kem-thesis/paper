\chapter{Method}
\label{chapter:method}

This chapter explains and motivates the chosen research questions and methods in greater depth.

Section \ref{section:method:selected-method} discusses the selected research method. The application of the selected method is then discussed and explained in greater depth in section \ref{section:method:literature-study} as well as section \ref{section:method:experiment}.

The research questions are listed below for the reader's convenience.

\begin{itemize}
    \item Does performance of \gls{post-quantum} cryptography algorithms differ between architectures and if so, how?
    \item What specialized instructions and features applicable for \gls{post-quantum} cryptography are available in \gls{z15} and how are they used in context?
    \item What techniques may be used to increase performance of \gls{post-quantum} cryptography algorithms for different architectures?
\end{itemize}

\section{Selected Methods}
\label{section:method:selected-method}

%Argue for one or more methods that are applicable for our questions...
We are using two methods to answer our three research questions - a literature study, and an experiment.

A literature study will be conducted to answer our research question, "What specialized instructions and features applicable for post-quantum cryptography are available in \gls{z15}, and how are they used in context?". We have chosen a study based in literature as the scope of the question, as well as our limited access to mainframe hardware makes it difficult for us to perform a practical study in the performance potentials of using various hardware features.\todo{revisit}

To provide information to help us answer our research questions "What specialized instructions and features applicable for \gls{post-quantum} cryptography are available in \gls{z15} and how are they used in context?" as well as "What techniques may be used to increase performance of \gls{post-quantum} cryptography algorithms for different architectures?", we will conduct a performance test. In this case, an experiment is the most appropriate method. We can isolate variables and control the environment in which the study is conducted. Furthermore, we are only interested in the raw performance metrics from different computer platforms. Therefore, a survey would not be relevant because we are not interested in user experience, just the algorithms' raw performance on various platforms. Neither is a case study relevant because it does not test any hypotheses, and it is mostly used in social sciences\cite{wohlin2012}.

\section{Literature Study}
\label{section:method:literature-study}

As previously mentioned, we aim to answer our research question "What specialized instructions and features applicable for post-quantum cryptography are available in \gls{z15}, and how are they used in context?" by conducting a literature study. We will limit the scope to post-quantum \glspl{kem}, specifically the four finalists in \gls{nist}'s standardization process, round 3.

By studying the algorithms' underlying mathematics, the authors' own optimizations on \gls{x86}, and relevant literature on cryptography optimization, we aim to identify what parts of the algorithms are possible and suitable to optimize.

When potential areas of improvements have been identified, literature will be studied to find relevant methods available on \gls{z15}, such as specialized instruction sets and other hardware features. IBM's official documentation as well as research conducted by third-parties will be studied. That way, we hope to get a balanced view of the capabilities of the platform.

To identify relevant research papers, we will search for peer-reviewed papers in reputable journals, such as IEEE and Springer. By searching for keywords related to our research topic, we will select papers that seem relevant based on their title and abstracts. We will then read the selected papers in their entirety to determine the quality and relevance.

The selected papers, as well as the the papers submitted to \gls{nist}, will be included in a start set. The start set will then be used for forward and backward snowballing - a useful technique to find more papers that are relevant to provide information to help answer our research question\cite{wohlin2014}.

\todo[inline]{
Vill svara på What specialized instructions and features applicable for post-quantum cryp-tography are available in z/15 and how are they used in context?

1. Sätt scope för "vad är post-quantum cryptography?" NIST-finalister?
2. Analysera litteratur för vad som är relevant att förbättra med dessa algoritmer (Applicable for post-quantum cryptography? - Studera implementationer i NIST-submissions? Studera rapporten / matten i NIST-submissions?, Lattice-based, (goppa) code-based, i sin tur SHA3 - sponge, bättre uniform random (AES)?) - Sätter scope för applicable for post-quantum
3. Specialized instructions in z/15?
4. Specialized features in z/15?
5. How are they used in context? - Hur definierar vi det här? Bara teoretiskt "de används för matrismultiplikation", eller faktiska implementationer?

"Identifiera att en algoritm är lattice-based, identifiera att lattice bygger på matriser, visa att matriser går att vektorisera enkelt med studier, identifiera att vektorisering är viktigt - z/15 har hårdvara för z/15"

I discussion argumentera saker som borde finnas i framtiden.
}

\section{Experiment}
\label{section:method:experiment}

This section describes and discusses the experiment design and rationale.

\subsection{Goal}
\label{section:method:experiment-goal}

Our goal with the experiment is to analyze \textbf{\gls{post-quantum} \glspl{kem}} for the purpose of evaluating \textbf{the readiness of today's consumer, server and mainframe hardware} with the respect to their \textbf{performance} from the point of view of \textbf{businesses and professionals}, in the context of \textbf{replacing the \glspl{kex} used today in TLS, SSH, VPNs and other applications.}

% The context is an analysis of a model of a crypto system using a "physical" implementation.

\subsection{Subjects}

The test subjects of the experiment are two of the post-quantum algorithms as submitted to \gls{nist}'s third round of submissions, namely \gls{mceliece} and \gls{ntru}. Three of the four finalists - \gls{ntru}, \gls{crystals-kyber}, and \gls{saber}, are lattice-based algorithms, and at most one of these will be standardized. We selected \gls{ntru} based on the positive comments it received and the negative comments the other contestants received from \gls{nist}. \gls{mceliece} was chosen as it was the only non-lattice-based finalist and has a long and excellent reputation\cite{nist2020}.

The \gls{nist} submissions under test are presented in table \ref{table:method:experiment:test-subjects}. All of the selected parameter sets for the subjects have been recommended by their respective authors in the third round of submissions. In the case of \gls{mceliece}, we also rely on previous work from the EU-funded project \gls{pgcrypto}, which identified that not all parameter sets recommended by the authors are appropriate. That is, \gls{pgcrypto} identified that the key size should be a minimum of one megabyte, meaning some candidates were excluded\cite{eu2015}.

\begin{table}[H]
    \centering
    \begin{tabularx}{\linewidth}{l c X}
        \toprule
        \thead{Name} & \thead{Parameter Set} & \thead{Comment} \\
        \midrule
        \gls{mceliece} & 6960119 & \\
        \gls{mceliece} & 8192128 & \\
        \gls{mceliece} & 6960119f & Differs in the key-generation only\\
        \gls{mceliece} & 8192128f & Differs in the key-generation only\\
        \gls{ntru} & hrss701 & \\
        \gls{ntru} & hps2048677 & \\
        \bottomrule
    \end{tabularx}
    \caption{Test Subjects}
    \label{table:method:experiment:test-subjects}
\end{table}

Today key exchange algorithms such as \gls{x25519} (in some contexts known as \gls{curve25519}\footnote{\href{https://mailarchive.ietf.org/arch/msg/cfrg/-9LEdnzVrE5RORux3Oo\_oDDRksU/}{https://mailarchive.ietf.org/arch/msg/cfrg/-9LEdnzVrE5RORux3Oo\_oDDRksU/}}), \gls{ecdh}, \gls{ecdhe}, \gls{dh}, \gls{dhe} are used to exchange session keys in TLS\footnote{\href{https://tools.ietf.org/html/rfc8446}{https://tools.ietf.org/html/rfc8446}}, SSH\cite{williams2011}, VPNs such as OpenVPN\footnote{\href{https://openvpn.net/community-resources/openvpn-cryptographic-layer/}{https://openvpn.net/community-resources/openvpn-cryptographic-layer/}}, IPSec\footnote{\href{https://tools.ietf.org/html/rfc2409}{https://tools.ietf.org/html/rfc2409}} and Wireguard\footnote{\href{https://www.wireguard.com/protocol/}{https://www.wireguard.com/protocol/}}. Variants of the mentioned key exchange algorithms are also used in messaging applications such as the Signal protocol\cite{gordon2017}. Some of these algorithms and parameter sets are recommended for use today by organizations such as \gls{nist} and the \gls{ietf}, namely \gls{x25519}\footnote{\href{https://tools.ietf.org/html/rfc7748}{https://tools.ietf.org/html/rfc7748}}, \gls{ecdhe}\cite{nist2019} and \gls{dhe}\cite{nist2019}. These recommended algorithms and parameter sets will constitute the control subjects for this study and are presented in table \ref{table:method:experiment:control-subjects}.

\begin{table}[H]
    \centering
    \begin{tabularx}{\linewidth}{l X}
        \toprule
        \thead{Name} & \thead{Parameter Set} \\
        \midrule
        \gls{ecdhe} & \gls{p-256}, \gls{x25519} \\
        \gls{dhe} & \gls{rsa} with 2048 bit keys \\
        \bottomrule
    \end{tabularx}
    \caption{Control Subjects}
    \label{table:method:experiment:control-subjects}
\end{table}

In the case of the \gls{nist} submissions, in this context, the submitted implementations are not thought to be production-ready due to them not yet being standardized nor battle-tested. This, in addition to our interest in exploring optimization techniques that are applicable for post-quantum cryptography, results in us exploring various ways the implementations may be optimized.

As the control subjects are intended to represent the algorithms already in use, they are thought to be a solved problem. That is, in this context, implementation details are of no concern - an implementation on the native platform is assumed to be optimized and production-ready.

\subsection{Variables}

The identified independent variables are presented in table \ref{table:method:experiment:independent-variables} below.

\todo{Mention lack of SSE, HMC, FPGA etc.? Argue that it is out of scope.}

\begin{table}[H]
    \centering
    \begin{tabularx}{\linewidth}{l>{\RaggedRight}l>{\RaggedRight}X>{\RaggedRight\arraybackslash}X}
        \toprule
        \thead{Group} & \thead{Label} & \thead{Potential Values} & \thead{Comment} \\
        \midrule
        Compiler & Compilers & GCC, Clang, IBM XL, IBM AutoSIMD, Intel C++ Compiler (ICC) & There are more, but these are the most commonly used compilers \\
        Compiler & Flags & -O0, -O1, ..., -O5, -march, -mtune, -fomit-frame-pointer, -fwrapv, -fPIC, -fPIE, -mcpu, -Qunused-arguments & There are virtually infinite possible flags. These are the minimal ones used for optimization in this context and as used by previous work (\gls{supercop}, \gls{nist} publications) \\
        Implementation & Libraries & \gls{sha3} / \gls{shake} / \gls{keccak}, \gls{aes-instruction-set}, \gls{cpacf} & \\
        Implementation & Optimizations & \gls{avx}, \gls{avx2}, \gls{avx512}, \gls{simd} (GPU), \gls{simd} on \gls{ibmz}, \gls{hsm} & These are the optimizations available in the selected implementations \\
        Environment & OS & OS Type and version, Scheduling & \\
        Environment & Hardware & CPU model, CPU Architecture, available cores, available RAM, RAM model and specification & \\
        \bottomrule
    \end{tabularx}
    \caption{Independent Variables}
    \label{table:method:experiment:independent-variables}
\end{table}

In table \ref{table:method:experiment:dependent-variables} below, the dependant variables of interest are presented. These are the variables that relate to the goal as specified in section \ref{section:method:experiment-goal}. Note that, as \gls{ecdh} and \gls{dh} are not \glspl{kem}, but \gls{kex} algorithms, its "encapsulation" and "decapsulation" is the same stage. As such, only one measurement is necessary and will henceforth be referred to as the encapsulation phase.

\begin{table}[H]
    \centering
    \begin{tabularx}{\linewidth}{l X X}
        \toprule
        \thead{Group} & \thead{Stages} & \thead{Comments}\\
        \midrule
        Throughput & Encapsulation, Decapsulation, Key-pair generation & CPU cycles, instruction count, wall-clock time\\
        Hardware Utilization & Encapsulation, Decapsulation, Key-pair generation & Memory usage (heap and stack allocation), logical core utilization\footnotemark\\
        \bottomrule
    \end{tabularx}
    \caption{Dependent Variables}
    \label{table:method:experiment:dependent-variables}
\end{table}
\addtocounter{footnote}{-1}
\addtocounter{footnote}{1}
\footnotetext{Note that some of the chosen environments may not expose physical core, rather \glspl{vcpu} or logical cores.}

\subsection{Instrumentation}

This section describes the tools and instrumentation used in the experiment.

\subsubsection{Toolset - Processor}
The instrumentation will rely on the Linux perf (perf\_event\_open) API. This API is part of the kernel and produces highly accurate measurements, regardless of the underlying hardware. The recorded values for CPU cycles, instructions etc. may be counted or sampled. The API may work externally, by monitoring the entire lifetime of a process, or internally by instrumenting a program with specifically measured blocks of code.\todo{Cite this properly?}

Other measurement alternatives that require the tested binary to be instrumented, such as prof, gprof and gperftools are unable to accurately measure IO-bound tasks, the performed instructions, cycles or time spent inside of the kernel.\todo{Cite this properly?}

Another tool that monitor the entire lifetime of a process, Valgrind (Callgrind) emulates the running code in a simplified machine, meaning the measurements are not applicable in our case where the underlying platform is of great importance.\todo{Cite this properly?}

Other libraries or APIs such as those used in \gls{supercop} and PAPI were identified to provide inaccurate and non-deterministic results and were therefore not applicable for this use case. In the case of \gls{supercop}, some measurements were identified to rely on extrapolation and estimation of data.\todo{Cite this properly?}

As such the Linux perf API was identified as the only applicable tool for profiling the CPU usage of an algorithm. Valgrind (Callgrind) was identified as usable for identifying hot paths in the code, but as perf provides the same capability with less overhead and better support, perf will be used for that analysis as well.

To help aid the usage of the API, we developed a small library which we used to instrument the algorithms. The library is open source and published as part of this work\footnote{\href{https://github.com/profiling-pqc-kem-thesis/perf}{https://github.com/profiling-pqc-kem-thesis/perf}}.

\subsubsection{Toolset - Memory}

As previously mentioned, Valgrind is a tool that emulates the machine code of a binary in order to analyze its behaviour in its entirety. Although we found it was not applicable for us to use for processor profiling, it was identified as the tool of choice for memory analysis.

\subsubsection{Measurements}

Using the aforementioned toolset, we will instrument the subjects and measure the following values.

\begin{itemize}
    \item Retired instruction count
    \item Total cycle count
    \item Page fault count
    \item Cache miss count \footnote{x86 only - not available on s390x}
    \item Heap allocation in bytes
    \item Stack usage in bytes
    \item Wall clock time in nanosecond resolution
\end{itemize}

The measurements will be taken at different points in the algorithms' lifecycles as explained in table \ref{table:method:instrumentation} below. The same measurements are to be repeated for the three phases - key-generation, encryption (key encapsulation) and decryption (key decapsulation). By first analyzing the hot paths of the \gls{nist} submissions, we will also measure the parts of the algorithm that are invoked the most. As some variables in the environment have been identified, such as the various implementations for \gls{shake} and \gls{aes}, the use of these will be measured as well. In some cases, the authors have provided optimized implementations. In such cases, these optimized functions will be measured as well.

\begin{table}[H]
    \centering
    \begin{tabularx}{\linewidth}{l>{\RaggedRight}X>{\RaggedRight\arraybackslash}X}
        \toprule
        \thead{Subject} & \thead{Location} & \thead{Comment}\\
        \midrule
        All & Right before and right after the algorithm is invoked & Provides the bounds of the timeline of the execution \\
        \gls{mceliece} & Right before and right after the matrix multiplication is invoked & \\
        \gls{mceliece}, \gls{ntru} & Right before and right after random bytes are requested & \\
        \gls{mceliece}, \gls{ntru} & Right before and right after the SHAKE implementation is called & \\
        \gls{mceliece} & int32\_sort & \\
        \gls{mceliece} & syndrome, transpose, vec256\_sq, vec256\_copy, ... &\\
        \gls{ntru} & Poly math - ntru\_poly\_S3\_inv, ntru\_poly\_rq\_to\_s3, ntru\_poly\_rq\_mul, square, ...& \\
        \gls{ntru} & crypto\_sort\_int32 & \\
        \bottomrule
    \end{tabularx}
    \caption{Measurement Locations}
    \label{table:method:instrumentation}
\end{table}
\todo{Update with accurate information}

For the exact instrumentation used in each subject, please refer to the source code published alongside this work\footnote{\href{https://github.com/profiling-pqc-kem-thesis/subjects}{https://github.com/profiling-pqc-kem-thesis/subjects}}.

\subsection{Validity}

\subsubsection{Conclusion validity}

\todo[inline]{Add Conclusion validity}

\subsubsection{Internal validity}
\label{section:method:internal-validity}
% History - An unrelated event influences the outcomes.
Other software running on the OS may influence the results. To prevent this, we will minimize the set of programs running on the system and run the experiment multiple times at different times.

% Maturation - The outcomes of the study vary as a natural result of time.
Another factor, \gls{jit} compilation and garbage collection could influence the results. As we use the programming languages C and Assembly this will not be a problem as they do not use \gls{jit} compilation and provide full control of memory. Other programs running on the system which use JIT compilation or garbage collection falls under the previous category.

\todo{GPU kernel \gls{jit} compilation}

\todo{Swap locks the system while writing/reading to from disk}
\todo{Cache misses}

\todo{Windows Update?}

The post-quantum implementations are not final nor standardized, which may affect the performance of the implementations and the relevance of the results in the future. \gls{dhe} and \gls{ecdhe} have been standardized and optimized for many years and have mature implementations, they are likely fully optimized by this point.

% Instrumentation - Different measures are used in the pre-test and post-test phases.
We have written the measurement tools, the implementation may have shortcomings and OS differences may lead to different results. To ensure the correctness and consistency between platforms we both have reviewed the code and tested it on different platforms\todo{??????}.

%Testing - The pre-test influences the outcomes of the post-test.
Hardware can throttle because of the temperature increase caused by the benchmarks. This could result in worse performance than intended when running consecutive tests. To mitigate this, benchmarks will not be run directly after each other. We will instead wait for the temperature of the system to normalize to the same level it was before the test was started.

We cannot necessarily use the same OS for the mainframes and x86, which could cause varying results. This is however not an issue. We do not aim to pit x86 against Z, rather represent a type of computer and evaluate their readiness.

%Selection bias - Groups are not comparable at the beginning of the study.
One may argue that the choice of the two post-quantum algorithms is biased, but we argue that that point is invalid. The algorithms were selected from four finalists in round 3 of \gls{nist}'s standardization process. \gls{ntru}, \gls{crystals-kyber}, and \gls{saber} are all lattice-based algorithms, and at most one of these will be standardized. \gls{ntru} was selected based on the comment \gls{nist} gave each of the participants\todo{insert the comment?}. \gls{mceliece} was the only non-lattice-based finalist. It also has a long and good reputation\cite{nist2020}. In addition to these four finalists, there are alternate candidates still in round 3. These were not considered. The choice was made to bring down the scope to a manageable level for this work.

As has been mentioned previously, we lean heavily on \gls{nist} recommendations as they in many cases provide the authoritative recommendation of algorithms used by protocol implementers (\todo{to some extent EU paper as well????}). As we have not identified any other standardization process like that of \gls{nist}, we have concluded that relying on their expertise in this context is correct.

For the implementations of \gls{dhe} and \gls{ecdhe}, we only use the underlying algorithms provided by \gls{openssl} and \gls{openssl} with the IBM engine. This was done as \gls{openssl} was identified as the main library for these algorithms on the tested platforms. Using other libraries such as BoringSSL might have resulted in different measurements. The implementations available in \gls{openssl} have been rigorously tested and analyzed by the industry over the decades it has seen use. We are therefore confident that, although the exact measurements may differ between libraries, \gls{openssl} provides a solid foundation for our use case as a representation of today's algorithms.

\todo{We have only selected a subset of potential optimizations?}

%Attrition - Dropout from participants
\todo{Not applicable? Perhaps all subjects and platforms will not be able to see all optimizations or compiler flags?}

\subsubsection{Construct validity}

% Extent to which the experiment setting actually reflects the construct under study. Treatment reflects the construct of the cause well. Output reflects the construct of the effect well

% Construct validity evaluates whether a measurement tool really represents the thing we are interested in measuring. It’s central to establishing the overall validity of a method.

As previously mentioned, we are interested in measuring the following values:

\begin{itemize}
    \item Encapsulation throughput in number of required CPU cycles, instruction count and wall clock time
    \item Decapsulation throughput in number of required CPU cycles, instruction count and wall clock time
    \item Keypair throughput in number of required CPU cycles, instruction count and wall clock time
    \item Hardware utilization in terms of memory (heap and stack allocation)
    \item Hardware utilization in terms of processing (logical core utilization)
\end{itemize}

For our measurements, we rely on the standard Linux kernel-based API named perf (perf\_event\_open). The API was introduced in Linux 2.6.31 which was released in 2009\footnote{\href{https://www.linux.com/news/linux-2631-released/}{https://www.linux.com/news/linux-2631-released/}}. The API has grown, and as is tradition with the Linux development, each iteration of the API has been reviewed extensively by multiple people throughout the years. We are confident that the API provides as accurate data as the kernel is able to collect.

To make the API usable, we provide a lightweight instrumentation tool which is used to measure the values as listed above. By using the C pre-processor, usage of the tool is readable and configurable and written to not introduce any overhead over the raw Linux API when performing measurements. We therefore argue that the tool does not introduce any additional risks over the base API.

By reading the official documentation of the API, we have ensured that the values we measure correspond to those we are interested in. That is, when we use the API to measure CPU cycles, for example, we have asserted that our code indeed refers to the correct measurements as described in the documentation.

\subsubsection{Content validity}

% Refers to the extent to which a measure represents all facets of a given construct.
%"refers to the degree to which an assessment instrument is relevant to, and representative of, the targeted construct it is designed to measure."

To answer our first research question, \textit{Does performance of post-quantum cryptography algorithms differ between architectures and if so, how?},  we do not need to measure anything other than at the start and end of the algorithm invocation. This, since the algorithm's usage will include all of the system's parts. As such, measuring the time, CPU cycles and total instructions of the entire system should suffice.

For our other research questions we examine optimizations of different parts of the algorithm. We need to see how the parts in the system as a whole are performing. For this, we will use micro-benchmarks. Micro-benchmarks may add a non-trivial overhead that depends on the number of benchmarks. We will evaluate the impact of these. We will perform the experiment once with and once without the micro-benchmarks to see the difference and make sure it is not statistically significant. If there is a significant amount of overhead, each micro-benchmark will be run in complete isolation.

When measuring memory usage, we cannot measure any other performance metric as it could induce a higher memory load unrelated to the algorithm under test. Instead, we perform the measurement of the memory usage of the key-pair generation, encryption, and decryption separate from the CPU measurements.

\subsubsection{Criterion validity}
In the publications for the \gls{nist} submissions, the authors have written their own performance analysis using the \gls{supercop} benchmark tool. The presented measurements are for the total number of cycles used by the algorithms for generating keys, encapsulation and decapsulation. As these figures have not been validated by a third party, we will not use them to validate ours. We will however present comparisons in order to identify potential issue in their measurements or ours.

\subsubsection{External validity}

% Sampling bias.
As previously discussed under section \ref{section:method:internal-validity}, internal validity, one may argue that the selection of subjects is biased as we do not take the entire population of post-quantum and modern-day algorithms into account. Such a comparison would however be unfeasible. That is why the presented sample is based on the accumulative recommendations of several organizations. We therefore argue that our sample is representative of the algorithms that are and likely will be in use.

% History.
Another factor that may hurt the generalizability of the results is the potential of a series of unrelated events influencing the outcome. We have identified several actions to help mitigate this risk, as discussed further in section \ref{section:method:internal-validity}, internal validity.

% Experimenter effect.
The implementations used for benchmarks are not created by us. They have, however, been slightly altered in order to support various forms of optimization. This fact may result in the tested implementations performing differently than if the original implementers would have applied the optimizations. It is therefore plausible that the measurements of the samples will not be general to other implementations using the same techniques.

% Aptitude treatment.
As there are several optimization techniques such as vectorization and compiler flags applied simultaneously, there is a potential for techniques to cancel each other out or in other ways impact the performance negatively. We will mitigate this risk by evaluating each form of optimization in isolation, before combining all techniques into a truly optimized implementation. These evaluations are listed in table \ref{table:method:instrumentation}.

% Situation effect.
Factors such as various settings, time of day and location may limit the generalizability of the presented findings. We have identified several mitigative actions as defined under \ref{section:method:internal-validity}, internal validity.

% Counter threats.
To counter threats across the experiment we aim to improve replication of the results by enabling third parties to carry out the experiment on their own. This is done by providing detailed methodology, the used tools and any accumulated data. The data, tools and the tested implementations used are available as open source\footnote{\href{https://github.com/profiling-pqc-kem-thesis}{https://github.com/profiling-pqc-kem-thesis}}.

\subsection{Execution}

This section describes how the experiment will be executed, including the configurations used for testing.

\subsubsection{Hardware Configurations}

To fulfil the goal of the experiment, several hardware configurations will be tested. These configurations have been divided into three categories, namely Consumer Hardware, Server Hardware and Mainframe Hardware.

\begin{table}[H]
    \centering
    \begin{tabularx}{\linewidth}{X c c}
        \toprule
        \thead{Label} & \thead{CPU} & \thead{RAM}\\
        \midrule
        Modern Workstation & Intel i9-9900k & 32GB 3600MHz DDR4\\
        Modern Laptop & Intel i7-8565u & 16 GB 2600MHz DDR4\\
        Old Mid-Range Laptop & Intel i5-3230m & 8GB 1600MHz DDR3\\
        Old Low-Range Laptop & Intel i3-3120m & 8GB 1600MHz DDR3\\
        \bottomrule
    \end{tabularx}
    \caption{Consumer Hardware - x86}
    \label{table:method:consumer-hardware}
\end{table}
\todo{The above table requires more up-to-date information.}

\begin{table}[H]
    \centering
    \begin{tabularx}{\linewidth}{X c c}
        \toprule
        \thead{Label} & \thead{CPU} & \thead{RAM}\\
        \midrule
        Cloud Provider 1 - Shared CPU\footnotemark & 1 vCPU & 1GB RAM \\
        Cloud Provider 1 - Dedicated CPU\footnotemark & 1 CPU & 1GB RAM \\
        Cloud Provider 2 - Shared CPU\footnotemark & 1 vCPU & 1GB RAM \\
        Cloud Provider 2 - Dedicated CPU\footnotemark & 1 CPU & 1GB RAM \\
        \bottomrule
    \end{tabularx}
    \caption{Server Hardware - x86}
    \label{table:method:server-hardware}
\end{table}
\addtocounter{footnote}{-4}
\addtocounter{footnote}{1}
\footnotetext{A DigitalOcean "small droplet"}
\addtocounter{footnote}{1}
\footnotetext{A DigitalOcean "small droplet"}
\addtocounter{footnote}{1}
\footnotetext{A Google Cloud Platform "small droplet"}
\addtocounter{footnote}{1}
\footnotetext{A Google Cloud Platform "small droplet"}
\todo{The above table requires more up-to-date information.}

\begin{table}[H]
    \centering
    \begin{tabularx}{\linewidth}{X c c c}
        \toprule
        \thead{Label} & \thead{Model} & \thead{CPU} & \thead{RAM}\\
        \midrule
        Mainframe & IBM \gls{z15} & \gls{z15} (\gls{s390x}) & 4GB\\
        \bottomrule
    \end{tabularx}
    \caption{Mainframe Hardware}
    \label{table:method:mainframe-hardware}
\end{table}
\todo{The above table requires more up-to-date information.}

\subsubsection{Compiler Configurations}

The following compiler configurations will be assessed where applicable.

\begin{table}[H]
    \centering
    \begin{tabularx}{\linewidth}{l c c X}
        \toprule
        \thead{Label} & \thead{Name} & \thead{Version} & \thead{Flags}\\
        \midrule
        GCC Minimal & GCC & 8 & Minimal required\\
        Clang Minimal & Clang & 8 & Minimal required\\
        IBM XL Minimal & IBM XL & 8 & Minimal required\\
        GCC Optimized & GCC & 8 & -march=native\\
        Clang Optimized & Clang & 8 & -march=native\\
        IBM XL Optimized & IBM XL & 8 & -march=native\\
        \bottomrule
    \end{tabularx}
    \caption{Caption}
    \label{table:method:compilers}
\end{table}

In order to see the exact flags used to compile each sample, please refer to the source code for the experiment\footnote{\href{https://github.com/profiling-pqc-kem-thesis}{https://github.com/profiling-pqc-kem-thesis}}.

\subsubsection{Implementation Configurations}

The following implementation details will be changed in order to evaluate potential performance differences.

\begin{table}[H]
    \centering
    \begin{tabularx}{\linewidth}{l>{\RaggedRight}X>{\RaggedRight\arraybackslash}X}
        \toprule
        \thead{Label} & \thead{Description}\\
        \midrule
        Reference Implementation & The reference implementation as published in the \gls{nist} submissions, or the available \gls{openssl} implementation. \\
        Optimized Implementation(s) & Use the authors' provided optimizations.\\
        Optimized \gls{shake} implementation & Use a \gls{keccak} implementation optimized for the native platform (plain, \gls{avx2}, \gls{avx512}).\\
        Optimized \gls{aes} implementation & Exchange the included \gls{aes} implementation with \gls{openssl} which is optimized for the native platform (plain, \gls{aes}, \gls{aes-ni}).\\
        Fully optimized implementation & Optimized implementation, optimized \gls{shake} implementation, optimized \gls{aes} implementation.\\
        \bottomrule
    \end{tabularx}
    \caption{Mainframe Hardware}
    \label{table:method:implementation-configurations}
\end{table}
\todo{The above table requires more up-to-date information.}

\subsubsection{Configuration Summary}

The following section summarizes the configurations that are to be tested.

\begin{table}[H]
    \centering
    \begin{tabularx}{\linewidth}{l X X}
        \toprule
        \thead{Hardware} & \thead{Compiler Configurations} & \thead{Implementation Configurations}\\
        \midrule
        Modern Workstation & GCC Minimal, GCC Optimized, Clang Minimal, Clang Optimized & All (\gls{avx2}, \gls{aes-instruction-set})\\
        Modern Laptop & GCC Minimal, GCC Optimized, Clang Minimal, Clang Optimized & All (\gls{avx2}, \gls{aes-instruction-set}) \\
        Old Mid-Range Laptop & GCC Minimal, GCC Optimized, Clang Minimal, Clang Optimized & All (\gls{avx}, \gls{aes-instruction-set})\\
        Old Low-Range Laptop & GCC Minimal, GCC Optimized, Clang Minimal, Clang Optimized & All (\gls{avx})\\
        Cloud Provider 1 - Shared CPU & GCC Minimal, GCC Optimized, Clang Minimal, Clang Optimized & All (\gls{avx512}, \gls{aes-ni})\\
        Cloud Provider 1 - Dedicated CPU & GCC Minimal, GCC Optimized, Clang Minimal, Clang Optimized & All (\gls{avx512}, \gls{aes-ni})\\
        Cloud Provider 2 - Shared CPU & GCC Minimal, GCC Optimized, Clang Minimal, Clang Optimized & All (\gls{avx512}, \gls{aes-ni})\\
        Cloud Provider 2 - Dedicated CPU & GCC Minimal, GCC Optimized, Clang Minimal, Clang Optimized & All (\gls{avx512}, \gls{aes-ni})\\
        Mainframe & GCC Minimal, GCC Optimized, IBM XL, IBM XL Optimized & All (\gls{cpacf})\\
        \bottomrule
    \end{tabularx}
    \caption{Configuration Summary}
    \label{table:method:configuration-summary}
\end{table}